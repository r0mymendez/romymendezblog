[
  {
    "path": "posts_en/2023-12-02-rabbitmq-pika/",
    "title": "RabbitMQ-Pika",
    "description": "RabbitMQ permite gestionar colas de mensajes entre emisores y destinatarios, en el siguiente post vamos a utilizar en python **Pika** para su implementación.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2023-12-02",
    "categories": [
      "Python"
    ],
    "contents": "\n\nContents\nRabbitMQ-Pika\nIntroduction: What is RabbitMQ?\nImplementation with Pika in Python 🐍\n1️⃣ . Install pika\n2️⃣ . Create send.py 📄 file\n3️⃣. Create send.py 📄 file\n4️⃣. MongoDB + Pika\n\n\n\nRabbitMQ-Pika\nRabbitMQ enables the management of message queues between senders and receivers.\nIn the following post, we will employ Python’s Pika library for its implementation.\n\nIntroduction: What is RabbitMQ?\nRabbitMQ is an intermediary system designed to facilitate the transfer of messages between producers and consumers through the implementation of queues.\nThis component, essential in distributed systems architecture, is grounded in key concepts:\n1️⃣.\nProducer: The entity responsible for originating and dispatching messages.\n2️⃣.\nQueue: A reservoir where messages are temporarily stored.\n3️⃣.\nConsumer: The receiving instance that processes messages according to the system’s needs.\nThis introduction aims to provide a clear and concise overview of the fundamental elements of RabbitMQ, paving the way for a deeper understanding of its functioning in messaging environments.\n\nImplementation with Pika in Python 🐍\nImplementing in Python with the Pika library involves creating two essential programs: the producer and the consumer.\nPika provides an effective interface for communication with RabbitMQ, leveraging a set of carefully designed objects for this purpose.\nIn our practical example, envision the producer as an application designed to manage food delivery orders 🛵.\nThis application, geared towards optimizing the delivery process, is responsible for sending multiple messages📝 related to user 📱 food orders.\nTo achieve this implementation, we will undertake the following steps:\nSteps\nDescriptions\nProducer:\nDevelop a program that, like an efficient order-taker, generates and sends messages📝 to the RabbitMQ queue. These messages will contain valuable information about food orders.\nConsumer:\nCreate a program that acts as the receiver of these messages in the queue. The consumer will be responsible for processing these messages according to the system’s needs, performing relevant actions, such as managing the delivery of orders.\nThis structured and efficient approach ensures a clear and functional implementation, providing a robust foundation for systems managing information flows in dynamic environments.\n1️⃣ . Install pika\n!pip install pika\n2️⃣ . Create send.py 📄 file\nimport pika\nfrom datetime import datetime\n\nconnection = pika.BlockingConnection(\n    pika.ConnectionParameters(host='localhost'))\nchannel = connection.channel()\n\nchannel.queue_declare(queue='delivery')\n\npedidos=['🍕🍕🍕','🍔🍔🍔','🍰🍰🍰','🍺🍺🍺']\n\nfor i in pedidos:\n    channel.basic_publish(exchange='', routing_key='delivery', body=i)\n    print(\" [x] Se envia pedido!'\"+ i)\n\nconnection.close()\n3️⃣. Create send.py 📄 file\nimport pika, sys, os\nfrom datetime import datetime\n\n\ndef main(queue='delivery'):\n    connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))\n    channel = connection.channel()\n    channel.queue_declare(queue=queue)\n\n    def callback(ch, method, properties, body):\n        print(\" [x] Received %r\" % body.decode())\n\n    channel.basic_consume(queue='delivery', on_message_callback=callback, auto_ack=True)\n\n    print(' [*] Waiting for messages. To exit press CTRL+C')\n    channel.start_consuming()\n\nif __name__ == '__main__':\n    try:\n        main(queue=queue)\n    except KeyboardInterrupt:\n        print('Interrupted')\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)\nImage description4️⃣. MongoDB + Pika\nIn the following, we will modify the script to enable it to connect to a MongoDB Atlas and perform the insertion of received messages.\nimport pymongo\nimport pika, sys, os\nfrom datetime import datetime\n\n# Crear una conexion con MongoClient\nclient = pymongo.MongoClient(\"mongodb+srv://NombreUser:PasswordUser@clusterName.moczg.mongodb.net/rabbit?retryWrites=true&w=majority\")\n\n# Database\ndb = client[\"rabbit\"]\n\n# Collection\ncollection= db[\"mensajes\"]\n\ndef main(queue='delivery'):\n    connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))\n    channel = connection.channel()\n    channel.queue_declare(queue=queue)\n\n    def callback(ch, method, properties, body):\n        print(\" [x] Received %r\" % body.decode())\n        body_indsert={'fecha':datetime.now(),'queue':queue,'message':body.decode()}\n        db[\"mensajes\"].insert_one(body_indsert)\n\n    channel.basic_consume(queue='hello', on_message_callback=callback, auto_ack=True)\n\n    print(' [*] Waiting for messages. To exit press CTRL+C')\n    channel.start_consuming()\n\nif __name__ == '__main__':\n    try:\n        main(queue=queue)\n    except KeyboardInterrupt:\n        print('Interrupted')\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)\nTo download the code for these two files, you can do so from the following link.\nTo learn more about RabbitMQ, you can visit the following sites:\n📄 Oficial Documentation\n📄 Rabbit Tutorial\n🐍 Pika\n\n\n\n",
    "preview": "https://image.freepik.com/free-vector/landing-page-send-message-illustration_126608-31.jpg",
    "last_modified": "2023-12-02T20:09:37+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-11-18-aws-copilot/",
    "title": "AWS Copilot",
    "description": "In the following article, I explain what AWS Copilot is, how to use this project, and the ease of implementing it.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://r0mymendez.github.io/romymendezblog/"
      }
    ],
    "date": "2023-11-14",
    "categories": [
      "Cloud Computing"
    ],
    "contents": "\n\nContents\nIntroduction\n💡AWS ECS (Elastic Container Services)\n⚒️ ECS with EC2 instances\n⚒️ ECS with AWS Fargate (Serverless)\n\n💡AWS Copilot\n🔎 Comparison of tasks to be performed \n🧩 Components\n🚀 Deployment with AWS Copilot: A 5-Step Guide\n🔎 Logs\n📉 Traffic to production\n🧪Testing\n💰 Service Cost\n\n🎯 Key Takeaways\n📚 References\n\nIntroduction\nIn this article, I explain the use of the AWS Copilot service.\nHowever, to carry it out, it’s necessary to start by analyzing what the AWS Elastic Container Service (ECS) is and how it works, along with its deployment methods.\nThis is necessary because AWS Copilot performs the implementation and deployment of an application using ECS\n💡AWS ECS (Elastic Container Services)\nElastic Container Service ( ECS ) is a scalable container orchestration platform proprietary to AWS.\nIt is designed to run, stop, and manage containers in a cluster.\nTherefore, ECS is AWS’s Docker container service that handles the orchestration and provisioning of Docker containers.\nThis service includes the following concepts:\n☁️ Task Definition: Describes how to start a Docker container.\n☁️ Task: This is a running container with the configuration defined in the task definition.\n☁️ Service: Defines long-running tasks from the same task definition.\n☁️ Cluster: A logical group of EC2 instances.\n☁️ Container Instance: This is just an EC2 instance that is part of an ECS cluster and has Docker installed.\n⚒️ ECS with EC2 instances\nIn this model, containers are deployed on EC2 instances (VMs) created for the cluster.\nECS manages them along with the tasks that are part of the task definition\n✅ Advantages\n❌ Disadvantages\n- Complete control over the type of EC2 instance used is provided.\n- When working with EC2, it’s necessary for the administrator of this architecture to handle all security updates and scaling of instances.\n- It allows the use of instances that can be optimized depending on what you want to execute.\n- The cost is based on the type of EC2 instance running within the cluster and the VPC networks.\n⚒️ ECS with AWS Fargate (Serverless)\nIn this serverless configuration, the reliance on EC2 instances is eliminated, simplifying the deployment process. Instead, you only need to specify the required CPU and memory combination. AWS Fargate allows for a fully managed and serverless container deployment experience.\n✅ Advantages\n❌ Disadvantages\n- There are no servers to manage.\n- ECS + Fargate supports only one network mode, and this limits control over the network layer.\n- AWS is in charge of the availability and scalability of the containers.\n- Cost is based on the CPU and memory you select. The number of CPU cores and GB determines the cost of running the cluster.\n- Fargate Spot is a new capability that can run ECS tasks that are interruption-tolerant at up to a 70% discount compared to the Fargate price.\n\n💡AWS Copilot\nAWS Copilot is a tool used through the AWS command line that simplifies the creation, deployment, monitoring, and operation of containers in ECS using a local development environment\nThis tool manages the components required for the deployment and operation of an application, such as VPC, load balancers, deployment pipelines, and storage.\nTherefore, it’s only necessary to provide an application container and minimal configurations, resulting in a faster deployment and focusing on application development.\n🔎 Comparison of tasks to be performed \nThe services will communicate with each other, so it is necessary to consider the following scenarios:\nActivities\nWithout AWS-copilot\nWith AWS-copilot\nApplication developmen\n📗 Development team\n📗 Development team\nContainer generation\n📗 Development team\n📗 Development team\nVirtual Private Cloud (VPC) Subnets\n📗 Development team\n📙 AWS-Copilot\nLoad balancers\n📗Development team\n📙 AWS-Copilot\nDeployment flows (ci/cd)\n📗 Development team\n📙 AWS-Copilot\nPersistent storage of your application\n📗 Development team\n📙 AWS-Copilot\nSynchronize deployment across environments\n📗 Development team\n📙 AWS-Copilot\n🧩 Components\nThe following table contains the components that are configured when using the AWS Copilot service.\nComponent\nDescription\nApplication\nAn application is a grouping mechanism for the pieces of your system.\nEnviroment\nAn environment is a stage in the deployment of an application.\nService\nA service is a single process of long-running code within a container.\n🚀 Deployment with AWS Copilot: A 5-Step Guide\nIn just 5 steps we can deploy an application using aws-copilot, as shown in the following image.\nThis allows the development team to only focus on development and not so much on the deployment of the infrastructure.\nThe first application that is deployed in copilot will make a default configuration and the same will be with a serverless container in fargate.\nAs seen in the following image, with only 5 steps we can deploy an application.\nImage descriptionThe steps in the flow are as follows:\nInstall AWS Copilot, which will require AWS client credentials.\nCreate the Dockerfile for our application.\nExecute copilot init in a terminal to initialize.\nWhen running init, some questions will appear to answer, such as the application name, service type, service name, and Dockerfile location.\nIn this final step, a URL will be provided to access the application\n🔎 Logs\nTo obtain the logs of the deployed containers, it is necessary to execute the following command:\n$ copilot svc logs- follow\n📉 Traffic to production\nTo deploy in production it is necessary to be able to generate different environments, so to generate them it is necessary to execute the following command.\n$ copilot env init\nSubsequently, it is important to be able to modify the manifest file that contains all the application configurations and is located in  nombredeaplicacion/manifest.yml\nOnce the environment configuration is complete, it is necessary to deploy it to production (or another environment, but the following example is in production).\n$ copilot svc deploy —env production\n🧪Testing\nIn order to test the deployed application, you can use ApacheBench which allows you to generate traffic to the web application.\nFor this it is necessary to be able to execute the following command in which you want to generate a number of 5000 transactions to my service with a concurrency of 25 requests at a time.\nab -n 5000 -c 25<http://app12345.us-east-1.elb.amazonaws.com>\nIf I do not have the expected response, I can modify my manifest file and horizontally scale the application based on the different environments.\n💰 Service Cost\nAWS Copilot is distributed by Amazon under an Apache 2.0 license, making it an open-source application.\nAs an open-source tool, AWS Copilot incurs no additional costs. The pricing is solely determined by the usage of the configured services. This cost-efficient model allows users to leverage the full capabilities of AWS Copilot without incurring any licensing fees.\n🎯 Key Takeaways\nIn conclusion, AWS Copilot stands out for the following features:\nAWS Copilot emerges as a robust, open-source AWS tool that streamlines the deployment of production-ready containers in a mere 5 steps, allowing development teams to concentrate on coding rather than grappling with infrastructure intricacies.\nConfiguration is effortless, demanding only the execution of a few commands and adjustments to the manifest file based on the application’s resource requirements.\nAddressing horizontal scaling needs is a breeze – a simple modification to the manifest file followed by a deployment is all it takes.\nAWS Copilot facilitates the establishment of a CI/CD pipeline for seamless, automatic deployments across various environments.\nEffortlessly generate KPIs, set up alarms, and collect metrics with just a few commands through the user-friendly AWS Copilot service.\n📚 References\n📚Title: Presentamos AWS Copilot, Site: Blog de Amazon Web Services (AWS), Author: Nathan Peck,Gabriel Gasca Torres y José Lorenzo Cuéncar, url: <https://aws.amazon.com/es/blogs/aws-spanish/presentamos-aws-copilot/>,\n📚Title: Introducción a Amazon ECS mediante AWS Copilot, Site: Documentación oficical de AWS, Author: AWS, url: <https://docs.aws.amazon.com/es_es/AmazonECS/latest/userguide/getting-started-aws-copilot-cli.html>\n📚Title: AWS Copilot, Site: AWS, Author: AWS, url: <https://aws.amazon.com/es/containers/copilot/>\n📚Title: Gentle Introduction to How AWS ECS Works with Example Tutorial, Site: Medium, Author: Tung Nguyen , Url: <https://medium.com/boltops/gentle-introduction-to-how-aws-ecs-works-with-example-tutorial-cea3d27ce63d>,\n\n\n\n",
    "preview": "https://img.freepik.com/free-vector/two-users-searchig-big-data-cloud-computing-storage-technology-large-database-data-analysis-digital-information-concept-vector-isolated-illustration_335657-2200.jpg",
    "last_modified": "2023-12-01T21:04:44+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-11-17-data-quality/",
    "title": "Data Quality",
    "description": "In the following article you will find the definition of data quality, what the domains are and how to quickly implement a solution.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://r0mymendez.github.io/romymendezblog/"
      }
    ],
    "date": "2023-11-12",
    "categories": [
      "Data",
      "Python"
    ],
    "contents": "\n\nContents\nIntroducción\nWhat is Data quality?\nData quality dimensions - Use case\nPython Frameworks\nDifferences between Licencia Apache 2.0 y MIT\nDataset\n🟢 Pandera\nInstall pandera\nImplementation Example\n\n🟠 Great Expectations\nInstall great expectation\nImplementation example\n\nIf you want to learn…\n\n\nIntroducción\nIn the current digital environment, the amount of available data is overwhelming.\nHowever, the true cornerstone for making informed decisions lies in the quality of this data.\nIn this article, we will explore the crucial importance of data quality, analyzing the inherent challenges that organizations face in managing information.\nAlthough often overlooked, data quality plays a fundamental role in the reliability and usefulness of the information that underpins our strategic decisions.\nWhat is Data quality?\nData quality measures how well a dataset complies with the criteria of accuracy, completeness, validity, consistency, uniqueness, timeliness, and fitness for purpose, and is fundamental for all data governance initiatives within an organization.\nData quality standards ensure that companies make decisions based on data to achieve their business objectives.\nsource: IBM\n\nsource: DataCamp cheat sheet\nThe following table highlights the various domains of data quality, from accuracy to fitness, providing an essential guide for assessing and enhancing the robustness of datasets:\nDimensions\nDescription\n🎯 Accuracy\nData accuracy, or how close data is to reality or truth. Accurate data is that which faithfully reflects the information it seeks to represent.\n🧩 Completeness\nMeasures the entirety of the data. A complete dataset is one that has no missing values or significant gaps. Data integrity is crucial for gaining a comprehensive and accurate understanding.\n✅ Validity\nIndicates whether the data conforms to defined rules and standards. Valid data complies with the established constraints and criteria for a specific dataset..\n🔄 Consistency\nRefers to the uniformity of data over time and across different datasets. Consistent data does not exhibit contradictions or discrepancies when compared with each other\n📇 Uniqueness\nEvaluates whether there are no duplicates in the data. Unique data ensures that each entity or element is represented only once in a dataset\n⌛Timeliness\nRefers to the timeliness of data. Timely information is that which is available when needed, without unnecessary delays.\n🏋️ Fitness\nThis aspect evaluates the relevance and usefulness of data for the intended purpose. Data should be suitable and applicable to the specific objectives of the organization or analysis being conducted.\nData quality dimensions - Use case\nNext, we provide an example where some issues with an e-commerce-based use case can be observed.\nID Transacción\nID Cliente\nProducto\nCantidad\nPrecio Unitario\nTotal\n⚪ 1\n10234\nLaptop HP\n1\n$800\n$800\n🟣 2\n\nWireless Headphones\n2\n$50\n$100\n🔵 3\n10235\nSmartphone\n-1\n$1000\n-$1000\n🟢 4\n10236\nWireless Mouse\n3\n$30\n$90\n🟢 4\n10237\nWireless Keyboard\n2\n$40\n$80\n🟣 Row 2 (Completeness): Row 2 does not comply with data integrity (Completeness) as the customer ID is missing.\nCustomer information is incomplete, making it challenging to trace the transaction back to a specific customer.\n🔵 Row 3 (Accuracy and Consistency): Row 3 exhibits accuracy (Accuracy) and consistency (Consistency) issues.\nThe quantity of products is negative, which is inaccurate and goes against the expected consistency in a transaction dataset.\n🟢 Row 4 (Uniqueness): The introduction of a second row with the same transaction ID (Transaction ID = 4) violates the uniqueness principle.\nEach transaction should have a unique identifier, and having two rows with the same Transaction ID creates duplicates, affecting the uniqueness of transactions.\n\nPython Frameworks\nThe following are some of the Python implementations carried out to perform data quality validations:\nFramework\nDescripción\nGreat Expectations\nGreat Expectations is an open-source library for data validation. It enables the definition, documentation, and validation of expectations about data, ensuring quality and consistency in data science and analysis projects\nPandera\nPandera is a data validation library for data structures in Python, specifically designed to work with pandas DataFrames. It allows you to define schemas and validation rules to ensure data conformity\nDora\nDora is a Python library designed to automate data exploration and perform exploratory data analysis.\nLet’s analyze some of the metrics that can be observed in their GitHub repositories, taking into account that the metrics were obtained on 2023-11-12.\nMetricas\nGreat Expectations\nPandera\nDora\n👥 Members\n399\n109\n106\n⚠️ Issues: Open\n112\n273\n1\n🟢 Issues: Close\n1642\n419\n7\n⭐ Stars\n9000\n2700\n623\n📺 Watching\n78\n17\n42\n🔎 Forks\n1400\n226\n63\n📬 Open PR\n43\n19\n0\n🐍 Version Python\n>=3.8\n>=3.7\nNo especificada\n📄 Version Number\n233\n76\n3\n📄 Last Version\n0.18.2\n0.17.2\n0.0.3\n📆 Last Date Version\n9 Nov 2023\n30 sep 2023\n30 jun 2020\n📄 Licence type\nApache-2.0 license\nMIT\nMIT\n📄 Languages\nPython 95.1%\nJupyter Notebook 4.3%\nJinja 0.4%\nJavaScript 0.1%\nCSS 0.1%\nHTML 0.0%\n\nPython 99.9%\nMakefile 0.1%\n\nPython100%\n\nDifferences between Licencia Apache 2.0 y MIT\nNotification of Changes:\nApache 2.0: Requires notification of changes made to the source code when distributing the software.\nMIT: Does not require specific notification of changes.\n\nCompatibility:\nApache 2.0: Known to be compatible with more licenses compared to MIT.\nMIT: Also quite compatible with various licenses, but Apache 2.0 License is often chosen in projects seeking greater interoperability with other licenses.\n\nAttribution:\nApache 2.0: Requires attribution and the inclusion of a copyright notice.\nMIT: Requires attribution to the original authorship but may have less strict requirements in terms of how that attribution is displayed.\n\nConsidering these currently analyzed metrics, let’s proceed with an example implementation using Pandera and Great Expectations.\n\nDataset\nFor the development of this example, we will use the dataset named ‘Tips.’ You can download the dataset from the followinge link.\nThe ‘tips’ dataset contains information about tips given in a restaurant, along with details about the total bill, the gender of the person who paid the bill, whether the customer is a smoker, the day of the week, and the meal’s time.\nColumn\nDescription\ntotal_bill\nThe total amount of the bill (including the tip).\ntip\nThe amount of tip given.\nsex\nThe gender of the bill payer (male or female).\nsmoker\nWhether the customer is a smoker or not.\nday\nThe day of the week when the meal was made.\ntime\nThe time of day (lunch or dinner).\nsize\nThe size of the group that shared the meal.\nBelow is a table with the first 5 rows of the dataset:\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n🟢 Pandera\nNext, we will provide an example of implementing Pandera using the dataset described earlier.\nInstall pandera\npip install pandas pandera \nImplementation Example\nImport pandas and pandera\nimport pandas as pd\nimport pandera as pa\nImport the dataframe file\npath = 'data/tips.csv'\ndata = pd.read_csv(path)\n\nprint(f\"Numero de columnas: {data.shape[1]}, Numero de filas: {data.shape[0]}\")\nprint(f\"Nombre de columnas: {list(data.columns)}\")\ndata.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   total_bill  244 non-null    float64\n 1   tip         244 non-null    float64\n 2   sex         244 non-null    object \n 3   smoker      244 non-null    object \n 4   day         244 non-null    object \n 5   time        244 non-null    object \n 6   size        244 non-null    int64  \ndtypes: float64(2), int64(1), object(4)\nmemory usage: 13.5+ KB\nNow, let’s create the schema object that contains all the validations we want to perform.\nYou can find additional validations that can be performed at the following link: <https://pandera.readthedocs.io/en/stable/dtype_validation.html>\nschema = pa.DataFrameSchema({\n  \"total_bill\": pa.Column(float, checks=pa.Check.le(50)),\n  \"tip\"       : pa.Column(float, checks=pa.Check.between(0,30)),\n  \"sex\"       : pa.Column(str, checks=[pa.Check.isin(['Female','Male'])]),\n  \"smoker\"    : pa.Column(str, checks=[pa.Check.isin(['No','Yes'])]),\n  \"day\"       : pa.Column(str, checks=[pa.Check.isin(['Sun','Sat'])]),\n  \"time\"      : pa.Column(str, checks=[pa.Check.isin(['Dinner','Lunch'])]),\n  \"size\"      : pa.Column(int, checks=[pa.Check.between(1,4)])\n})\nTo capture the error and subsequently analyze the output, it is necessary to catch it using an exception.\ntry:\n    schema(data).validate()\nexcept Exception as e:\n    print(e)\n    error = e\nSchema None: A total of 3 schema errors were found.\n\nError Counts\n------------\n- SchemaErrorReason.SCHEMA_COMPONENT_CHECK: 3\n\nSchema Error Summary\n--------------------\nschema_context column     check                     failure_cases  n_failure_cases\n                                                   \nColumn         day        isin(['Sun', 'Sat'])      [Thur, Fri]             2\n               size       in_range(1, 4)              [5, 6]                2\n               total_bill less_than_or_equal_to(50)   [50.81]               1\nBelow is a function that allows you to transform the output into a dictionary or a pandas dataframe\ndef get_errors(error, dtype_dict=True):\n    response = []\n\n \n    for item in range(len(error.schema_errors)):\n        error_item = error.schema_errors[item]\n        response.append(\n        {\n            'column'     :error_item.schema.name,\n            'check_error':error_item.schema.checks[0].error,\n            'num_cases'  :error_item.failure_cases.index.shape[0],\n            'check_rows' :error_item.failure_cases.to_dict()\n        })\n    \n    if dtype_dict:\n        return response\n    else:\n        return pd.DataFrame(response)\nget_errors(error,dtype_dict=True)\n[{'column': 'total_bill',\n  'check_error': 'less_than_or_equal_to(50)',\n  'num_cases': 1,\n  'check_rows': {'index': {0: 170}, 'failure_case': {0: 50.81}}},\n {'column': 'day',\n  'check_error': \"isin(['Sun', 'Sat'])\",\n  'num_cases': 81,\n  'check_rows': {'index': {0: 77,\n    1: 78,\n    2: 79,\n    3: 80,\n    4: 81,\n    5: 82,\n    6: 83,\n    7: 84,\n...\n    5: 156,\n    6: 185,\n    7: 187,\n    8: 216},\n   'failure_case': {0: 6, 1: 6, 2: 5, 3: 6, 4: 5, 5: 6, 6: 5, 7: 5, 8: 5}}}]\n🟠 Great Expectations\nGreat Expectations is an open-source Python-based library for validating, documenting, and profiling your data.\nIt helps maintain data quality and improve communication about data across teams.\n\nsource : <https://docs.greatexpectations.io/docs/>\nTherefore, we can describe Great Expectations as an open source tool designed to guarantee the quality and reliability of data in various sources, such as databases, tables, files and dataframes.\nIts operation is based on the creation of validation groups that specify the expectations or rules that the data must comply with.\nThe following are the steps that we must define when using this framework:\nDefinition of Expectations: Specify the expectations you have for the data.\nThese expectations can include simple constraints, such as value ranges, or more complex rules about data coherence and quality.\nConnecting to Data Sources: In this step, define the connections you need to make to various data sources, such as databases, tables, files, or dataframes.\nGeneration of Validation Suites: Based on the defined expectations, Great Expectations generates validation suites, which are organized sets of rules to be applied to the data.\nExecution of Validations: Validation suites are applied to the data to verify if they meet the defined expectations.\nThis can be done automatically in a scheduled workflow or interactively as needed.\nGeneration of Analysis and Reports: Great Expectations provides advanced analysis and reporting capabilities.\nThis includes detailed data quality profiles and reports summarizing the overall health of the data based on expectations.\nAlerts and Notifications: If the data does not meet the defined expectations, Great Expectations can generate alerts or notifications, allowing users to take immediate action to address data quality issues.\nTogether, Great Expectations offers a comprehensive solution to ensure data quality over time, facilitating early detection of problems and providing confidence in the integrity and usefulness of data used in analysis and decision-making\nInstall great expectation\n!pip install great_expectations==0.17.22 seaborn matplotlib numpy pandas\nImplementation example\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport re \n\nimport great_expectations as gx\nfrom ruamel.yaml import YAML\nfrom great_expectations.cli.datasource import sanitize_yaml_and_save_datasource\nfrom great_expectations.core.expectation_configuration import ExpectationConfiguration\n\nprint(f\"* great expectations version:{gx.__version__}\")\nprint(f\"* seaborn version:{sns.__version__}\")\nprint(f\"* numpy version:{np.__version__}\")\nprint(f\"* pandas:{pd.__version__}\")\n* great expectations version:0.17.22\n* seaborn version:0.13.0\n* numpy version:1.26.1\n* pandas:2.1.3\nImport dataset using great expectation\npath = 'data/tips.csv'\ndata_gx = gx.read_csv(path)\nList all available expectations by type\nlist_expectations = pd.DataFrame([item for item in dir(data_gx) if item.find('expect_')==0],columns=['expectation'])\nlist_expectations['expectation_type'] = np.select( [\n        list_expectations.expectation.str.find('_table_')>0, \n        list_expectations.expectation.str.find('_column_')>0,  \n        list_expectations.expectation.str.find('_multicolumn_')>0,\n    ],['table','column','multicolumn'],\n    default='other'\n)\n\nplt.figure(figsize=(20,6))\nsns.countplot(x=list_expectations.expectation_type)\nplt.show()\n\n\nIn the image, it can be observed that the available expectations are mainly applied to columns (for example: expect_column_max_to_be_between) and tables (for example: expect_table_columns_to_match_set), although an expectation based on the values of multiple columns can also be applied (for example: expect_multicolumn_values_to_be_unique).\n\nExpectations: Tables\n# The following list contains the columns that the dataframe must have:\ncolumns = ['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size']\ndata_gx.expect_table_columns_to_match_set(column_set = columns)\n{\n  \"success\": true,\n  \"result\": {\n    \"observed_value\": [\n      \"total_bill\",\n      \"tip\",\n      \"sex\",\n      \"smoker\",\n      \"day\",\n      \"time\",\n      \"size\"\n    ]\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\n# Now, we delete two columns, 'time' and 'size,' to validate the outcome\n\ncolumns = = ['total_bill', 'tip', 'sex', 'smoker', 'day']\ndata_gx.expect_table_columns_to_match_set(column_set = columns)\nIf we observe, the result is False, and in the details, they provide information about the columns that the dataframe has in addition to those expected.\n{\n  \"success\": false,\n  \"result\": {\n    \"observed_value\": [\n      \"day\",\n      \"sex\",\n      \"size\",\n      \"smoker\",\n      \"time\",\n      \"tip\",\n      \"total_bill\"\n    ],\n    \"details\": {\n      \"mismatched\": {\n        \"unexpected\": [\n          \"size\",\n          \"time\"\n        ]\n      }\n    }\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\nExpectations: Columns\nLet’s validate that there is a categorical value within a column\ndata_gx['total_bill_group'] = pd.cut(data_gx['total_bill'],\n                              bins=[0,10,20,30,40,50,float('inf')], \n                              labels=['0-10', '10-20', '20-30', '30-40', '40-50', '>50'],\n                              right=False, \n                              include_lowest=True)\n\n# Now, let's validate if 3 categories exist within the dataset\n\ndata_gx.expect_column_distinct_values_to_contain_set(column='total_bill_group',\n                                                      value_set=['0-10','10-20', '20-30'],\n                                                      result_format='BASIC') \n{\n  \"success\": true,\n  \"result\": {\n    \"observed_value\": [\n      \"0-10\",\n      \"10-20\",\n      \"20-30\",\n      \"30-40\",\n      \"40-50\",\n      \">50\"\n    ],\n    \"element_count\": 244,\n    \"missing_count\": null,\n    \"missing_percent\": null\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\nLet’s validate that the column does not have null values\ndata_gx.expect_column_values_to_not_be_null('sex')\n{\n  \"success\": true,\n  \"result\": {\n    \"element_count\": 244,\n    \"unexpected_count\": 0,\n    \"unexpected_percent\": 0.0,\n    \"unexpected_percent_total\": 0.0,\n    \"partial_unexpected_list\": []\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\nGreat Expectation Project\nNow, let’s generate a Great Expectations project to run a group of validations based on one or more datasets.\nInitialize the Great Expectations project:\n !yes Y | great_expectations init\n ___              _     ___                  _        _   _\n / __|_ _ ___ __ _| |_  | __|_ ___ __  ___ __| |_ __ _| |_(_)___ _ _  ___\n| (_ | '_/ -_) _` |  _| | _|\\ \\ / '_ \\/ -_) _|  _/ _` |  _| / _ \\ ' \\(_-<\n \\___|_| \\___\\__,_|\\__| |___/_\\_\\ .__/\\___\\__|\\__\\__,_|\\__|_\\___/_||_/__/\n                                |_|\n             ~ Always know what to expect from your data ~\n\nLet's create a new Data Context to hold your project configuration.\n\nGreat Expectations will create a new directory with the following structure:\n\n    great_expectations\n    |-- great_expectations.yml\n    |-- expectations\n    |-- checkpoints\n    |-- plugins\n    |-- .gitignore\n    |-- uncommitted\n        |-- config_variables.yml\n        |-- data_docs\n        |-- validations\n\nOK to proceed? [Y/n]: \n================================================================================\n\nCongratulations! You are now ready to customize your Great Expectations configuration.\n\nYou can customize your configuration in many ways. Here are some examples:\n\n  Use the CLI to:\n    - Run `great_expectations datasource new` to connect to your data.\n    - Run `great_expectations checkpoint new <checkpoint_name>` to bundle data with Expectation Suite(s) in a Checkpoint for later re-validation.\n    - Run `great_expectations suite --help` to create, edit, list, profile Expectation Suites.\n    - Run `great_expectations docs --help` to build and manage Data Docs sites.\n\n  Edit your configuration in great_expectations.yml to:\n    - Move Stores to the cloud\n    - Add Slack notifications, PagerDuty alerts, etc.\n    - Customize your Data Docs\n\nPlease see our documentation for more configuration options!\nCopy data into the ‘great_expectations’ folder generated from the project initialization\n!cp -r data gx\n# Let's print the contents of the folder\n\ndef print_directory_structure(directory_path, indent=0):\n    current_dir = os.path.basename(directory_path)\n    print(\"    |\" + \"    \" * indent + f\"-- {current_dir}\")\n    indent += 1\n    with os.scandir(directory_path) as entries:\n        for entry in entries:\n            if entry.is_dir():\n                print_directory_structure(entry.path, indent)\n            else:\n                print(\"    |\" + \"    \" * indent + f\"-- {entry.name}\")\n\n\nprint_directory_structure('gx')\n    |-- gx\n    |    -- great_expectations.yml\n    |    -- plugins\n    |        -- custom_data_docs\n    |            -- renderers\n    |            -- styles\n    |                -- data_docs_custom_styles.css\n    |            -- views\n    |    -- checkpoints\n    |    -- expectations\n    |        -- .ge_store_backend_id\n    |    -- profilers\n    |    -- .gitignore\n    |    -- data\n    |        -- tips.csv\n    |    -- uncommitted\n    |        -- data_docs\n    |        -- config_variables.yml\n    |        -- validations\n    |            -- .ge_store_backend_id\nHere are some clarifications about the files and folders generated in this directory:\nFiles/Folders\nDescription\n📄 great_expectations.yml\nThis file contains the main configuration of the project. Details such as storage locations and other configuration parameters are specified here\n📂 plugins\ncustom_data_docs:\n📄renderers: It contains custom renderers for data documents.\n📄 styles: It includes custom styles for data documents, such as CSS style sheets (data_docs_custom_styles.css).\n📄 views: It can contain custom views for data documents.\n\n📂 checkpoints\nThis folder could contain definitions of checkpoints, which are points in the data flow where specific validations can be performed.\n📂 expectations\nThis is where the expectations defined for the data are stored. This directory may contain various subfolders and files, depending on the project’s organization.\n📂 profilers\nIt can contain configurations for data profiles, which are detailed analyses of data statistics.\n📄 .gitignore\nIt is a Git configuration file that specifies files and folders to be ignored when performing tracking and commit operations. (commit)\n📂 data\nIt contains the data used in the project, in this case, the file tips.csv.\n📂 uncommitted\n📂data_docs: Folder where data documents are generated.\n📄config_variables.yml: Configuration file that can contain project-specific variables\n📂validations: It can contain results of validations performed on the data.\n\nConfiguration of datasource and data connectors:\nDataSource: It is the data source used (can be a file, API, database, etc.).\nData Connectors: These are the connectors that facilitate the connection to data sources and where access credentials, location, etc., should be defined.\n\ndatasource_name_file = 'tips.csv'\ndatasource_name = 'datasource_tips'\ndataconnector_name = 'connector_tips'\n# Let's create the configuration for the datasource\n\ncontext = gx.data_context.DataContext()\nmy_datasource_config = f\"\"\"\n    name: {datasource_name}\n    class_name: Datasource\n    execution_engine:\n      class_name: PandasExecutionEngine\n    data_connectors:\n      {dataconnector_name}:\n        class_name: InferredAssetFilesystemDataConnector\n        base_directory: data\n        default_regex:\n          group_names:\n            - data_asset_name\n          pattern: (.*)\n      default_runtime_data_connector_name:\n        class_name: RuntimeDataConnector\n        assets:\n          my_runtime_asset_name:\n            batch_identifiers:\n              - runtime_batch_identifier_name\n\"\"\"\n\nyaml = YAML()\ncontext.add_datasource(**yaml.load(my_datasource_config))\nsanitize_yaml_and_save_datasource(context, my_datasource_config, overwrite_existing=True)\nConfiguration of the expectations\n\nIn the following code snippet, the configuration of three expectations is presented.\nIn particular, the last one includes a parameter called ‘mostly’ with a value of 0.75.\nThis parameter indicates that the expectation can fail in up to 25% of cases, as by default, 100% compliance is expected unless specified otherwise.\nAdditionally, an error message can be specified in markdown format, as shown in the last expectation.\n\nexpectation_configuration_table =  ExpectationConfiguration(\n   expectation_type=\"expect_table_columns_to_match_set\",\n      kwargs= {\n        \"column_set\": ['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size']\n      },\n      meta= {}\n)\n\nexpectation_configuration_total_bill = ExpectationConfiguration(\n      expectation_type= \"expect_column_values_to_be_between\",\n      kwargs= {\n        \"column\": \"total_bill\",\n        \"min_value\": 0,\n        \"max_value\": 100\n      },\n      meta= {}\n)\n\n\nexpectation_configuration_size = ExpectationConfiguration(\n   expectation_type=\"expect_column_values_to_not_be_null\",\n   kwargs={\n      \"column\": \"size\",\n      \"mostly\": 0.75,\n   },\n   meta={\n      \"notes\": {\n         \"format\": \"markdown\",\n         \"content\": \"Expectation to validate column `size` does not have null values.\"\n      }\n   }\n)\nCreation of the expectation suite\nexpectation_suite_name = \"tips_expectation_suite\"\nexpectation_suite = context.create_expectation_suite(\n    expectation_suite_name=expectation_suite_name, \n    overwrite_existing=True\n)\n\n# Add expectations\nexpectation_suite.add_expectation(expectation_configuration=expectation_configuration_table)\nexpectation_suite.add_expectation(expectation_configuration=expectation_configuration_total_bill)\nexpectation_suite.add_expectation(expectation_configuration=expectation_configuration_size)\n\n# save expectation_suite\ncontext.save_expectation_suite(expectation_suite=expectation_suite, \n                               expectation_suite_name=expectation_suite_name)\ndata-quality/gx/expectations/tips_expectation_suite.json\n\nWithin the ‘expectations’ folder, a JSON file is created with all the expectations generated earlier.\n\nConfiguration of the checkpoints\ncheckpoint_name ='tips_checkpoint'\n\nconfig_checkpoint = f\"\"\"\n    name: {checkpoint_name}\n    config_version: 1\n    class_name: SimpleCheckpoint\n    expectation_suite_name: {expectation_suite_name}\n    validations:\n      - batch_request:\n          datasource_name: {datasource_name}\n          data_connector_name: {dataconnector_name}\n          data_asset_name: {datasource_name_file}\n          batch_spec_passthrough:\n            reader_method: read_csv\n            reader_options: \n              sep: \",\"\n          data_connector_query:\n            index: -1\n        expectation_suite_name: {expectation_suite_name}\n\"\"\"\n\n# Validate if the YAML structure is correct\ncontext.test_yaml_config(config_checkpoint)\n\n# Add the checkpoint to the generated context\ncontext.add_checkpoint(**yaml.load(config_checkpoint)) \nExecute the checkpoint to validate all the configured expectations on the dataset\nresponse = context.run_checkpoint(checkpoint_name=checkpoint_name)\nTo observe the result obtained from the validations, it can be converted to JSON\n response.to_json_dict()\n{'run_id': {'run_name': None, 'run_time': '2023-11-12T20:39:23.346946+01:00'},\n 'run_results': {'ValidationResultIdentifier::tips_expectation_suite/__none__/20231112T193923.346946Z/722b2e93e32fd7222c8ad9339f3e0e1d': {'validation_result': {'success': True,\n    'results': [{'success': True,\n      'expectation_config': {'expectation_type': 'expect_table_columns_to_match_set',\n       'kwargs': {'column_set': ['total_bill',\n         'tip',\n         'sex',\n         'smoker',\n         'day',\n         'time',\n         'size'],\n        'batch_id': '722b2e93e32fd7222c8ad9339f3e0e1d'},\n       'meta': {}},\n      'result': {'observed_value': ['total_bill',\n        'tip',\n        'sex',\n        'smoker',\n        'day',\n        'time',\n        'size']},\n      'meta': {},\n      'exception_info': {'raised_exception': False,\n       'exception_traceback': None,\n       'exception_message': None}},\n     {'success': True,\n...\n  'notify_on': None,\n  'default_validation_id': None,\n  'site_names': None,\n  'profilers': []},\n 'success': True}\nNow, let’s obtain the results\n context.open_data_docs()\n\nBy executing this code chunk, an HTML file with the results of the validations will open at gx/uncommitted/data_docs/local_site/validations/tips_expectation_suite/__none__/20231112T192529.002401Z/722b2e93e32fd7222c8ad9339f3e0e1d.html\n\n\n\n\nIf you want to learn…\nPandera Documentación Oficial\nPandera: Statistical Data Validation of Pandas Dataframes - Researchgate\nGreat Expectation Documentación Oficial\nData Quality Fundamentals Book O’relly\nGreat Expectation Yoututbe Channel\n\n\n\n",
    "preview": "https://img.freepik.com/free-vector/statistics-concept-illustration_114360-4254.jpg",
    "last_modified": "2023-11-17T21:19:52+01:00",
    "input_file": {}
  }
]
