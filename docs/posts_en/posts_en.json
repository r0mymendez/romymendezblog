[
  {
    "path": "posts_en/2024-04-02-simplify-database-migrations-using-python-with-alembic/",
    "title": "Simplify Database Migrations using Python with Alembic",
    "description": "In this article, you will discover how to use Alembic for database migration in üêçPython.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2024-04-02",
    "categories": [
      "Python",
      "Database"
    ],
    "contents": "\n\nContents\nüü£ ORM (Object Relational Mapping)\nüü£ SQLAlchemy\nüü£ Database migrations\nüü£ What are the benefits of using migrations?\n\nüü£ What is Alembic?\nüü£ What is Audit alembic?\n‚öôÔ∏è Alembic Tutorial\nüîß Prerequisites\n\nQuick Start\nüîß Create a postgres database\nüîß Alembic: Configuration\nüîß Alembic: Create our first migration\n\nüìö References\n\n\nThe following article will discuss working with database migration using the popular Python library called Alembic.\nHowever, before diving into this tutorial, we‚Äôll mention some definitions that will help us better understand how Alembic works and the best way to implement database migration.\nüü£ ORM (Object Relational Mapping)\nThe ORM (Object Relational Mapping) is a layer that allows connecting object-oriented programming with relational databases, abstracting the underlying SQL queries.\n\nüü£ SQLAlchemy\nSQLAlchemy is a üêç python library that implements ORM and allows you to perform different actions on a related database.\n\nThe following are the key components of SQLAlchemy to understand how it interacts with the database:\nüü£ Engine: It is the interface that allows interaction with the database.\nIt handles connections and executes queries.\nüü£ Pool: It is a collection of connections that allows reusing connections and improving query performance by reducing time.\nüü£ Dialect: It is the component that allows interaction with the database.\nEach dialect is designed to interact and translate queries for a database; By default, this library has dialects for MySQL, MariaDB, PostgreSQL, SQL Server, and Oracle.\nBut there are external dialects, in which you should import other libraries, which you can see in the following image.\nüü£ DBAPI: It is the interface that provides methods to enable communication between Python and the database.\n\nBelow is a simple example of how to execute a query in SQL and SQLAlchemy:\nSELECT \n    customer_id, \n    customer_unique_id,\n    customer_zip_code_prefix, \n    customer_city, \n    customer_state \nFROM ecommerce.customers \nLIMIT 10;\nfrom sqlalchemy import Column, Integer, String,create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\n# Create a base class which allows us to define classes that map to tables\nBase = declarative_base()\n\n# Define the class that maps to the table\nclass Customer(Base):\n    __tablename__ = 'customers'  # Cambia a min√∫sculas\n    __table_args__ = {'schema': 'ecommerce'}\n\n    customer_id = Column(Integer, primary_key=True)\n    customer_unique_id = Column(Integer)\n    customer_zip_code_prefix= Column(Integer)\n    customer_city= Column(String)\n    customer_state = Column(String)\n\n# Create an engine that connects to the PostgreSQL server\nconn ='postgresql://postgres:postgres@localhost/postgres'\nengine = create_engine(conn)\n\n# Create a session\nconn_session = sessionmaker(bind=engine)\nsession = conn_session()\n\n# Execute the query\ncustomers = session.query(Customer).limit(10)\n\n# Extract the data and create a list of tuples\ndata = [ (customer.customer_id,\n          customer.customer_unique_id,\n          customer.customer_zip_code_prefix,\n          customer.customer_city,\n          customer.customer_state) \n    for customer in customers]\nüü£ Database migrations\nA migration is the process that allows you to modify the structure of the database, these migrations are created to maintain consistency and integrity.\nüü£ What are the benefits of using migrations?\n\nüü£ Version Control: Avoids manual intervention in the database by maintaining control over schema versions.\nüü£ Environment Management: Facilitates the creation of new environments through the application of migrations, enabling easy reproduction of specific configurations and maintaining coherence between them.\nüü£ Upgrade & Downgrade: Another benefit is the ability not only to apply changes but also to revert them.\nThis provides flexibility and security in database management.\nüü£ Auditing: Alembic-audit is another library that can be implemented to maintain a chronological record of changes made to the database, facilitating traceability.\nüü£ CI/CD Integration: Easily integrates into CI/CD pipelines to apply database changes automatically, streamlining and ensuring consistency in application deployment.\nüü£ Standardization: This implementation enables cleaner, structured, and coherent development for defining and applying changes to the database schema.\nBy using templates, script reuse is promoted, ensuring efficient and consistent management of database changes.\nüü£ What is Alembic?\nAlembic is a üêçPython library that enables controlled and automated database migrations.\nThis library utilizes SQLAlchemy and it allows for the management of changes in the database schema through scripts, which describe the modifications and can be applied automatically.\n\nüü£ What is Audit alembic?\n¬†Audit Alembic is a üêçPython library that complements Alembic by providing an audit table with a detailed record of applied changes.\nWhile Alembic typically maintains only a table in the database with the ID of the last applied migration and allows tracking files using the history command, Audit Alembic goes a step further by creating an additional table that facilitates change tracking and enables the addition of metadata to applied transactions.\ncolumn_name\ncolumn_description\nid\nunique identifier\nalembic_version\nversion of the migration\nprev_alembic_version\nprevious version of the migration\noperation\n‚Äúmigration‚Äù or ‚Äúrollback‚Äù\noperation_direction\ntype of operation (upgrade or downgrade)\nuser_verion\nuser version of the migration in our case we are using the timestamp\nchanged_at\ntimestamp of the migration\n‚öôÔ∏è Alembic Tutorial\nYou can find the complete code with a step-by-step example in the üêç Python notebook in this link.\nHowever, I will provide a brief overview of the main commands in rest sections of this post.\nFor detailed commands and the implementation of Audit Alembic, please refer to the notebook.\n\n\nFeel free to check it out and give it a star if you find it helpful!\n‚≠êÔ∏è\n\nüîß Prerequisites\nüê≥ Docker\nüêô Docker Compose\nüêç Install python libraries: !pip install alembic Audit-Alembic\nQuick Start\nüîß Create a postgres database\n1Ô∏è‚É£ - Create docker-compose.yml file\n    version: \"3.7\"\n    services:\n      db:\n        image: postgres:13.3-alpine\n        volumes:\n          - ./db_data:/var/lib/postgresql/data\n        environment:\n          POSTGRES_USER: postgres\n          POSTGRES_PASSWORD: postgres\n          POSTGRES_DB: postgres\n        ports:\n          - \"5433:5432\"\n\n    volumes:\n      db_data:\n2Ô∏è‚É£ - Create postgres database Execute in the terminal: docker-compose -f docker-compose.yml up --build\n3Ô∏è‚É£ - Check if your container is running Execute in the terminal: docker ps\nCONTAINER ID   IMAGE     COMMAND                  CREATED             STATUS             PORTS                                       NAMES\nedb1f7077e66   sqla_db   \"docker-entrypoint.s‚Ä¶\"   About an hour ago   Up About an hour   0.0.0.0:5433->5432/tcp, :::5433->5432/tcp   sqla_db_1\nüîß Alembic: Configuration\n1Ô∏è‚É£ - Create a new Alembic environment Now, we are going to create a new Alembic project.\nFor this reason, we need to execute the following command, which will create a group of directories and files.\nalembic init project\n2Ô∏è‚É£ - Files & New directory\nfile_name\nDescription\nüìÑalembic.ini\nThis file is the main configuration file for Alembic, containing the configuration settings for the Alembic environment.\nüìÅproject\\verions\nThis directory is where the migration scripts will be stored.\nüìÑproject\\env.py\nThis Python script contains the function for executing the migration scripts.\nüìÑproject\\script.py.mako\nThis file is the template for generating new migration scripts.\nüìÑproject\\README\nThis file contains a short description of the directory.\n3Ô∏è‚É£ - Add the database connection In the alembic.ini file, add the database connection string to the sqlalchemy.url variable.\nThe connection string should be in the format:\n sqlalchemy.url = driver://user:pass@localhost/dbname \nIn my case I need to configure the following connection * driver: postgresql * user: postgres * password: postgres * dbname: postgres\nsqlalchemy.url = postgresql://postgres:postgres@localhost:5433/postgres \n4Ô∏è‚É£ - File name template We can uncomment the following line in the alembic.ini file to change the name of the files created by Alembic, ensuring a chronological order of the files created.\nfile_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s \nüîß Alembic: Create our first migration\n1Ô∏è‚É£ - Create a migration script\nalembic revision -m \"create schema ecommerce\"\nNow you can see the new file created in the project/version folder, the file has the following name:\n{current_timestamp}-{unique_identifier}_create_schema_ecommerce.py\ncurrent timestamp\nUnique identifier, in my case is 9ec3d7e4bde9\nThe message that I added in the command, only change the space for a underscore.\n2Ô∏è‚É£ - Modify the migration file In our case, we will create the schema for the ecommerce project.\nHowever, Alembic does not have a specific method for this task.\nTherefore, we will use the op.execute method to execute the SQL query that will create the schema.\n# alembic does not support creating schema directly and we need to use op.execute\ndef upgrade() -> None:\n    op.execute('CREATE SCHEMA IF NOT EXISTS ecommerce_olist;')\n\n\ndef downgrade() -> None:\n    op.execute('DROP SCHEMA IF EXISTS ecommerce_olist CASCADE;')\n3Ô∏è‚É£ - Execute the migration The following command will execute the migration and create the schema in the database.\nIf you see the message ‚ÄúDone,‚Äù the migration was successful.\nYou can also check the database to verify that the new schema was created and the Alembic version table was updated.\nalembic upgrade head\n4Ô∏è‚É£ - Check the migrations Now, we can verify the current migration that was executed.\nThis can be controlled using the Alembic command or by checking the table created earlier.\nalembic current\nINFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\nINFO  [alembic.runtime.migration] Will assume transactional DDL.\nb1bc43e2f536 (head)\n5Ô∏è‚É£ - History of migrations Additionally, we can check all the migrations that were executed in this project by using the following command.\nThis will display a detailed history of the migrations, including revision IDs, parent revisions, paths to migration files, and descriptions of the changes made in each migration.\nalembic history --verbose\nRev: b1bc43e2f536 (head)\nParent: 784a7abb86b7\nPath: /project/versions/2024_04_01_1719-b1bc43e2f536_create_table_customer.py\n\n    create table customer\n    \n    Revision ID: b1bc43e2f536\n    Revises: 784a7abb86b7\n    Create Date: 2024-04-01 17:19:09.844065\n\nRev: 784a7abb86b7\nParent: <base>\nPath: /project/versions/2024_04_01_1718-784a7abb86b7_create_schema_ecommerce.py\n\n    create schema ecommerce\n    \n    Revision ID: 784a7abb86b7\n    Revises: \n    Create Date: 2024-04-01 17:18:06.680872\n6Ô∏è‚É£ - Downgrade the migration The following code allows you to revert the changes made previously.\nalembic downgrade -1\nAs mentioned earlier, don‚Äôt forget to check out my repository containing the step-by-step guide and the implementation of Audit Alembic, allowing you to have a table with traceability of changes.\nIf you find it useful, you can leave a star ‚≠êÔ∏è.\nRemember, it‚Äôs always a good development practice to have a tool that applies changes to your database, ensuring coherence, avoiding manual tasks, and enabling quick reproduction of new environments.\nüìö References\nIf you want to learn‚Ä¶\n1.Alembic official documentation\n2.sqlalchemy\n3.Audit-Alembic\nOther references:\n- Image preview reference: [Imagen de Freepik]\n\n\n\n",
    "preview": "posts_en/2024-04-02-simplify-database-migrations-using-python-with-alembic/preview.jpg",
    "last_modified": "2024-04-02T01:39:56+02:00",
    "input_file": "simplify-database-migrations-using-python-with-alembic.knit.md"
  },
  {
    "path": "posts_en/2024-02-12-learning-aws-s3-on-localhost-best-practices-with-boto3-and-localstack/",
    "title": "Learning AWS S3 on Localhost: Best Practices with Boto3 and LocalStack",
    "description": "In this article, you will discover new features of **S3** and learn how to implement some of them using Boto3 in üêçPython.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2024-02-12",
    "categories": [
      "Python",
      "Cloud Computing"
    ],
    "contents": "\n\nContents\nüî∏ What is AWS s3?\nüìô Multiple Use Cases for S3\nüìô S3 storage type\nüìô Object tagging\nüìô S3 Inventory\n\nüìô What is boto3?\nüìò What is localstack?\nüìô Boto3 & üìò LocalStudio\nPrerequisites\n\nüü£ Build and run the Docker Compose environment\nüöÄ Using LocalStack with Boto3: A Step-by-Step Guide\nüõ†Ô∏è Install Boto3\nüõ†Ô∏è Create a session using the localstack endpoint\nüõ†Ô∏è Create new buckets\nüìã List all buckets\nüì§ Upload the JSON file to s3\nüìã List all objects\nüìÑ Upload multiple CSV files to s3\nüìÑ Read csv file from s3\nüè∑Ô∏è Add tags to the bucket\nüîÑ Versioning in the bucket\nüóëÔ∏è Create a static site using s3 bucket\n\nüìö References\n\n\nIn this article, you will discover new features of S3 and learn how to implement some of them using Boto3 in üêçPython.\nAdditionally, you will deploy a Localstack container to explore these functionalities without the need to use a credit card.\nIüî∏ What is AWS s3?\nAWS S3, or Simple Storage Service, is a core service of AWS that serves as a primary storage solution.\nWhile it is commonly known for storing objects, it offers a wide range of functionalities beyond basic storage.\nUnderstanding these features can significantly enhance the utilization of this service.\nSome of the main features of Amazon S3 include:\nüåê Web accessibility via API or HTTPS, allowing for easy access and integration with web applications.\nüîÑ Object versioning, which enables the creation of copies of objects, providing additional data protection and recovery options.\nüîí Policy creation and application at the bucket level, enhancing security by controlling access to resources and defining permissions.\nüìâ Low storage costs and serverless architecture, providing cost-effective and scalable storage solutions with virtually unlimited capacity.\n\n#¬†‚ú® Exploring 8 Key Features of Amazon S3\nüìô Multiple Use Cases for S3\nWhile S3 is commonly associated with file storage, such as CSV, JSON, or Parquet files, it offers a wide range of other use cases as well.\nThese include hosting static websites, sharing files, storing data for machine learning models, application configuration, and logging purposes.\n\nüìô S3 storage type\nIn the following image, you can see the types of storage that S3 allows, which depend on the frequency of accessing the object.\nIf you need to access objects frequently, it‚Äôs advisable to use the standard storage type.\nOn the other hand, if access to objects is less frequent, it‚Äôs recommended to use S3 üßä Glacier services.\n\nüìô Object tagging\nAnother important feature of S3 is the ability to use tags, this tags are useful for classifying objects and managing costs.\nThey also serve as a strategy for data protection, allowing you to label objects with categories such as confidentiality or sensitive data, and apply policies accordingly.\nAdditionally, S3 supports using tags to trigger Lambda functions, enabling you to perform various actions based on these labels.\n\nüìô S3 Inventory\nWhen managing an S3 bucket, it‚Äôs common to accumulate a large number of files within the same bucket.\nTo efficiently organize and manage these files, it‚Äôs often necessary to generate an inventory.\nIn S3, an inventory is a file that can be scheduled for updates and contains information about the objects stored in the bucket, including their type, size, and other metadata.\nThis allows for better organization and management of objects within the bucket.\n\n##¬†üìô S3 Lifecycle configuration\nComprising a set of rules, the S3 lifecycle configuration dictates actions applied by AWS S3 to a group of objects.\nIn the following image, you can observe the typical actions that can be configured.\n\n##¬†üìô S3 Batch operators\nIt is a feature provided by S3 that allows users to perform operations on objects stored in buckets.\nWith S3 Batch Operations, users can automate tasks such as copying, tagging, deleting, and restoring objects.\nThis feature is particularly useful for organizations managing large amounts of data in S3 and need to perform these operations at scale efficiently.\n\n##¬†üìô S3 Query Select\nS3 Select is a feature provided by S3 that enables users to find some data from objects stored in S3 buckets using simple SQL queries.\nWith S3 Select, users can efficiently query large datasets stored in various formats such as CSV, JSON, and Parquet, without the need to download and process the entire object.\nThis feature is particularly useful for applications that require selective access to data stored in S3, as it minimizes data transfer and processing overhead.\n\n##¬†üìô S3 Storage Lens\nS3 Storage Lens is a feature that offers a centralized dashboard with customizable reports and visualizations, allowing users to monitor key metrics such as storage usage, access patterns, and data transfer costs.\nAlso it provides detailed metrics, analytics, and recommendations to help organizations optimize their S3 storage resources, improve data security, and reduce costs.\n\nüìô What is boto3?\nBoto3 is a üêç Python library that allows the integration with AWS services, facilitating various tasks such as creation, management, and configuration of these services.\nThere are two primary implementations within Boto3: * Resource implementation: provides a higher-level, object-oriented interface, abstracting away low-level details and offering simplified interactions with AWS services.\n* Client implementation: offers a lower-level, service-oriented interface, providing more granular control and flexibility for interacting with AWS services directly.\nüìò What is localstack?\nLocalstack is a platform that provides a local version of several cloud services, allowing you to simulate a development environment with AWS services.\nThis allows you to debug and refine your code before deploying it to a production environment.\nFor this reason, Localstack is a valuable tool for emulating essential AWS services such as object storage and message queues, among others.\nAlso, Localstack serves as an effective tool for learning to implement and deploy services using a Docker container without the need for an AWS account or the use of your credit card.\nIn this tutorial, we create a Localstack container to implement the main functionalities of S3 services.\n\nüìô Boto3 & üìò LocalStudio\nAs mentioned earlier, LocalStudio provides a means to emulate a local environment for Amazon with some of the most popular services.\nThis article will guide you through the process of creating a container using the LocalStudio image.\nSubsequently, it will demonstrate the utilization of Boto3 to create an S3 bucket and implement key functionalities within these services.\nBy the end of this tutorial, you‚Äôll have a clearer understanding of how to seamlessly integrate Boto3 with LocalStudio, allowing you to simulate AWS services locally for development and testing purposes.\nPrerequisites\nBefore you begin, ensure that you have the following installed:\nüê≥ Docker\nüêô Docker Compose\nüü£ Build and run the Docker Compose environment\nClone the repository > Feel free to check it out and give it a star if you find it helpful! ‚≠êÔ∏è\n\n{% github r0mymendez/LocalStack-boto3 %}\ngit clone https://github.com/r0mymendez/LocalStack-boto3.git\ncd LocalStack-boto3\nBuild an run the docker compose\n\n docker-compose -f docker-compose.yaml up --build\nRecreating localstack ... done\nAttaching to localstack\nlocalstack    | LocalStack supervisor: starting\nlocalstack    | LocalStack supervisor: localstack process (PID 16) starting\nlocalstack    | \nlocalstack    | LocalStack version: 3.0.3.dev\nlocalstack    | LocalStack Docker container id: f313c21a96df\nlocalstack    | LocalStack build date: 2024-01-19\nlocalstack    | LocalStack build git hash: 553dd7e4\nü•≥Now we have in Localtcak running in localhost!\nüöÄ Using LocalStack with Boto3: A Step-by-Step Guide\nüõ†Ô∏è Install Boto3\n!pip install boto3\nüõ†Ô∏è Create a session using the localstack endpoint\nThe following code snippet initializes a client for accessing the S3 service using the LocalStack endpoint.\nimport boto3\nimport json \nimport requests\nimport pandas as pd\nfrom datetime import datetime\nimport io\nimport os\n\n\ns3 = boto3.client(\n    service_name='s3',\n    aws_access_key_id='test',\n    aws_secret_access_key='test',\n    endpoint_url='http://localhost:4566',\n)\nüõ†Ô∏è Create new buckets\nBelow is the code snippet to create new buckets using the Boto3 library\n# create buckets\nbucket_name_news = 'news'\nbucket_name_config = 'news-config'\n\ns3.create_bucket(Bucket= bucket_name_new )\ns3.create_bucket(Bucket=bucket_name_config)\nüìã List all buckets\nAfter creating a bucket, you can use the following code to list all the buckets available at your endpoint.\n# List all buckets\nresponse = s3.list_buckets()\npd.json_normalize(response['Buckets'])\n\nüì§ Upload the JSON file to s3\nOnce we extract data from the API to gather information about news topics, the following code generates a JSON file and uploads it to the S3 bucket previously created.\n# invoke the config news\nurl = 'https://ok.surf/api/v1/cors/news-section-names' \nresponse = requests.get(url)\nif response.status_code==200:\n    data = response.json()\n    # ad json file to s3\n    print('data', data)\n    # upload the data to s3\n    s3.put_object(Bucket=bucket_name_config, Key='news-section/data_config.json', Body=json.dumps(data))\ndata ['US', 'World', 'Business', 'Technology', 'Entertainment', 'Sports', 'Science', 'Health']\nüìã List all objects\nNow, let‚Äôs list all the objects stored in our bucket.\nSince we might have stored a JSON file in the previous step, we‚Äôll include code to retrieve all objects from the bucket.\ndef list_objects(bucket_name):\n    response = s3.list_objects(Bucket=bucket_name)\n    return pd.json_normalize(response['Contents'])\n\n# list all objects in the bucket\nlist_objects(bucket_name=bucket_name_config)\n\nüìÑ Upload multiple CSV files to s3\nIn the following code snippet, we will request another method from the API to extract news for each topic.\nSubsequently, we will create different folders in the bucket to save CSV files containing the news for each topic.\nThis code enables you to save multiple files in the same bucket while organizing them into folders based on the topic and the date of the data request.\n# Request the news feed API Method\nurl = 'https://ok.surf/api/v1/news-feed' \nresponse = requests.get(url)\nif response.status_code==200:\n    data = response.json()\n\n# Add the json file to s3\nfolder_dt =  f'dt={datetime.now().strftime(\"%Y%m%d\")}'\n\nfor item in data.keys():\n    tmp = pd.json_normalize(data[item])\n    tmp['section'] = item   \n    tmp['download_date'] = datetime.now()\n    tmp['date'] = pd.to_datetime(tmp['download_date']).dt.date\n    path = f\"s3://{bucket_name_news}/{item}/{folder_dt}/data_{item}_news.csv\"\n\n    # upload multiple files to s3\n    bytes_io = io.BytesIO()\n    tmp.to_csv(bytes_io, index=False)\n    bytes_io.seek(0)\n    s3.put_object(Bucket=bucket_name_news, Key=path, Body=bytes_io)\n\n# list all objects in the bucket\nlist_objects(bucket_name=bucket_name_news)\n\nüìÑ Read csv file from s3\nIn this section, we aim to read a file containing news about technology topics from S3.\nTo accomplish this, we first retrieve the name of the file in the bucket.\nThen, we read this file and print the contents as a pandas dataframe.\n# Get the technology file\nfiles = list_objects(bucket_name=bucket_name_news)\ntechnology_file = files[files['Key'].str.find('Technology')>=0]['Key'].values[0]\nprint('file_name',technology_file)\nfile_name s3://news/Technology/dt=20240211/data_Technology_news.csv\n# get the file from s3 using boto3\nobj = s3.get_object(Bucket=bucket_name_news, Key=technology_file)\ndata_tech = pd.read_csv(obj['Body'])\n\ndata_tech\n\nüè∑Ô∏è Add tags to the bucket\nWhen creating a resource in the cloud, it is considered a best practice to add tags for organizing resources, controlling costs, or applying security policies based on these labels.\nThe following code demonstrates how to add tags to a bucket using a method from the boto3 library.\ns3.put_bucket_tagging(\n    Bucket=bucket_name_news,\n    Tagging={\n        'TagSet': [\n            {\n                'Key': 'Environment',\n                'Value': 'Test'\n            },\n            {\n                'Key': 'Project',\n                'Value': 'Localstack+Boto3'\n            }\n        ]\n    }\n)\n# get the tagging\npd.json_normalize(s3.get_bucket_tagging(Bucket=bucket_name_news)['TagSet'])\n\nüîÑ Versioning in the bucket\nAnother good practice to apply is enabling versioning for your bucket.\nVersioning provides a way to recover and keep different versions of the same object.\nIn the following code, we will create a file with the inventory of objects in the bucket and save the file twice.\n# allow versioning in the bucket\ns3.put_bucket_versioning(\n    Bucket=bucket_name_news,\n    VersioningConfiguration={\n        'Status': 'Enabled'\n    }\n)\n# Add new file to the bucket\n\n# file name\nfile_name = 'inventory.csv'\n\n# list all objects in the bucket\nfiles = list_objects(bucket_name=bucket_name_news)\nbytes_io = io.BytesIO()\nfiles.to_csv(bytes_io, index=False)\nbytes_io.seek(0)\n# upload the data to s3\ns3.put_object(Bucket=bucket_name_news, Key=file_name, Body=bytes_io)\n#¬†add again the same file\ns3.put_object(Bucket=bucket_name_news, Key=file_name, Body=bytes_io)\n# List all the version of the object\nversions = s3.list_object_versions(Bucket=bucket_name, Prefix=file_name)\n\npd.json_normalize(versions['Versions'])\n\nüóëÔ∏è Create a static site using s3 bucket\nIn this section, we need to utilize a different command, which requires prior installation of the awscli-local tool specifically designed for use with LocalStack.\nThe awscli-local tool facilitates developers in seamlessly engaging with the LocalStack instance, because you can automatically redirecting commands to local endpoints instead of real AWS endpoints.\n# install awslocal to use the cli to interact with localstack\n!pip3.11 install awscli-local\n# the following command creates a static website in s3\n!awslocal s3api create-bucket --bucket docs-web\n# add the website configuration\n!awslocal s3 website s3://docs-web/ --index-document index.html --error-document error.html\n# syncronize the static site with the s3 bucket\n!awslocal s3 sync static-site s3://docs-web\n\nIf you are using localstack, you can access the website using the following\nUrl: http://docs-web.s3-website.localhost.localstack.cloud:4566/\n\n\nüìö References\nIf you want to learn‚Ä¶\nAWS Boto3\nLocalStack\nAPI:OkSurf News\nOther references:\n- Image preview reference: [Imagen de vectorjuice en Freepik]\n\n\n\n",
    "preview": "posts_en/2024-02-12-learning-aws-s3-on-localhost-best-practices-with-boto3-and-localstack/preview.jpg",
    "last_modified": "2024-04-02T01:23:29+02:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2024-01-14-transform-your-pandas-dataframes-in-r/",
    "title": "Transform your R Dataframes: Styles, üé® Colors, and üòé Emojis ",
    "description": "In the following article, we will explore a method to add colors and styles to R DataFrames.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2024-01-21",
    "categories": [
      "R",
      "Data",
      "DataViz"
    ],
    "contents": "\n\nContents\nWhat libraries can I use to style my R dataframes?\nüü£ Pivot Tables\nExample\n\n\nüü£ Dataframe: Apple Store apps\nData Schema overview\nüü£ Create Dataframe\nüü£ Pivot Table\nüü£ Styles with R libraries\nüé® Styling: Setting Background Color for Headers\nüé® Style: Setting the background color for all the rows\nüé® Style: Setting the background color for a particular cell\nüé® Style: Setting the background color for max/min values in the dataframe\nüé® Style: Color Background Gradients\nüé® Style: Color Background by columns\nüé® Style: Color Background by rows\nüé® Style: Color Bar\n\n\nüé® Style: Image in Columns\nüé® Style: Icons and Charts derived from column comparisons\nüé® Style: Emoji Representation Based on Percentile Values\nüìö References\n\n\nA few weeks ago I wrote an article about pandas dataframes and how to assign styles, but I received messages about how to do it in R (my first love ‚ù§Ô∏è in languages with data) and so I decided to rewrite the article using R libraries.\nSo in the next section of this article, we will explore a method to add üé®colors and üñåÔ∏èstyles in R DataFrames.\nWe will focus on the application of colors and emojis, using approaches similar to the popular conditional formatting commonly used in pivot tables within spreadsheets.\nThrough this strategy, we aim to improve the presentation of our data, making the exploration and understanding of the information not only informative but also visually attractive.\nWhat libraries can I use to style my R dataframes?\nThe R libraries used to create this article are as follows:\n\nüîç tidyverse: Among the best, it integrates various R libraries for data manipulation, graphics, and analysis, promoting clear and efficient code.\nüìù knitr: Automates the generation of dynamic reports.\nüìù kableExtra: An additional extension that enhances table presentation in R Markdown documents with extra formatting options.\nüìÑ reactablefrmtr: Incorporates functions to craft interactive and flexible tables in R, featuring filtering, sorting, and search functionalities.\n‚úèÔ∏è htmltools: Offers functions to build and manipulate HTML objects in R.\nüìÑ formattable: Equipped with functions for formatting and customizing tables in R.\nüìÑ flextable: Another library enabling the creation of flexible and customizable tables in R, with advanced formatting options for documents and presentations.\nüìä ggplot2: Among the most popular R visualization libraries, it produces appealing and comprehensible graphs.\nüé® viridis: A R library for creating visually appealing color maps\nThese libraries empowered me to employ functions for generating HTML-style representations of DataFrames.\nThis capability enables customization of DataFrame visual appearance during viewing.\nThe functions employed in this article facilitate the highlighting, coloring, and formatting of cells based on specific conditions.\nThis makes it effortless to visually identify patterns and trends within datasets.\n\nNext we have the code with we are going to create a pivot table using a set of data and from this you will begin to give it different styles and conditional formats such as can be seen in the previous image.\nüü£ Pivot Tables\n\nThe pivot table is a tabular data structure that provides a summarized overview of information from another table, organizing the data based on one variable and displaying values associated with another variable.\nIn this specific scenario, the pivot table organizes the data according to the ‚Äòsmoker‚Äô column and presents the total sum of tips, categorized by the days on which clients consume in the restaurant\n\n\nExample\nThe following example shows the pivot_table method with the ‚Äòtips‚Äô DataFrame\n\n\nlibrary(reshape2)\nlibrary(tidyverse)\n\ndata = tips\ndata_pivot <- data %>%\n  group_by(smoker, day) %>%\n  summarise(total_bill = sum(total_bill), .groups = 'drop') %>%\n  pivot_wider(names_from = day, values_from = total_bill)\n\ndata_pivot\n\n# A tibble: 2 √ó 5\n  smoker   Fri   Sat   Sun  Thur\n  <fct>  <dbl> <dbl> <dbl> <dbl>\n1 No      73.7  885. 1169.  770.\n2 Yes    252.   894.  458.  326.\n\nüü£ Dataframe: Apple Store apps\nIn this analysis, we will use the ‚Äòüçé Apple Store apps‚Äô DataFrame to explore the creation of pivot tables and customization of table styles.\nThis dataset provides detailed insights into Apple App Store applications, covering aspects from app names to specifics like size, price, and ratings.\nOur objective is to efficiently break down the information while applying styles that enhance the presentation and comprehension of data effectively.\nThe dataset was downloaded from Kaggle and it contains more than 7000 Apple iOS mobile application details.\nIt is important to note that the data was collected in July 2017.\nData Schema overview\ncolumn_name\ncolumn description\ntrack_name\nthe column contains the name of the app.\nsize_bytes\nthe column contains the size of the app in bytes.\ncurrency\nthe column contains the currency type.\nprice\nthe column contains the price of the app.\nrating_count_tot\nthe column contains the total number of ratings.\nrating_count_ver\nthe column contains the number of ratings for the current version of the app.\nuser_rating\nthe column contains the average user rating for the app.\nuser_rating_ver\nthe column contains the average user rating for the current version of the app.\nver\nthe column contains the current version of the app.\ncont_rating\nthe column contains the content rating.\nprime_genre\nthe column contains the primary genre.\nsup_devices.num\nthe column contains the number of supported devices.\nipadSc_urls.num\nthe column contains the number of screenshots showed for display.\nlang.num\nthe column contains the number of supported languages.\nvpp_lic\nthe column contains the Vpp Device Based Licensing Enabled.\nüü£ Create Dataframe\nIn the following code chunk, we will create a DataFrame by reading the CSV file.\n\n\nprint(paste0(\"tidyverse version: \",packageVersion(\"tidyverse\")[1]))\n\n[1] \"tidyverse version: 1.3.1\"\n\n\n\n# Create a dataframe from a csv file\n# You can download the file from the following link https://github.com/r0mymendez/pandas-styles\npath = 'https://raw.githubusercontent.com/r0mymendez/pandas-styles/main/data/AppleStore.csv'\ndata = read_delim(path , delim = \";\")\n\n\nüü£ Pivot Table\nIn the next step, our goal is to generate a dynamic table from a Dataframe, in which the top 15 genres with the largest number of applications are filtered.\n\n\n# Pivot table\n\n# filter the data to keep only the top 15 genres\ntop_genre = data %>%\n  group_by(prime_genre) %>%\n  summarise(count = n(), .groups = 'drop') %>%\n  arrange(desc(count)) %>%\n  head(n = 15) %>%\n  pull(prime_genre)\n\n\ntmp = data %>%\n  filter(prime_genre %in% top_genre) %>%\n  select(prime_genre, user_rating, price)\n\n\n# create a new column with the rating rounded to the nearest integer\ntmp$user_rating = paste0(\"rating_\", as.character(trunc(tmp$user_rating)))\n\n\n# create a pivot table\ntmp_pivot <- tmp %>%\n  group_by(prime_genre, user_rating) %>%\n  summarise(price = mean(price, na.rm = TRUE), .groups = 'drop') %>%\n  pivot_wider(names_from = user_rating, values_from = price, values_fill = 0) %>%\n  mutate(across(where(is.numeric), ~round(., 2)))\n\n\n# print the pivot table\ntmp_pivot\n\n# A tibble: 15 √ó 7\n   prime_genre   rating_0 rating_1 rating_2 rating_3 rating_4 rating_5\n   <chr>            <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n 1 Book              0.49     0        5.32     1.66     3.04     1.92\n 2 Education         3.42     1.79     1.95     2.32     5.2      3.12\n 3 Entertainment     0.51     1.99     0.78     0.9      0.95     1.03\n 4 Finance           0.3      0        0        0.72     0.53     0.5 \n 5 Games             0.85     0.84     1.21     1.71     1.52     1.29\n 6 Health & Fit‚Ä¶     1.33     3.24     1.5      1.21     2.15     1.83\n 7 Lifestyle         0.29     1.27     0.81     0.9      1.08     1.37\n 8 Music             2.74     0        0        2.08     5.27    13.2 \n 9 Photo & Video     0.75     0.74     1.36     2.16     1.47     1.33\n10 Productivity      0.66     2.49     0.99     4.9      4.73     2.61\n11 Shopping          0        0        0        0        0.03     0   \n12 Social Netwo‚Ä¶     0.12     0.4      0.66     0.33     0.41     0.2 \n13 Sports            0.92     2        0.68     0.61     1.25     1.66\n14 Travel            1.1      0        1.28     0.28     1.53     0.2 \n15 Utilities         2.03     2.49     1.25     1.67     1.65     0.66\n\nüü£ Styles with R libraries\nNow we will explore the functions of the aforementioned libraries that will allow us to improve the visual presentation of DataFrames.\nThis functionality provides us with different options to modify the appearance of the data, allowing us to customize aspects such as:\nHighlighting: Emphasize specific rows, columns, or values.\nFormatting: Adjust the format of the displayed values, including precision and alignment.\nBar Charts: Represent data with horizontal or vertical bar charts within cells.\nüé® Styling: Setting Background Color for Headers\nIn this section, we will apply styles to both the titles and the table.\nTherefore we use background colors to highlight the headers and the rest of the table.\n\n\nlibrary(knitr)\nlibrary(kableExtra)\n\nkable(tmp_pivot, \"html\") %>%\n  kable_styling(\"striped\", full_width = F) %>%\n  row_spec(0, background = \"#5E17EB\", color = \"white\")\n\n\nprime_genre\n\n\nrating_0\n\n\nrating_1\n\n\nrating_2\n\n\nrating_3\n\n\nrating_4\n\n\nrating_5\n\n\nBook\n\n\n0.49\n\n\n0.00\n\n\n5.32\n\n\n1.66\n\n\n3.04\n\n\n1.92\n\n\nEducation\n\n\n3.42\n\n\n1.79\n\n\n1.95\n\n\n2.32\n\n\n5.20\n\n\n3.12\n\n\nEntertainment\n\n\n0.51\n\n\n1.99\n\n\n0.78\n\n\n0.90\n\n\n0.95\n\n\n1.03\n\n\nFinance\n\n\n0.30\n\n\n0.00\n\n\n0.00\n\n\n0.72\n\n\n0.53\n\n\n0.50\n\n\nGames\n\n\n0.85\n\n\n0.84\n\n\n1.21\n\n\n1.71\n\n\n1.52\n\n\n1.29\n\n\nHealth & Fitness\n\n\n1.33\n\n\n3.24\n\n\n1.50\n\n\n1.21\n\n\n2.15\n\n\n1.83\n\n\nLifestyle\n\n\n0.29\n\n\n1.27\n\n\n0.81\n\n\n0.90\n\n\n1.08\n\n\n1.37\n\n\nMusic\n\n\n2.74\n\n\n0.00\n\n\n0.00\n\n\n2.08\n\n\n5.27\n\n\n13.16\n\n\nPhoto & Video\n\n\n0.75\n\n\n0.74\n\n\n1.36\n\n\n2.16\n\n\n1.47\n\n\n1.33\n\n\nProductivity\n\n\n0.66\n\n\n2.49\n\n\n0.99\n\n\n4.90\n\n\n4.73\n\n\n2.61\n\n\nShopping\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.03\n\n\n0.00\n\n\nSocial Networking\n\n\n0.12\n\n\n0.40\n\n\n0.66\n\n\n0.33\n\n\n0.41\n\n\n0.20\n\n\nSports\n\n\n0.92\n\n\n2.00\n\n\n0.68\n\n\n0.61\n\n\n1.25\n\n\n1.66\n\n\nTravel\n\n\n1.10\n\n\n0.00\n\n\n1.28\n\n\n0.28\n\n\n1.53\n\n\n0.20\n\n\nUtilities\n\n\n2.03\n\n\n2.49\n\n\n1.25\n\n\n1.67\n\n\n1.65\n\n\n0.66\n\n\nüé® Style: Setting the background color for all the rows\nIn following code snippet illustrates how to set a custom background color for all the rows in our DataFrame.\n\n\nkable(tmp_pivot, \"html\") %>%\n  kable_styling(\"striped\", full_width = F)  %>%\n  row_spec(0, background = \"#5E17EB\", color = \"white\") %>%\n  column_spec(1, column=1:ncol(tmp_pivot) ,background = \"#ECE3FF\", color = \"black\")\n\n\nprime_genre\n\n\nrating_0\n\n\nrating_1\n\n\nrating_2\n\n\nrating_3\n\n\nrating_4\n\n\nrating_5\n\n\nBook\n\n\n0.49\n\n\n0.00\n\n\n5.32\n\n\n1.66\n\n\n3.04\n\n\n1.92\n\n\nEducation\n\n\n3.42\n\n\n1.79\n\n\n1.95\n\n\n2.32\n\n\n5.20\n\n\n3.12\n\n\nEntertainment\n\n\n0.51\n\n\n1.99\n\n\n0.78\n\n\n0.90\n\n\n0.95\n\n\n1.03\n\n\nFinance\n\n\n0.30\n\n\n0.00\n\n\n0.00\n\n\n0.72\n\n\n0.53\n\n\n0.50\n\n\nGames\n\n\n0.85\n\n\n0.84\n\n\n1.21\n\n\n1.71\n\n\n1.52\n\n\n1.29\n\n\nHealth & Fitness\n\n\n1.33\n\n\n3.24\n\n\n1.50\n\n\n1.21\n\n\n2.15\n\n\n1.83\n\n\nLifestyle\n\n\n0.29\n\n\n1.27\n\n\n0.81\n\n\n0.90\n\n\n1.08\n\n\n1.37\n\n\nMusic\n\n\n2.74\n\n\n0.00\n\n\n0.00\n\n\n2.08\n\n\n5.27\n\n\n13.16\n\n\nPhoto & Video\n\n\n0.75\n\n\n0.74\n\n\n1.36\n\n\n2.16\n\n\n1.47\n\n\n1.33\n\n\nProductivity\n\n\n0.66\n\n\n2.49\n\n\n0.99\n\n\n4.90\n\n\n4.73\n\n\n2.61\n\n\nShopping\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.03\n\n\n0.00\n\n\nSocial Networking\n\n\n0.12\n\n\n0.40\n\n\n0.66\n\n\n0.33\n\n\n0.41\n\n\n0.20\n\n\nSports\n\n\n0.92\n\n\n2.00\n\n\n0.68\n\n\n0.61\n\n\n1.25\n\n\n1.66\n\n\nTravel\n\n\n1.10\n\n\n0.00\n\n\n1.28\n\n\n0.28\n\n\n1.53\n\n\n0.20\n\n\nUtilities\n\n\n2.03\n\n\n2.49\n\n\n1.25\n\n\n1.67\n\n\n1.65\n\n\n0.66\n\n\nüé® Style: Setting the background color for a particular cell\nIn following code snippet illustrates how to set a custom background color for a particular cell in our DataFrame using pandas styling.\n\n\nvalue = 4\n\ntmp_pivot %>%\n  mutate(\n    rating_4 = cell_spec(rating_4, \"html\", \n                    background = if_else(tmp_pivot$rating_4>value, \"#FD636B\", \"#ECE3FF\"),\n                    color = if_else(tmp_pivot$rating_4>value, \"white\", \"black\")\n    )\n  ) %>%\n  kable(format = \"html\", escape = F) %>%\n  kable_styling(\"striped\", full_width = FALSE) %>%\n  row_spec(0, background = \"#5E17EB\", color = \"white\") %>%\n  column_spec(1:ncol(tmp_pivot), background = \"#ECE3FF\", color = \"black\")\n\n\nprime_genre\n\n\nrating_0\n\n\nrating_1\n\n\nrating_2\n\n\nrating_3\n\n\nrating_4\n\n\nrating_5\n\n\nBook\n\n\n0.49\n\n\n0.00\n\n\n5.32\n\n\n1.66\n\n\n3.04\n\n\n1.92\n\n\nEducation\n\n\n3.42\n\n\n1.79\n\n\n1.95\n\n\n2.32\n\n\n5.2\n\n\n3.12\n\n\nEntertainment\n\n\n0.51\n\n\n1.99\n\n\n0.78\n\n\n0.90\n\n\n0.95\n\n\n1.03\n\n\nFinance\n\n\n0.30\n\n\n0.00\n\n\n0.00\n\n\n0.72\n\n\n0.53\n\n\n0.50\n\n\nGames\n\n\n0.85\n\n\n0.84\n\n\n1.21\n\n\n1.71\n\n\n1.52\n\n\n1.29\n\n\nHealth & Fitness\n\n\n1.33\n\n\n3.24\n\n\n1.50\n\n\n1.21\n\n\n2.15\n\n\n1.83\n\n\nLifestyle\n\n\n0.29\n\n\n1.27\n\n\n0.81\n\n\n0.90\n\n\n1.08\n\n\n1.37\n\n\nMusic\n\n\n2.74\n\n\n0.00\n\n\n0.00\n\n\n2.08\n\n\n5.27\n\n\n13.16\n\n\nPhoto & Video\n\n\n0.75\n\n\n0.74\n\n\n1.36\n\n\n2.16\n\n\n1.47\n\n\n1.33\n\n\nProductivity\n\n\n0.66\n\n\n2.49\n\n\n0.99\n\n\n4.90\n\n\n4.73\n\n\n2.61\n\n\nShopping\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.03\n\n\n0.00\n\n\nSocial Networking\n\n\n0.12\n\n\n0.40\n\n\n0.66\n\n\n0.33\n\n\n0.41\n\n\n0.20\n\n\nSports\n\n\n0.92\n\n\n2.00\n\n\n0.68\n\n\n0.61\n\n\n1.25\n\n\n1.66\n\n\nTravel\n\n\n1.10\n\n\n0.00\n\n\n1.28\n\n\n0.28\n\n\n1.53\n\n\n0.20\n\n\nUtilities\n\n\n2.03\n\n\n2.49\n\n\n1.25\n\n\n1.67\n\n\n1.65\n\n\n0.66\n\n\nüé® Style: Setting the background color for max/min values in the dataframe\nNow, we will focus on highlighting the maximum and minimum values in our DataFrame.\nFor this reason, we will assign distinctive background colors to these extreme values, facilitating a quicker and more intuitive understanding of the dataset.\nThe code snippet below demonstrates how to implement this stylistic enhancement.\n\n\nrating_columns <- grep(\"^rating\", names(tmp_pivot), value = TRUE)\nmax_value <- max(unlist(tmp_pivot %>% select(rating_columns), use.names = FALSE))\nmin_value <- min(unlist(tmp_pivot %>% select(rating_columns), use.names = FALSE))\n\n# The next function to apply specific formatting and preserve the original\nformat_spec <- function(x) {\n  if_else(x == max_value, sprintf(\"%.2f\", x),\n          if_else(x == min_value, sprintf(\"%.2f\", x),\n                  sprintf(\"%.2f\", x)))\n}\n\ntmp_pivot %>%\n  mutate(\n    across(rating_columns, \n           ~ cell_spec(format_spec(.x),\n          \"html\", \n           background = if_else(. == max_value, \"#3BE8B0\",\n                                if_else(. == min_value, \"#FF66C4\", \"#ECE3FF\")),\n           bold = if_else(. == max_value, TRUE,if_else(. == min_value, TRUE, FALSE))\n                )\n         )\n  ) %>%\n  kable(format = \"html\", escape = F) %>%\n  kable_styling(\"striped\", full_width = FALSE) %>%\n  row_spec(0, background = \"#5E17EB\", color = \"white\") %>%\n  column_spec(1:ncol(tmp_pivot), background = \"#ECE3FF\", color = \"black\")\n\n\nprime_genre\n\n\nrating_0\n\n\nrating_1\n\n\nrating_2\n\n\nrating_3\n\n\nrating_4\n\n\nrating_5\n\n\nBook\n\n\n0.49\n\n\n0.00\n\n\n5.32\n\n\n1.66\n\n\n3.04\n\n\n1.92\n\n\nEducation\n\n\n3.42\n\n\n1.79\n\n\n1.95\n\n\n2.32\n\n\n5.20\n\n\n3.12\n\n\nEntertainment\n\n\n0.51\n\n\n1.99\n\n\n0.78\n\n\n0.90\n\n\n0.95\n\n\n1.03\n\n\nFinance\n\n\n0.30\n\n\n0.00\n\n\n0.00\n\n\n0.72\n\n\n0.53\n\n\n0.50\n\n\nGames\n\n\n0.85\n\n\n0.84\n\n\n1.21\n\n\n1.71\n\n\n1.52\n\n\n1.29\n\n\nHealth & Fitness\n\n\n1.33\n\n\n3.24\n\n\n1.50\n\n\n1.21\n\n\n2.15\n\n\n1.83\n\n\nLifestyle\n\n\n0.29\n\n\n1.27\n\n\n0.81\n\n\n0.90\n\n\n1.08\n\n\n1.37\n\n\nMusic\n\n\n2.74\n\n\n0.00\n\n\n0.00\n\n\n2.08\n\n\n5.27\n\n\n13.16\n\n\nPhoto & Video\n\n\n0.75\n\n\n0.74\n\n\n1.36\n\n\n2.16\n\n\n1.47\n\n\n1.33\n\n\nProductivity\n\n\n0.66\n\n\n2.49\n\n\n0.99\n\n\n4.90\n\n\n4.73\n\n\n2.61\n\n\nShopping\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.03\n\n\n0.00\n\n\nSocial Networking\n\n\n0.12\n\n\n0.40\n\n\n0.66\n\n\n0.33\n\n\n0.41\n\n\n0.20\n\n\nSports\n\n\n0.92\n\n\n2.00\n\n\n0.68\n\n\n0.61\n\n\n1.25\n\n\n1.66\n\n\nTravel\n\n\n1.10\n\n\n0.00\n\n\n1.28\n\n\n0.28\n\n\n1.53\n\n\n0.20\n\n\nUtilities\n\n\n2.03\n\n\n2.49\n\n\n1.25\n\n\n1.67\n\n\n1.65\n\n\n0.66\n\n\nüé® Style: Color Background Gradients\nIn the upcoming section, we will delve into the concept of color maps, representing a spectrum of colors arranged in a gradient.\nA colormap, essentially a palette of colors, consists of distinctive denominations, with the most popular ones being [‚Äòviridis,‚Äô ‚Äòmagma,‚Äô ‚Äòinferno,‚Äô ‚Äòplasma‚Äô, ‚Äòcividis‚Äô].\nThe primary objective behind creating these color spectrums is to enhance the visual representation of data.\nEach color in the gradient carries specific nuances, contributing to a more nuanced data visualization experience.\n\n\nlibrary(viridisLite)  \nlibrary(viridis)\nlibrary(unikn)  # load package\nseecol(pal = pal_unikn)\n\n\n# Reference of the following code: https://bookdown.org/hneth/ds4psy/D-4-apx-colors-pkgs.html\nn <- 10  # number of colors\n\n# define 5 different color scales (n colors each):\nv1 <- viridis(n)\nv2 <- magma(n)\nv3 <- inferno(n)\nv4 <- plasma(n)\nv5 <- cividis(n)\n\n# See and compare color scales:\nseecol(list(v1, v2, v3, v4, v5), \n       col_brd = \"white\", lwd_brd = 4, \n       title = \"Various viridis color palettes (n = 10)\",\n       pal_names = c(\"v1: viridis\", \"v2: magma\", \"v3: inferno\", \"v4: plasma\",  \"v5: cividis\"))\n\n\n\nViridis palette\n\nNow, we will apply a color gradient to our pivot table, allowing you to observe how it is colored using the Viridis palette.\nIn this context, lighter colors signify larger values within the distribution, while darker shades correspond to smaller values in the distribution.\nThis approach provides a visual representation that intuitively conveys the magnitude of the data, making it easier to discern patterns and variations across the dataset.\n\n\n\nlibrary(ggplot2)\n# N√∫mero de tonos (lut)\nlut <- 10\n\n# Crear un data frame con una variable continua\ndata <- data.frame(x = seq(1, lut))\n\noptions(repr.plot.width = 5, repr.plot.height =2) \n\n# Crear un gr√°fico de barras con colores de la paleta \"viridis\"\nggplot(data, aes(x = x, y = 0.2, fill = as.factor(x))) +\n  geom_tile() +\n  scale_fill_manual(values = viridis(lut, option = \"D\")) +\n  labs(x = \"√çndice\", y = \"\") +\n  theme_void() +\n  theme(legend.position = \"none\") \n\n\n\n\n\n# Calculate maximum and minimum values\nmax_value <- max(unlist(tmp_pivot %>% select(rating_columns), use.names = FALSE))\nmin_value <- min(unlist(tmp_pivot %>% select(rating_columns), use.names = FALSE))\n\n# Define the number of cuts for the \"viridis\" palette\nnum_cuts <-nrow(tmp_pivot)\n\nxc <- seq(min_value, max_value, length.out = num_cuts)\npal <- viridis(num_cuts)\n\n# Apply color gradients to each cell with viridis \nstyled_table <- map(tmp_pivot, function(col) {\n  if (is.numeric(col)) {\n    cell_spec( format_spec(col),\n               \"html\", \n               background = pal[cut(col, breaks = xc, include.lowest = TRUE)])\n  } else {\n    cell_spec(col, \"html\")\n  }\n}) %>%\n  as.data.frame() %>%\n  kable(format = \"html\", escape = F)  %>%\n  kable_styling(\"striped\", full_width = FALSE)  %>%\n  row_spec(0, background = \"#440154FF\", color = \"white\") %>%\n  column_spec(2:ncol(tmp_pivot), color = \"white\") %>%\n  column_spec(1:1, background = \"#ECE3FF\") \n\nstyled_table\n\n\nprime_genre\n\n\nrating_0\n\n\nrating_1\n\n\nrating_2\n\n\nrating_3\n\n\nrating_4\n\n\nrating_5\n\n\nBook\n\n\n0.49\n\n\n0.00\n\n\n5.32\n\n\n1.66\n\n\n3.04\n\n\n1.92\n\n\nEducation\n\n\n3.42\n\n\n1.79\n\n\n1.95\n\n\n2.32\n\n\n5.20\n\n\n3.12\n\n\nEntertainment\n\n\n0.51\n\n\n1.99\n\n\n0.78\n\n\n0.90\n\n\n0.95\n\n\n1.03\n\n\nFinance\n\n\n0.30\n\n\n0.00\n\n\n0.00\n\n\n0.72\n\n\n0.53\n\n\n0.50\n\n\nGames\n\n\n0.85\n\n\n0.84\n\n\n1.21\n\n\n1.71\n\n\n1.52\n\n\n1.29\n\n\nHealth & Fitness\n\n\n1.33\n\n\n3.24\n\n\n1.50\n\n\n1.21\n\n\n2.15\n\n\n1.83\n\n\nLifestyle\n\n\n0.29\n\n\n1.27\n\n\n0.81\n\n\n0.90\n\n\n1.08\n\n\n1.37\n\n\nMusic\n\n\n2.74\n\n\n0.00\n\n\n0.00\n\n\n2.08\n\n\n5.27\n\n\n13.16\n\n\nPhoto & Video\n\n\n0.75\n\n\n0.74\n\n\n1.36\n\n\n2.16\n\n\n1.47\n\n\n1.33\n\n\nProductivity\n\n\n0.66\n\n\n2.49\n\n\n0.99\n\n\n4.90\n\n\n4.73\n\n\n2.61\n\n\nShopping\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.03\n\n\n0.00\n\n\nSocial Networking\n\n\n0.12\n\n\n0.40\n\n\n0.66\n\n\n0.33\n\n\n0.41\n\n\n0.20\n\n\nSports\n\n\n0.92\n\n\n2.00\n\n\n0.68\n\n\n0.61\n\n\n1.25\n\n\n1.66\n\n\nTravel\n\n\n1.10\n\n\n0.00\n\n\n1.28\n\n\n0.28\n\n\n1.53\n\n\n0.20\n\n\nUtilities\n\n\n2.03\n\n\n2.49\n\n\n1.25\n\n\n1.67\n\n\n1.65\n\n\n0.66\n\n\nüé® Style: Color Background by columns\nIn the next code chunk, we will enhance the visual representation of our pivot table by introducing distinct color backgrounds to specific columns.\nThis technique aids in better highlighting and categorizing data, making it easier to draw insights from the table.\n\n\ntmp_pivot%>%\n  kable(format = \"html\", escape = F)  %>%\n  kable_styling(\"striped\", full_width = FALSE)  %>%\n  column_spec(2:3, background = \"#FFCFC9\", color = \"black\") %>%\n  column_spec(4:5, background = \"#FFF1B0\", color = \"black\") %>%\n  column_spec(6:7, background = \"#BEEAE5\", color = \"black\")%>%\n  row_spec(0, background = \"#440154FF\", color = \"white\") %>%\n  column_spec(1:1, background = \"#ECE3FF\") \n\n\nprime_genre\n\n\nrating_0\n\n\nrating_1\n\n\nrating_2\n\n\nrating_3\n\n\nrating_4\n\n\nrating_5\n\n\nBook\n\n\n0.49\n\n\n0.00\n\n\n5.32\n\n\n1.66\n\n\n3.04\n\n\n1.92\n\n\nEducation\n\n\n3.42\n\n\n1.79\n\n\n1.95\n\n\n2.32\n\n\n5.20\n\n\n3.12\n\n\nEntertainment\n\n\n0.51\n\n\n1.99\n\n\n0.78\n\n\n0.90\n\n\n0.95\n\n\n1.03\n\n\nFinance\n\n\n0.30\n\n\n0.00\n\n\n0.00\n\n\n0.72\n\n\n0.53\n\n\n0.50\n\n\nGames\n\n\n0.85\n\n\n0.84\n\n\n1.21\n\n\n1.71\n\n\n1.52\n\n\n1.29\n\n\nHealth & Fitness\n\n\n1.33\n\n\n3.24\n\n\n1.50\n\n\n1.21\n\n\n2.15\n\n\n1.83\n\n\nLifestyle\n\n\n0.29\n\n\n1.27\n\n\n0.81\n\n\n0.90\n\n\n1.08\n\n\n1.37\n\n\nMusic\n\n\n2.74\n\n\n0.00\n\n\n0.00\n\n\n2.08\n\n\n5.27\n\n\n13.16\n\n\nPhoto & Video\n\n\n0.75\n\n\n0.74\n\n\n1.36\n\n\n2.16\n\n\n1.47\n\n\n1.33\n\n\nProductivity\n\n\n0.66\n\n\n2.49\n\n\n0.99\n\n\n4.90\n\n\n4.73\n\n\n2.61\n\n\nShopping\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.03\n\n\n0.00\n\n\nSocial Networking\n\n\n0.12\n\n\n0.40\n\n\n0.66\n\n\n0.33\n\n\n0.41\n\n\n0.20\n\n\nSports\n\n\n0.92\n\n\n2.00\n\n\n0.68\n\n\n0.61\n\n\n1.25\n\n\n1.66\n\n\nTravel\n\n\n1.10\n\n\n0.00\n\n\n1.28\n\n\n0.28\n\n\n1.53\n\n\n0.20\n\n\nUtilities\n\n\n2.03\n\n\n2.49\n\n\n1.25\n\n\n1.67\n\n\n1.65\n\n\n0.66\n\n\nüé® Style: Color Background by rows\nIn the next code chunk, we will enhance the visual representation of our pivot table by introducing distinct color backgrounds by rows.\n\n\nn <- nrow(tmp_pivot)\nrow_0 <-c()\nrow_1 <-c()\n\n\nfor (item in seq(1, n)) {\n  if (item %% 2 == 0) {\n    row_0 <- c(row_0,item)\n  } else {\n    row_1 <- c(row_1,item)\n  }\n}\n\n tmp_pivot %>%\n  kable(format = \"html\", escape = F) %>%\n  kable_styling(\"striped\", full_width = FALSE) %>%\n  row_spec(row_0, background = \"#ECE3FF\", color = \"black\") %>%\n  row_spec(row_1, background = \"#ffdefa\", color = \"black\")%>%\n  row_spec(0, background = \"#440154FF\", color = \"white\") \n\n\nprime_genre\n\n\nrating_0\n\n\nrating_1\n\n\nrating_2\n\n\nrating_3\n\n\nrating_4\n\n\nrating_5\n\n\nBook\n\n\n0.49\n\n\n0.00\n\n\n5.32\n\n\n1.66\n\n\n3.04\n\n\n1.92\n\n\nEducation\n\n\n3.42\n\n\n1.79\n\n\n1.95\n\n\n2.32\n\n\n5.20\n\n\n3.12\n\n\nEntertainment\n\n\n0.51\n\n\n1.99\n\n\n0.78\n\n\n0.90\n\n\n0.95\n\n\n1.03\n\n\nFinance\n\n\n0.30\n\n\n0.00\n\n\n0.00\n\n\n0.72\n\n\n0.53\n\n\n0.50\n\n\nGames\n\n\n0.85\n\n\n0.84\n\n\n1.21\n\n\n1.71\n\n\n1.52\n\n\n1.29\n\n\nHealth & Fitness\n\n\n1.33\n\n\n3.24\n\n\n1.50\n\n\n1.21\n\n\n2.15\n\n\n1.83\n\n\nLifestyle\n\n\n0.29\n\n\n1.27\n\n\n0.81\n\n\n0.90\n\n\n1.08\n\n\n1.37\n\n\nMusic\n\n\n2.74\n\n\n0.00\n\n\n0.00\n\n\n2.08\n\n\n5.27\n\n\n13.16\n\n\nPhoto & Video\n\n\n0.75\n\n\n0.74\n\n\n1.36\n\n\n2.16\n\n\n1.47\n\n\n1.33\n\n\nProductivity\n\n\n0.66\n\n\n2.49\n\n\n0.99\n\n\n4.90\n\n\n4.73\n\n\n2.61\n\n\nShopping\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.03\n\n\n0.00\n\n\nSocial Networking\n\n\n0.12\n\n\n0.40\n\n\n0.66\n\n\n0.33\n\n\n0.41\n\n\n0.20\n\n\nSports\n\n\n0.92\n\n\n2.00\n\n\n0.68\n\n\n0.61\n\n\n1.25\n\n\n1.66\n\n\nTravel\n\n\n1.10\n\n\n0.00\n\n\n1.28\n\n\n0.28\n\n\n1.53\n\n\n0.20\n\n\nUtilities\n\n\n2.03\n\n\n2.49\n\n\n1.25\n\n\n1.67\n\n\n1.65\n\n\n0.66\n\n\nüé® Style: Color Bar\nIn this section, we will implement the style.bar function to introduce a dynamic color bar into our DataFrame.\nThe color bar provides a visual representation of data values, assigning varying colors to different data ranges.\n\n\nlibrary(htmlwidgets)\nlibrary(htmltools)\nlibrary(formattable)\n\nformattable(tmp_pivot, list(\n  rating_0 = color_bar(\"#FFCFC9\"),\n  rating_1 = color_bar(\"#FFCFC9\"),\n  rating_2 = color_bar(\"#FFF1B0\"),\n  rating_3 = color_bar(\"#FFF1B0\"),\n  rating_4 = color_bar(\"#BEEAE5\"),\n  rating_5 = color_bar(\"#BEEAE5\")\n)) %>%\n  as.htmlwidget() %>%\n  prependContent(tags$style(\"th { padding: 0px !important; background: #5E17EB; color: white }\")) %>%\n  prependContent(tags$style(\"table tr td:first-child { background-color: #ECE3FF }\")) \n\n\n\nüé® Style: Image in Columns\nIn this section, we explore the enhancement of data representation by adding an image to an additional column.\nThis approach provides an alternative method to elevate the visual impact of the data being presented.\nThese images can serve as icons, represent brands, or convey additional visual elements to captivate and engage the audience.\n\n\nlibrary(flextable)\n\nflextable(tmp_pivot%>%\n              head(5)%>%\n              mutate(id=0:4,\n                     id=paste0('img/img_',id,'.png'))%>%\n              select(c('id',names(tmp_pivot)))\n         ) %>%\n  colformat_image(j = \"id\", width = .5, height = 0.5) %>%\n  bg(part = \"header\", bg = \"#5E17EB\", j =  c('id',names(tmp_pivot)) ) %>% \n  color(color = \"white\", part = \"header\") %>% \n  bg(part = \"body\", bg = \"#ECE3FF\")\n\nidprime_genrerating_0rating_1rating_2rating_3rating_4rating_5Book0.490.005.321.663.041.92Education3.421.791.952.325.203.12Entertainment0.511.990.780.900.951.03Finance0.300.000.000.720.530.50Games0.850.841.211.711.521.29\n\nüé® Style: Icons and Charts derived from column comparisons\nIn this section, we‚Äôll explore a creative approach using icons and charts to visually represent comparisons between two or more columns of data.\n\n\n# Define the column names\ncolumn_names <- paste0(\"rating_\", 0:5)\n\n# Apply the formatter function to each column\nformatters_list <- lapply(column_names, function(col) {\n  prev_col <- paste0(\"rating_\", as.numeric(str_extract(col, \"\\\\d+\")) - 1)\n  if (col == \"rating_0\") {\n    formatter(\n      \"span\",\n      style = ~ formattable::style(\"font.weight\" = \"bold\"),\n      ~icontext(\"arrow-right\", tmp_pivot[[col]])\n    )\n  } else {\n    formatter(\n      \"span\",\n      style = ~ formattable::style(color = ifelse(tmp_pivot[[col]] > tmp_pivot[[prev_col]], \n                                                  \"green\", \"red\"),\n                      \"font.weight\" = \"bold\"),\n      ~icontext(ifelse(tmp_pivot[[col]] > tmp_pivot[[prev_col]], \"arrow-up\", \"arrow-down\"), tmp_pivot[[col]])\n    )\n  }\n})\n\n# Create a named list for formattable\nformatters_list <- setNames(formatters_list, column_names)\n\n# Apply formattable and convert to htmlwidget\nformattable(tmp_pivot, formatters_list) %>%\n  as.htmlwidget() %>%\n  prependContent(\n    tags$style(\"th { padding: 0px !important; background: #5E17EB; color: white }\")) %>%\n  prependContent(\n    tags$style(\"table tr td:first-child { background-color: #ECE3FF }\"))\n\n\n\n\n\n# Get the names of columns that start with 'rating'\ncolumn_str = names(tmp_pivot[startsWith(names(tmp_pivot), \"rating\")])\n\n# Get the names of columns that we will expect to have\nexpected_columns = c(names(tmp_pivot), 'plot_line', 'plot_bar')\n\n# Create the new data frame with two new columns with the charts representation\ntmp_nest <- tmp_pivot %>%\n  nest(values = c(column_str), values_1=c(column_str)) %>%\n  mutate( values = map(values, ~ as.numeric(.x) ) ) %>%\n  unnest(values_1)%>%\n  mutate(\n    plot_line = map_chr(values,\n                        ~ as.character(htmltools::as.tags(sparkline::sparkline(.x, \n                                                               type = \"line\",\n                                                               lineColor='#5E17EB',\n                                                               fillColor='#5E17EB'))\n                                       )\n                        ),\n     plot_bar = map_chr(values, \n                       ~ as.character(htmltools::as.tags(sparkline::sparkline(.x,\n                        type = \"bar\",\n                        barColor = '#5E17EB'))\n                        )\n                       )\n  )%>%\n  select(expected_columns)\n\n# Print the table with html format\nout = as.htmlwidget(formattable(tmp_nest))\nout$dependencies = c(out$dependencies, htmlwidgets:::widget_dependencies(\"sparkline\", \"sparkline\"))\nout %>%\n  prependContent(tags$style(\"th { padding: 0px !important; background: #5E17EB; color: white }\")) %>%\n  prependContent(tags$style(\"table tr td:first-child { background-color: #ECE3FF }\"))\n\n\n\n\n\nlibrary(reactablefmtr)\n\nreactable(tmp_pivot%>%head(10),\n          defaultColDef = colDef(\n            cell = data_bars(\n              tmp_pivot, \n              box_shadow = TRUE, \n              round_edges = TRUE,\n              text_position = \"outside-base\",\n              fill_color = c(\"#5E17EB\", \"#ECE3FF\"),\n              background = \"#e5e5e5\",\n              fill_gradient = TRUE\n            ),\n            style = list(\n              #backgroundColor = \"#ECE3FF\",\n              color = \"black\",\n              fontSize = \"14px\"\n            ),\n            headerStyle = list(\n              backgroundColor = \"#5E17EB\",\n              color = \"white\",\n              fontFamily = \"Comic Sans MS\",\n              fontSize = \"14px\"\n            )\n          )\n)\n\n\n\n\n\nreactable(\n  tmp_pivot%>%head(5)%>%select(-c('rating_0')),\n  defaultColDef = colDef(\n    align = 'center',\n    cell = bubble_grid(\n      data = tmp_pivot%>%head(5),\n      shape = \"circles\",\n      number_fmt = scales::comma,\n      colors = c(\"#D6B1FB\",\"#C687FC\",\"#A060FF\"),\n      brighten_text=TRUE,\n      text_size=14,\n      box_shadow = TRUE,\n      opacity = 0.6\n    ),\n    headerStyle = list(\n      backgroundColor = \"#5E17EB\",\n      color = \"white\",\n      fontFamily = \"Comic Sans MS\",\n      fontSize = \"14px\"\n    )\n  ),\n  columns = list(\n    prime_genre = colDef(\n      style = list(\n        fontSize = \"14px\"\n      )\n    )\n  )\n)%>% \n  add_title(\n    title = reactablefmtr::html(\"üçé Apple Store apps\"),\n    margin = reactablefmtr::margin(t=0,r=0,b=5,l=0)\n    ) \n\nüçé Apple Store apps\n\n\n\n\ntmp_pivot_5 = tmp_pivot%>%head(5)\n\nreactable(\n  tmp_pivot_5%>%select(-c('rating_0')),\n  defaultColDef = colDef(\n    align = 'center',\n    cell = bubble_grid(\n      data = tmp_pivot_5,\n      shape = \"squares\",\n      number_fmt = scales::comma,\n      colors = c(\"#D6B1FB\",\"#C687FC\",\"#A060FF\"),\n      brighten_text=TRUE,\n      text_size=14,\n      box_shadow = TRUE,\n      opacity = 0.6,\n    ),\n    headerStyle = list(\n      backgroundColor = \"#5E17EB\",\n      color = \"white\",\n      fontFamily = \"Comic Sans MS\",\n      fontSize = \"14px\"\n    )\n  ),\ncolumns = list(\n    prime_genre = colDef(\n      minWidth = 175,\n      cell = pill_buttons(data = tmp_pivot_5$prime_genre, box_shadow = TRUE,colors=\"#A060FF\",text_color='white'),\n      style = list(\n        fontSize = \"14px\"\n      )\n    )\n  )\n) %>% \n  add_title(\n    title = reactablefmtr::html(\"üçé Apple Store apps\"),\n    margin = reactablefmtr::margin(t=0,r=0,b=5,l=0)\n  ) \n\nüçé Apple Store apps\n\n\n\n\nreactable(\n  tmp_pivot%>%head(5)%>%select(-c('rating_0')),,\n  defaultColDef = colDef(\n    align = 'left',\n    cell = gauge_chart(\n      data = tmp_pivot_5,\n      number_fmt = scales::comma,\n      fill_color=\"#5E17EB\",\n      background=\"#e5e5e5\",\n    ),\n    headerStyle = list(\n      backgroundColor = \"#5E17EB\",\n      color = \"white\",\n      fontFamily = \"Comic Sans MS\",\n      fontSize = \"14px\"\n    )\n  ),\n  columns = list(\n    prime_genre = colDef(\n      minWidth = 175,\n      header = htmltools::tags$div(\n        \"Prime Genre\", \n        style = list(\n          backgroundColor = \"#5E17EB\",\n          color = \"white\",\n          fontFamily = \"Comic Sans MS\",\n          fontSize = \"14px\"\n        )\n      ),\n      cell = pill_buttons(data = tmp_pivot_5$prime_genre, box_shadow = TRUE,colors=\"#A060FF\",text_color='white'),\n      style = list(\n        fontSize = \"14px\",\n        fontFamily = \"Comic Sans MS\"\n      )\n    )\n  )\n) %>% \n  add_title(\n  title = htmltools::HTML(\n    sprintf(\"<div style='font-family:Comic Sans MS;'>%s<\/div>\", \"üçé Apple Store apps\")\n  ),\n  margin = reactablefmtr::margin(t=0,r=0,b=5,l=0)\n)\n\nüçé Apple Store apps\n\n\nüé® Style: Emoji Representation Based on Percentile Values\nIn this section, we delve into the creative use of emojis based on percentile values, offering a distinctive approach to elevate data representation.\nBy incorporating diverse emojis, we enhance the visual impact of the data.\nSpecifically, we employ circles and squads as emojis to bring nuanced expressions to our data points.\nIf you‚Äôd like to view the code for creating this style, it‚Äôs available in my GitHub repository.\nFeel free to check it out and give it a star if you find it helpful!\n‚≠êÔ∏è\n\n\ncreate_series <- function(row_data, emoji) {\n  if (emoji == 'max') {\n    return(ifelse(row_data == max(row_data), 'üü©', '‚¨ú'))\n  } else if (emoji == 'min') {\n    return(ifelse(row_data == min(row_data), 'üü•', '‚¨ú'))\n  } else if (emoji == 'min_max') {\n    return(ifelse(row_data == min(row_data), 'üü•',\n                  ifelse(row_data == max(row_data), 'üü©', '‚¨ú')))\n }\n}\n\n\n\n\nget_percentiles <- function(row_data, bins=3, emoji='circle') {\n  emoji_labels <- list(\n    'circle' = list('3' = c('üî¥', 'üü°', 'üü¢'), \n                    '4' = c('üî¥', 'üü†', 'üü°', 'üü¢')),\n    'squad' =  list('3' = c('üü•', 'üü®', 'üü©'), \n                   '4' = c('üü•', 'üü®', 'üüß', 'üü©'))\n  )\n  \n  if (emoji %in% c('max', 'min', 'min_max')) {\n    return(create_series(row_data, emoji))\n  } else if (emoji %in% names(emoji_labels) & bins %in% names(emoji_labels[[emoji]])) {\n    labels <- emoji_labels[[emoji]][[as.character(bins)]]\n    return(cut(row_data, breaks=length(labels), labels=labels, ordered_result=FALSE))\n  } else {\n    return(row_data)\n  }\n}\n\n\n\n\nget_conditional_table_column <- function(data, bins=3, emoji='circle') {\n  tmp <- data\n  for (column in colnames(data)) {\n    if (is.numeric(data[[column]])) {\n      row_data_emoji <- as.character(get_percentiles(data[[column]], bins, emoji))\n      tmp[[column]] <- paste(format(round(data[[column]], 2), nsmall = 2), row_data_emoji)\n    }\n  }\n  return(tmp)\n}\n\n\n\n\nget_conditional_table_row <- function(data, bins=3, emoji='circle') {\n  response_values <- list()\n  column_str <- names(data)[sapply(data, is.character)]\n  columns_num <- names(data)[sapply(data, is.numeric)]\n  \n  for (row in 1:nrow(data)) {\n    row_data <- data[row, columns_num]\n    percentil <- get_percentiles(row_data, bins, emoji)\n    row_data <- sapply(round(row_data, 2),format_spec)\n    percentil_values <- paste(row_data, percentil)\n    response_values <- append(response_values, list(percentil_values))\n  }\n  \n  result_df <- data.frame(matrix(unlist(response_values), \n                                 nrow=length(response_values), byrow=TRUE))\n  \n  names(result_df) <- columns_num\n  result_df <- cbind(data[column_str], result_df)\n  return(result_df)\n}\n\n\n\n\n# get conditional table by column with 3 bins \nget_conditional_table_row(data=tmp_pivot%>%head(5),emoji='min_max')\n\n    prime_genre rating_0 rating_1 rating_2 rating_3 rating_4 rating_5\n1          Book  0.49 ‚¨ú  0.00 üü•  5.32 üü©  1.66 ‚¨ú  3.04 ‚¨ú  1.92 ‚¨ú\n2     Education  3.42 ‚¨ú  1.79 üü•  1.95 ‚¨ú  2.32 ‚¨ú  5.20 üü©  3.12 ‚¨ú\n3 Entertainment  0.51 üü•  1.99 üü©  0.78 ‚¨ú  0.90 ‚¨ú  0.95 ‚¨ú  1.03 ‚¨ú\n4       Finance  0.30 ‚¨ú  0.00 üü•  0.00 üü•  0.72 üü©  0.53 ‚¨ú  0.50 ‚¨ú\n5         Games  0.85 ‚¨ú  0.84 üü•  1.21 ‚¨ú  1.71 üü©  1.52 ‚¨ú  1.29 ‚¨ú\n\n\n\n# get conditional table by column using the max value\nget_conditional_table_column(data=tmp_pivot_5,emoji='max')\n\n# A tibble: 5 √ó 7\n  prime_genre   rating_0 rating_1 rating_2 rating_3 rating_4 rating_5\n  <chr>         <chr>    <chr>    <chr>    <chr>    <chr>    <chr>   \n1 Book          0.49 ‚¨ú  0.00 ‚¨ú  5.32 üü©  1.66 ‚¨ú  3.04 ‚¨ú  1.92 ‚¨ú \n2 Education     3.42 üü©  1.79 ‚¨ú  1.95 ‚¨ú  2.32 üü©  5.20 üü©  3.12 üü© \n3 Entertainment 0.51 ‚¨ú  1.99 üü©  0.78 ‚¨ú  0.90 ‚¨ú  0.95 ‚¨ú  1.03 ‚¨ú \n4 Finance       0.30 ‚¨ú  0.00 ‚¨ú  0.00 ‚¨ú  0.72 ‚¨ú  0.53 ‚¨ú  0.50 ‚¨ú \n5 Games         0.85 ‚¨ú  0.84 ‚¨ú  1.21 ‚¨ú  1.71 ‚¨ú  1.52 ‚¨ú  1.29 ‚¨ú \n\n\n\n# get conditional table by column using the circle emoji with 4 bins\nget_conditional_table_column(data=tmp_pivot_5,emoji='circle',bins=4)\n\n# A tibble: 5 √ó 7\n  prime_genre   rating_0 rating_1 rating_2 rating_3 rating_4 rating_5\n  <chr>         <chr>    <chr>    <chr>    <chr>    <chr>    <chr>   \n1 Book          0.49 üî¥  0.00 üî¥  5.32 üü¢  1.66 üü°  3.04 üü°  1.92 üü° \n2 Education     3.42 üü¢  1.79 üü¢  1.95 üü†  2.32 üü¢  5.20 üü¢  3.12 üü¢ \n3 Entertainment 0.51 üî¥  1.99 üü¢  0.78 üî¥  0.90 üî¥  0.95 üî¥  1.03 üî¥ \n4 Finance       0.30 üî¥  0.00 üî¥  0.00 üî¥  0.72 üî¥  0.53 üî¥  0.50 üî¥ \n5 Games         0.85 üî¥  0.84 üü†  1.21 üî¥  1.71 üü°  1.52 üî¥  1.29 üü† \n\nüìö References\nIf you want to learn‚Ä¶\nData Science for Psychologists, D.4 Using color packages\nUsing the flextable R package\nCreate Awesome HTML Table with knitr::kable and kableExtra\nViridisLite: Documentation\nIntroduction to the viridis color maps\nFormattable: Documentation\nReactablefmtr: Documentation\nOther references:\nImage preview reference: Imagen de vectorjuice en storyset\n\n\n\n",
    "preview": "posts_en/2024-01-14-transform-your-pandas-dataframes-in-r/preview.jpg",
    "last_modified": "2024-02-04T19:57:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2024-01-13-sql-assistant-text-to-sql-application-in-streamlit/",
    "title": "SQL Assistant: Text-to-SQL Application in Streamlit ü§ñ",
    "description": "In this article, we will explore the application of Vanna.ai, a Python library specifically designed for training a model capable of processing natural language questions and generating SQL queries as responses.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2024-01-13",
    "categories": [
      "Python",
      "Data",
      "DataViz",
      "AI"
    ],
    "contents": "\n\nContents\nü§ñ Text-to-SQL & Vanna.ai\nVanna.AI: Output Possibilities\nVanna.AI: Features\n\nüõ†Ô∏è Simple Implementation\nInstall vanna-ai\nImplementation Steps\n\nüß™ Model Training\nüí¨ Streamlit Chatbot\nDescription\nQuick Start\nü§ñ Chatbot Preview\n\nüìö References\n\n\nIn this article, we will explore the application of Vanna.ai, a üêç Python library specifically designed for training a model capable of processing natural language questions and generating SQL queries as responses.\nThis implementation will be integrated into a ‚ú®Streamlit application, creating a ü§ñchatbot that facilitates posing questions and provides explanations for the returned queries.\n\n\n\n\n\n\n\nü§ñ Text-to-SQL & Vanna.ai\nText-to-SQL is a tool that utilizes models to translate natural language queries into SQL queries, aiming to make it easy for users to generate SQL queries and interact with databases seamlessly.\nThe implementation of Text-to-SQL can be achieved through the use of Vanna.AI, an open-source üêçPython library that allows the training of an RAG model with queries, DDL, and documentation from a database.\n\nVanna.AI: Output Possibilities\nThe following are the possible outputs that can be generated with Vanna methods, facilitating diverse ways of interacting with the database using natural language.\nOutput\nDescription\nüìÑ SQL\nVanna.AI can generate SQL queries from natural language questions. These SQL queries can be used to interact with a database.\nüìÅ DataFrame\nAfter generating an SQL query, Vanna.AI can execute it in the database and return the results as a pandas DataFrame.\nüìä Charts\nVanna.AI can generate plots using Plotly from the results of the SQL query.\n‚ùì Follow-up questions\nVanna.AI can generate follow-up questions based on the generated SQL query. These follow-up questions can help users refine their queries or explore the data in more detail.\nüîç Explanations queries\nVanna.AI can provide explanations for the generated SQL queries. These explanations can help users understand how their natural language question is interpreted into an SQL query.\nVanna.AI: Features\nThe following table contains the key features available with Vanna.AI, enriching data interaction capabilities:\nFeature\nDescription\nüöÄ Model Training\nTrain the RAG model on data sources, including Data Definition Language (DDL) statements, documentation, and SQL queries during the training process.\nü§ñ User Query Handling\nUsers can pose questions in natural language, and Vanna.AI responds by generating SQL queries.\nüìö Documentation\nExtensive documentation, a dedicated website, and a support community on Discord are available to offer comprehensive assistance.\nüîå Database Connections\nVanna allows connecting to multiple databases, enabling users not only to retrieve SQL queries but also to execute them by establishing connections to their respective databases.\nü§î AI-Generated Recommendation Questions\nThis framework includes a feature of generating AI-driven questions, offering suggestions for additional queries that the user could explore.\nüõ†Ô∏è Simple Implementation\nFor this initial implementation, we will leverage an example provided by vanna.ai, using the Chinook database.\nThis database includes tables and data relevant to a music store, encompassing information about artists, albums, tracks, customers, orders, and various aspects associated with the management of an online music store.\n\nInstall vanna-ai\n!pip install vanna\nImplementation Steps\nFollow these steps to implement a straightforward example of text-to-SQL:\nAPI Key Retrieval: To initiate this example, acquire an API key by registering at https://vanna.ai/.\nUtilize your registered email to obtain the key.\nSetting API Key and Model: Configure the API key obtained and specify the model to be used, in this instance, the pre-existing ‚Äúchinook‚Äù model\nConnecting to the Database: Establish a connection with the database, utilizing an SQLite file available at https://vanna.ai/Chinook.sqlite.\nAsking a Question: Finally, pose a natural language question to extract specific information from the database.\nIn this step, the result includes the SQL query, the DataFrame from the query execution, and a representative chart.\nThe system also generates follow-up questions based on the dataset.\nimport vanna as vn\n\n# STEP 01: This is a simple example of how to use the Vanna API\napi_key = vn.get_api_key('your_email')\n\n# Set the API key and the model\nvn.set_api_key(api_key)\n\n# STEP 02:  Set the model\nvn.set_model('chinook')\n\n# STEP 03:  Connect with the database\nvn.connect_to_sqlite('https://vanna.ai/Chinook.sqlite')\n\n# STEP 04:  Ask a question\nvn.ask('What are the top 10 artists by sales?')\nüìÉsql query\nSELECT a.name,\n       sum(il.quantity) as totalsales\nFROM   artist a\n    INNER JOIN album al\n        ON a.artistid = al.artistid\n    INNER JOIN track t\n        ON al.albumid = t.albumid\n    INNER JOIN invoiceline il\n        ON t.trackid = il.trackid\nGROUP BY a.name\nORDER BY totalsales desc limit 10;\nüìäplotly chart\n\n‚ùìNew Questions\nAI-generated follow-up questions:\n\n* Who is the artist with the highest sales?\n* What are the total sales for each artist?\n* Which genre has the highest sales?\n* Can you provide a breakdown of sales by album?\n* Which artist has the lowest sales?\n* What is the average sales per artist?\n* Can you provide a list of the top-selling albums?\n* Which genre has the least sales?\n* Can you provide a breakdown of sales by country?\n* What is the total sales for each genre?\nüß™ Model Training\nTo train your own model follow the following steps: 1.\nLog in to your account https://vanna.ai/ and create a new model.\n2.\nNext we will define how to train the model.\nIn our next example we will use ddl (data definition language), documentation and queries.\n\n# Check the models available in the account\nvn.get_models()\n['ecommerce-test', 'demo-tpc-h', 'tpc', 'chinook', 'thelook']\n# Set the model\nvn.set_model(\"ecommerce-test\")\n# Get the ddl for training the model\n# Train the model with the ddl\nddl = \"\"\"\nCREATE TABLE if not exists stage.customers(\n    customer_id           INT NOT NULL PRIMARY KEY,\n    email_address         VARCHAR(50) NOT NULL,\n    name                  VARCHAR(50) NOT NULL,\n    business_type_id      INT NOT NULL,\n    site_code             VARCHAR(10) NOT NULL,\n    archived              BOOLEAN NOT NULL,\n    is_key_account        BOOLEAN NOT NULL,\n    date_updated          TIMESTAMP NOT NULL,\n    date_created          TIMESTAMP NOT NULL,\n    job_created_date  TIMESTAMP WITH TIME ZONE DEFAULT \n        CURRENT_TIMESTAMP,\n    job_created_user  varchar(50) default null,\n    job_updated_date  TIMESTAMP default null,\n    job_updated_user  varchar(50) default null,\n    CONSTRAINT fk_business_type_id FOREIGN KEY(business_type_id) REFERENCES stage.business_types (business_type_id)\n);\n\"\"\"\nvn.train(ddl=ddl)\n\nIn my repository, you can find all the scripts, documentation, and queries to train the model and answer questions such as the following\n\n# Ask a question for generating the SQL\nquestion  =  \"\"\"What is the total count of new clients who registered between October 1, 2020, and \nJanuary 1, 2022, and have made more than 10 purchases, each exceeding $20? Additionally,\n could you provide their email addresses, the number of purchases made, and the date of their\n  most recent purchase?\"\"\"\n\nprint(vn.generate_sql(question=question))\nSELECT COUNT(*) AS total_count,\n       c.email_address,\n       COUNT(o.order_id) AS num_purchases,\n       MAX(o.order_date) AS most_recent_purchase_date\nFROM Customers c\nJOIN Orders o ON c.customer_id = o.customer_id\nWHERE c.registration_date >= '2020-10-01' AND c.registration_date <= '2022-01-01'\n  AND o.order_value > 20\nGROUP BY c.email_address\nHAVING COUNT(o.order_id) > 10;\nüí¨ Streamlit Chatbot\nstreamlit - vanna-aiDescription\nIn this section, we will implement a ü§ñchatbot application using text-to-SQL capabilities with ‚ú®Streamlit.\nThis app will be developed through the integration of Vanna.AI and ‚ú®Streamlit, providing a user-friendly interface for entering your username, selecting an avatar, and initiating a üí¨chat.\nQuick Start\nClone the Repository {% github r0mymendez/text-to-sql %}\nAdd your ddl scripts, documentation and sql queries in src\\db\\\nAdd your credentials in src\\.streamlit\\secrets.toml\nExecute the Application\n\nDetailed instructions on how to run the application and add credentials can be found in the repository‚Äôs README.md.\n\nü§ñ Chatbot Preview\nThe application, crafted with Vanna.AI and ‚ú®Streamlit, and you can see below a video of how it works, remember that all the explanations are in the readme.md file of the repository.\nFeel free to check it out and give it a star if you find it helpful!\n‚≠êÔ∏è\n\n\n\n\n\n\n\n\nchatbot-3üìö References\nIf you want to learn‚Ä¶\nvanna-ai\n‚ú®streamlit\nOther references:\nImage preview reference: [Imagen de vectorjuice en Freepik]\n\n‚ö†Ô∏è Disclaimer: Before proceeding with the use of your own data in the Vanna.AI application, it is recommended to review and validate the data protection policies established by Vanna.AI. Ensure a thorough understanding of how your personal data is handled and protected within the application framework. It is crucial to comply with privacy and security regulations before providing any sensitive information.\n\n\n\n\n",
    "preview": "posts_en/2024-01-13-sql-assistant-text-to-sql-application-in-streamlit/preview.jpg",
    "last_modified": "2024-01-14T07:39:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2024-01-02-transform-your-pandas-dataframes/",
    "title": "Transform your Pandas Dataframes: Styles, üé® Colors, and üòé Emojis",
    "description": "In the following article, we will explore a method to add colors and styles to Pandas DataFrames.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2024-01-02",
    "categories": [
      "Python",
      "Data",
      "DataViz"
    ],
    "contents": "\n\nContents\nWhat is Pandas Style?\nüü£ Pivot Tables\nExample\n\nüü£ Dataframe: Apple Store apps\nData Schema overview\n\nüü£ Create Dataframe\nüü£ Pivot Table\nüü£ Styling with Pandas\nüé® Styling: Setting Background Color for Headers\nüé® Style: Setting the background color for a particular cell\nüé® Style: Setting the background color for max/min values in the dataframe\nüé® Style: Color Background Gradients\nüé® Style: Color Background in columns\nüé® Style: Color Bar\nüé® Style: Image in Columns\nüé® Style: Emoji Representation Based on Percentile Values\n\nüìö References\n\n\n\nIn the following section of this article, we will explore a method to add colors and styles to Pandas DataFrames.\nOur focus will be on the application of colors and emojis, utilizing approaches similar to the popular conditional formatting commonly employed in pivot tables within spreadsheets.\nThrough this strategy, we aim to enhance the presentation of our data, making the exploration and understanding of information not only informative but also visually appealing\nimage generated using partyrockWhat is Pandas Style?\nPandas Styler is a module within the Pandas library that provides methods for creating HTML-styled representations of DataFrames.\nThis feature allows for the customization of the visual appearance of DataFrames during their visualization.\nThe core functionality of Pandas Styler lies in the ability to highlight, color, and format cells based on specific conditions, facilitating the visual identification of patterns and trends in datasets.\nAlso, Pandas Styler stands out for its capability to assist in the design of DataFrames or series by generating visual representations using HTML and CSS.\nThis functionality simplifies the creation of attractive and customized data presentations, enhancing the visualization experience, and enabling a more intuitive interpretation of the information contained in the datasets.\n\nNext we have the code with we are going to create a pivot table using a set of data and from this you will begin to give it different styles and conditional formats such as can be seen in the previous image.\nüü£ Pivot Tables\n\nThe pivot table is a tabular data structure that provides a summarized overview of information from another table, organizing the data based on one variable and displaying values associated with another variable.\nIn this specific scenario, the pivot table organizes the data according to the ‚Äòsmoker‚Äô column and presents the total sum of tips, categorized by the days on which clients consume in the restaurant\n\nExample\nThe following example shows the pivot_table method with the ‚Äòtips‚Äô DataFrame\n\npython code\nimport pandas as pd\nimport seaborn as sns\n\n# create the tips dataframe \ndata = sns.load_dataset('tips')\ndata_pivot = pd.pivot_table(data,\n                    index='smoker',\n                    columns='day',\n                    values='total_bill',\n                    aggfunc='sum').reset_index()\ndata_pivot\nouput\nday\nsmoker\nThur\nFri\nSat\nSun\n0\nYes\n326.24\n252.20\n893.62\n458.28\n1\nNo\n770.09\n73.68\n884.78\n1168.88\nüü£ Dataframe: Apple Store apps\nIn this analysis, we will use the ‚Äòüçé Apple Store apps‚Äô DataFrame to explore the creation of pivot tables and customization of table styles.\nThis dataset provides detailed insights into Apple App Store applications, covering aspects from app names to specifics like size, price, and ratings.\nOur objective is to efficiently break down the information while applying styles that enhance the presentation and comprehension of data effectively.\nThe dataset was downloaded from Kaggle and it contains more than 7000 Apple iOS mobile application details.\nIt is important to note that the data was collected in July 2017.\nData Schema overview\ncolumn_name\n¬†column description\ntrack_name\nthe column contains the name of the app.\nsize_bytes\nthe column contains the size of the app in bytes.\ncurrency\nthe column contains the currency type.\nprice\nthe column contains the price of the app.\nrating_count_tot\nthe column contains the total number of ratings.\nrating_count_ver\nthe column contains the number of ratings for the current version of the app.\nuser_rating\nthe column contains the average user rating for the app.\nuser_rating_ver\nthe column contains the average user rating for the current version of the app.\nver\nthe column contains the current version of the app.\ncont_rating\nthe column contains the content rating.\nprime_genre\nthe column contains the primary genre.\nsup_devices.num\nthe column contains the number of supported devices.\nipadSc_urls.num\nthe column contains the number of screenshots showed for display.\nlang.num\nthe column contains the number of supported languages.\nvpp_lic\nthe column contains the Vpp Device Based Licensing Enabled.\nüü£ Create Dataframe\nIn the following code chunk, we will create a DataFrame by reading the CSV file.\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport warnings\n\n# Deactivate pandas warning\nwarnings.filterwarnings('ignore')\n\n\nprint(\"Python Libraries version:\")\nprint('--'*20)\nprint(\"Pandas version: \", pd.__version__)\nprint(\"Numpy version: \", np.__version__)\nprint(\"Matplotlib version: \", plt.matplotlib.__version__)\nPython Libraries version:\n----------------------------------------\nPandas version:  2.1.3\nNumpy version:  1.26.1\nMatplotlib version:  3.8.1\n# Create a dataframe from a csv file\n# You can download the file from the following link https://github.com/r0mymendez/pandas-styles\npath='data/AppleStore.csv'\ndata =pd.read_csv(path,sep=';')\nüü£ Pivot Table\nIn the next step, we are going to create a pivot table from a DataFrame.\n# Pivot table\n\n# filter the data to keep only the top 15 genres\ntop_genre = data.value_counts('prime_genre')[:15].index.tolist()\ntmp = data.loc[data['prime_genre'].isin(top_genre),['prime_genre','user_rating','price']]\n\n# create a new column with the rating rounded to the nearest integer\ntmp['user_rating'] = [f'rating_{str(math.trunc(item))}' for item in  tmp['user_rating']]\n\n# create a pivot table\ntmp_pivot = (\n        pd.pivot_table(\n            data = tmp,\n            columns='user_rating',\n            index='prime_genre',\n            values='price',\n            aggfunc='mean',\n            fill_value=0\n            ).reset_index().round(2)\n)\n# rename the columns\ntmp_pivot.columns.name=''\n# print the pivot table\ntmp_pivot\n\nüü£ Styling with Pandas\nNow we will explore the style module in Pandas, that enables us to enhance the visual presentation of DataFrames.\nThe style module provides a differents of options to modify the appearance of the data, allowing us to customize aspects such as:\nColoring Cells: Apply different colors based on cell values or conditions.\nHighlighting: Emphasize specific rows, columns, or values.\nFormatting: Adjust the format of the displayed values, including precision and alignment.\nBar Charts: Represent data with horizontal or vertical bar charts within cells.\nüé® Styling: Setting Background Color for Headers\nIn this section, we will apply styles to both the titles and the table.\nTherefore we use background colors to highlight the headers and the rest of the table.\n# Styling: Changing Background Color for Column Headers\nheaders = {\n    'selector': 'th.col_heading',\n    'props': 'background-color: #5E17EB; color: white;'\n}\n\nindex_style = {\n    'selector': 'th.index_name',\n    'props': 'background-color: #5E17EB; color: white;'\n}\n\ntmp_pivot_style = (\n    tmp_pivot\n        .style\n            .set_table_styles([headers,index_style])\n            .set_properties(**{'background-color': '#ECE3FF','color': 'black'})\n)\n\ntmp_pivot_style\n\nüé® Style: Setting the background color for a particular cell\nIn following code snippet illustrates how to set a custom background color for a particular cell in our DataFrame using pandas styling.\n(\n    tmp_pivot\n        .style\n            .set_table_styles([headers, index_style])\n            .set_properties(**{'background-color': '#ECE3FF', 'color': 'black'})\n            .set_properties(**{'background-color': '#FD636B', 'color': 'white'},subset=pd.IndexSlice[4, 'rating_5'])\n)\n\nüé® Style: Setting the background color for max/min values in the dataframe\nNow, we will focus on highlighting the maximum and minimum values in our DataFrame.\nFor this reason, we will assign distinctive background colors to these extreme values, facilitating a quicker and more intuitive understanding of the dataset.\nThe code snippet below demonstrates how to implement this stylistic enhancement.\n#¬†select the columns that start with 'rating_'\ncolumns = tmp_pivot.columns[tmp_pivot.columns.str.startswith('rating_')]\n\n#¬†get the max and min values\nmax_value = tmp_pivot[columns].max().max()\nmin_value = tmp_pivot[columns].min().min()\n\n# Establecer el estilo para la celda con el valor m√°ximo\nmax_style = f'border: 4px solid #3BE8B0 !important;'\n\n# Establecer el estilo para la celda con el valor m√≠nimo\nmin_style = f'background-color: #FF66C4; '\n\n(\n    tmp_pivot\n        .style\n            .set_table_styles([headers, index_style])\n            .set_properties(**{'background-color': '#ECE3FF', 'color': 'black'})\n            .set_properties(**{'background-color': '#FD636B', 'color': 'white'}, subset=pd.IndexSlice[4, 'rating_5'])\n            .applymap(lambda x: max_style if x == max_value else '')\n            .applymap(lambda x: min_style if x == min_value else '', subset=columns)\n)\n\nüé® Style: Color Background Gradients\nIn the upcoming section, we will delve into the concept of color maps, representing a spectrum of colors arranged in a gradient.\nA colormap, essentially a palette of colors, consists of distinctive denominations, with the most popular ones being [‚Äòviridis,‚Äô ‚Äòmagma,‚Äô ‚ÄòGreens,‚Äô ‚ÄòReds‚Äô].\nThe primary objective behind creating these color spectrums is to enhance the visual representation of data.\nEach color in the gradient carries specific nuances, contributing to a more nuanced data visualization experience.\nFor an extensive array of color options, you can explore the matplotlib colormaps link.\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the colormap\nfor cmap_item in ['viridis', 'magma','Greens','Reds']:\n    cmap = plt.get_cmap(cmap_item)\n    # Create a color gradient\n    gradient = np.linspace(0, 1, 256).reshape(1, -1)\n\n    # Display the color palette\n    plt.figure(figsize=(10, 0.2))\n    plt.imshow(gradient, aspect='auto', cmap=cmap)\n    plt.axis('off')\n    plt.title(f'{cmap_item.capitalize()} Color Palette', loc='left', fontsize=9)\n    plt.show()\n\n\nViridis palette\nNow, we will apply a color gradient to our pivot table, allowing you to observe how it is colored using the Viridis palette.\nIn this context, lighter colors signify larger values within the distribution, while darker shades correspond to smaller values in the distribution.\nThis approach provides a visual representation that intuitively conveys the magnitude of the data, making it easier to discern patterns and variations across the dataset.\n\nplt.get_cmap('viridis',lut=20)\n\n(\n    tmp_pivot\n        .style\n            .set_table_styles([headers, index_style])\n            .background_gradient(cmap='viridis',subset=columns)\n)\n\nüé® Style: Color Background in columns\nIn the next code chunk, we will enhance the visual representation of our pivot table by introducing distinct color backgrounds to specific columns.\nThis technique aids in better highlighting and categorizing data, making it easier to draw insights from the table.\n(\n    tmp_pivot\n        .style\n            .set_table_styles([headers, index_style])\n            .set_properties(**{'background-color': '#FFCFC9','color':'black'},subset=['rating_0','rating_1'])\n            .set_properties(**{'background-color': '#FFF1B0','color':'black'},subset=['rating_2','rating_3'])\n            .set_properties(**{'background-color': '#BEEAE5','color':'black'},subset=['rating_4','rating_5'])\n)\nIüé® Style: Color Bar\nIn this section, we will implement the style.bar function to introduce a dynamic color bar into our DataFrame.\nThe color bar provides a visual representation of data values, assigning varying colors to different data ranges.\n(\n    tmp_pivot\n        .style\n            .set_table_styles([headers, index_style])\n            .set_properties(**{'background-color': '#ECE3FF', 'color': 'black'})\n            .set_properties(**{'background-color': 'white','color':'black'},subset=columns)\n            .bar(color='#FFCFC9',subset=['rating_0','rating_1'])\n            .bar(color='#FFF1B0',subset=['rating_2','rating_3'])\n            .bar(color='#BEEAE5',subset=['rating_4','rating_5'])\n )\n\nüé® Style: Image in Columns\nIn this section, we explore the enhancement of data representation by adding an image to an additional column.\nThis approach provides an alternative method to elevate the visual impact of the data being presented.\nThese images can serve as icons, represent brands, or convey additional visual elements to captivate and engage the audience.\n# create a function to add an image to the dataframe depending on the genre\ndef add_image(image_name):\n    img_url = f\"img/icons/img_{image_name}.png\"\n    width   = \"width: 50px\"\n    height  = \"height: 50px\"\n    text_align =\"center\"\n    return f'{width};{height}; content: url({img_url}); text-align:{text_align}'\n\n# apply the function to the dataframe\nstyled_df = (\n    tmp_pivot\n        .head(5)\n        .reset_index()\n        .rename({'index': 'genre'}, axis=1)\n        .style.applymap(add_image, subset=pd.IndexSlice[:, ['genre']])\n        .set_table_styles([headers, index_style])\n        .set_properties(**{'background-color': '#ECE3FF', 'color': 'black'})\n)\n\n# display the dataframe with the images\ndisplay(styled_df)\n\n\nDisclaimer: Issues with Notebook Cache\nDuring the creation of this content, I encountered difficulties related to the notebook cache.\nDespite making changes to the images, the visualization did not update correctly.\nEven after attempting to restart the kernel and clear the cell output, the problem persisted.\nThe only effective solution I found was to change the file names of the images, thus avoiding unexpected cache behavior.\nIt‚Äôs important to note that these issues may be specific to the Jupyter Notebooks environment and may not reflect inherent limitations in the code or libraries used.\nWhile I tried to address this problem, I did not find a complete solution and opted for an alternative fix by changing the file names.\nIf you have suggestions or additional solutions, I would be delighted to learn and improve this process.\n\nüé® Style: Emoji Representation Based on Percentile Values\nIn this section, we delve into the creative use of emojis based on percentile values, offering a distinctive approach to elevate data representation.\nBy incorporating diverse emojis, we enhance the visual impact of the data.\nSpecifically, we employ circles and squads as emojis to bring nuanced expressions to our data points.\nIf you‚Äôd like to view the code for creating this style, it‚Äôs available in my GitHub repository.\nFeel free to check it out and give it a star if you find it helpful!\n‚≠êÔ∏è\n\n\n\nüìö References\nIf you want to learn‚Ä¶ * üêº Pandas Style Documentation\nOther references: * Image preview reference: [Imagen de vectorjuice en Freepik]\n\n\n\n",
    "preview": "posts_en/2024-01-02-transform-your-pandas-dataframes/preview.jpg",
    "last_modified": "2024-01-02T23:04:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-12-22-decoding-a-data-model-using-schemaspy-in-snowflake/",
    "title": "Decoding a Data Model - Using SchemaSpy in Snowflake",
    "description": "In following article, we will do on the implementation journey of SchemaPy, leveraging Docker for deployment, and employing a Snowflake database.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2023-12-22",
    "categories": [
      "Python",
      "SQL",
      "Database",
      "Data"
    ],
    "contents": "\n\nContents\nData Model\nConceptual Data Model (CDM)\nLogical Data Model (LDM)\nPhysical data model\n\n‚ùÑÔ∏èSnowflake SchemaSpy üê≥ Docker Compose\nüöÄ Benefits of SchemaSpy\nPrerequisites\nUsage\nüìÅ Clone this repository\n‚ùÑÔ∏è Snowflake Configuration\n‚ùÑÔ∏è Snowflake Account\n\nüêô Build and run the Docker Compose environment\n\nüîç Schemapy\nFunctionality Tabs\nSchemaSpy UI\n\nüìö References\n\n\nIn this article, we will delve into a comprehensive exploration of the intricacies of data modeling, spanning from its conceptual inception to its logical definition and eventual physical implementation.\nUnderstanding the life cycle of a data model is crucial for efficiently designing and managing databases.\nFurthermore, in real-world scenarios, existing databases often necessitate reverse engineering to unveil and comprehend their underlying structures.\nIn following sections, we will do on the implementation journey of SchemaPy, leveraging Docker for deployment, and employing a Snowflake database.\n\n\n\n\n\n\n\nData Model\nData modeling is a fundamental task that provides us with a clear understanding of data and the most efficient way to store it.\nThis approach involves representing entities and relationships based on business processes, facilitating documentation and the efficient exploration of data.\nThe ability to generate these documents and understand how data is stored is essential knowledge for any data team.\nIn this context, we will delve into the significance and process of data modeling, emphasizing how this practice becomes a valuable tool for the effective management and exploitation of information.\n\nConceptual Data Model (CDM)\nA conceptual data model (CDM) operates at a high level and offers an organization‚Äôs data needs.\nIt defines a broad and simplified view of the data and the purpose of conceptual data modeling is to establish a shared understanding of the business by capturing the essential concepts of a business process.\nThe focus is on abstracting and representing key entities, relationships, and their interdependencies, fostering a common understanding among stakeholders about the fundamental aspects of the business and its data requirements.\nBelow is an example of a CDM\n\nLogical Data Model (LDM)\nA logical data model (LDM) extends the conceptual data model by providing a complete definition of relationships, incorporating details and the structure of essential entities.\nIn summary the LDM encompasses specific attributes for each entity and the relationships between entities.\nBelow is an example of a LDM\n\nPhysical data model\nA Physical Data Model (PDM) outlines how the model will be translated into an actual database.\nThe PDM incorporates all necessary physical specifications essential for constructing a database, providing a comprehensive guide for database construction, including tables, columns, data types, indexes, constraints, and other implementation details tailored to the chosen database management system.\n\n‚ùÑÔ∏èSnowflake SchemaSpy üê≥ Docker Compose\nIn this following example, we will implement of SchemaSpy through a Docker image in a Snowflake database.\nIt‚Äôs important to note that the implementation can be extended to other databases such as: mysql, PostgreSQL, Oracle, SQL Server, DB2, H2, HSQLDB, SQLite, Firebird, Redshift and Informix.\nSchemaSpy is a tool that generates an HTML representation of a database schema‚Äôs relationships, providing insights into the database structure.\nSnowflake is a cloud-based data storage and processing service that provides a highly scalable and fully managed data storage environment.\nIts architecture is built on the separation of storage and computing, allowing elastic scalability and optimal performance.\nüöÄ Benefits of SchemaSpy\nVisual representation of the database schema.\nRelationship insights between tables.\nHTML report for easy sharing and documentation.\nPrerequisites\nBefore you begin, ensure that you have the following installed:\nüê≥ Docker\nüêô Docker Compose\n‚ùÑÔ∏è Snowflake account (You can create an account with a 30-day free trial which includes $400 worth of free usage.)\nUsage\nüìÅ Clone this repository\nsource:r0mymendez/schemaspy-snowflakegit clone https://github.com/r0mymendez/schemaspy-snowflake.git\ncd schemaspy-snowflake\n‚ùÑÔ∏è Snowflake Configuration\nAt this stage, you need to configure the configuration file located at the following path: config/snowflake.properties.\nThis configuration will be used to establish the connection to Snowflake, so it is necessary to modify the account, role, warehouse, schema, and database settings.\nschemaspy.t=snowflake\nschemaspy.u=<user>\nschemaspy.p=<password>\nschemaspy.account=<account>\nschemaspy.role=<role\nschemaspy.warehouse=<warehouse>\nschemaspy.db=<database>\nschemaspy.s=<schema>\n‚ùÑÔ∏è Snowflake Account\nBelow is an example of an account URL.\nHowever, you only need to use a portion of it in the configuration:\nFull account URL: https://%60nl1111.eu-west-3.aws`.snowflakecomputing.com\nAccount to use in the configuration: nl1111.eu-west-3.aws\nImage descriptionüêô Build and run the Docker Compose environment\nThis command will build the Docker image and start the container.\ndocker-compose -f docker-compose-snowflake.yaml up\nüîç Schemapy\nAfter executing Docker Compose, you‚Äôll find a web site in the output folder.\nThis page features multiple tabs, each offering distinct functionalities, and below, we will explain them.\nFunctionality Tabs\nTables: Provides an overview of all tables in the database schema.\nColumns: Displays detailed information about columns within each table.\nConstraints: Offers insights into constraints applied to the database.\nRelationships: Visualizes the relationships between different tables.\nOrphan Tables: Identifies tables without established relationships.\nAnomalies: Highlights any irregularities or anomalies in the schema.\nRoutines: Presents information about stored routines or procedures.\nSchemaSpy UI\nIn the provided example, we showcase a demo utilizing a synthetic database called Synthea.\nSynthea is a state-of-the-art synthetic data generation tool designed for creating realistic, yet entirely fictitious, patient records and healthcare data.\nIt enables the simulation of diverse medical scenarios, making it a valuable resource for testing and development purposes in the healthcare domain.\nVisit the following üëâlink to access a demoüëà.\n\n\nüìö References\nIf you want to learn‚Ä¶\nSnowflake\nSnowflake Free courses\nSchemaSpy\nSchemaSpy: Docker Image\nSynthea Project\nOther references:\n- Image preview reference: [Imagen de vectorjuice en Freepik]\n\n\n\n",
    "preview": "posts_en/2023-12-22-decoding-a-data-model-using-schemaspy-in-snowflake/preview.jpg",
    "last_modified": "2023-12-24T18:31:23+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-12-17-sql-to-python-pandas-a-sql-users-quick-guide/",
    "title": "SQL to Python Pandas: A SQL User's Quick Guide",
    "description": "Unlock the essentials of translating your code from SQL to Python with this quick guide tailored for SQL users. Dive into key insights and streamline your data manipulation process.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2023-12-17",
    "categories": [
      "Python",
      "SQL",
      "Database",
      "Data"
    ],
    "contents": "\n\nContents\nNew York Flights ‚úàÔ∏è üß≥ üóΩ\nEntity-relationship diagram [DER]\nInstallation: Setting Up nycflights13\n\nüü¢ Pandas, NumPy, and nycflights13 for Data Analysis in Python\nüü¢ SELECT and FROM Statements\nüìó SELECT: All Columns\nüìó SELECT: Specific Columns\n\nüü¢ Filtering Operators (WHERE)\nüìó Utilizing ‚ÄòWHERE‚Äô for Equality ( = )\nüìó Utilizing ‚ÄòWHERE‚Äô for Equality ( = )\nüìó Utilizing ‚ÄòWHERE‚Äô with Inequality ( != )\nüìó Utilizing ‚ÄòWHERE‚Äô for Comparisons (>=, <=, <, >)\nüìó Utilizing ‚ÄòWHERE‚Äô with between operator\nüìó Utilizing ‚ÄòWHERE‚Äô with ‚ÄúLIKE‚Äù Clause\nüìó Utilizing ‚ÄòWHERE‚Äô with Null or Not Null Values\n\nüü¢ Order by Statement\nüü¢ Distinct Values: Removing Duplicates from Results\nüü¢ Adding Calculated Columns\nüü¢ Group by Statement\nüü¢ Group by and Having Statement\nüü¢ Group by with multiple calculations\nüü¢ Union Statement\nüü¢ CASE WHEN Statement\nüü¢ JOIN Statement\nüìó Join Types\nüìó Join Key\n\nüü¢ Rename\nüìö References\n\n\nIn this post, we will compare the implementation of Pandas and SQL for data queries.\nWe‚Äôll explore how to use Pandas in a manner similar to SQL by translating SQL queries into Pandas operations.\nIt‚Äôs important to note that there are various ways to achieve similar results, and the translation of SQL queries to Pandas will be done by employing some of its core methods.\nNew York Flights ‚úàÔ∏è üß≥ üóΩ\n\nsource image Image by upklyak on Freepik\nWe aim to explore the diverse Python Pandas methods, focusing on their application through the nycflights13 datasets.\nThis datasets offer comprehensive information about airlines, airports, weather conditions, and aircraft for all flights passing through New York airports in 2013.\nThrough this exercise, we‚Äôll not only explore Pandas functionality but also learn to apply fundamental SQL concepts in a Python data manipulation environment.\nEntity-relationship diagram [DER]\n\nThe nycflights13 library contains tables with flight data from New York airports in 2023.\nBelow, you can find a high-level representation of an entity-relationship diagram with its five tables.\n\n\nInstallation: Setting Up nycflights13\nTo install the nycflights13 library, you can use the following command:\n!pip install nycflights13\nThis library provides datasets containing comprehensive information about flights from New York airports in 2023.\nOnce installed, you can easily access and analyze this flight data using various tools and functionalities provided by the nycflights13 package.\nüü¢ Pandas, NumPy, and nycflights13 for Data Analysis in Python\nIn the next code snippet, we are importing essential Python libraries for data analysis.\n* üìó Pandas is a library for data manipulation and analysis, * üìó Numpy provides support for numerical operations * üìó Nycflights13 is a specialized library containing datasets related to flights from New York airports in 2023.\nimport pandas as pd\nimport numpy as np\nimport nycflights13 as nyc\nIn the following lines of code, we are assigning two specific datasets from the nycflights13 library to variables.\nflights = nyc.flights\nairlines = nyc.airlines\nüü¢ SELECT and FROM Statements\nüìó SELECT: All Columns\nThe following SQL query retrieves all columns and rows from the ‚Äúüõ©Ô∏è flights‚Äù table.\nIn Pandas, the equivalent is simply writing the DataFrame name, in this case, ‚Äúflights.‚Äù For example:\nüîçsql\n  SELECT * FROM flights;\nüêçpython\nflights\nüìó SELECT: Specific Columns\nTo select specific columns from a Pandas DataFrame, you can use the following syntax:\nüîçsql\n  select \n    year, \n    month, \n    day, \n    dep_time, \n    flight, \n    tailnum, \n    origin, \n    dest \n  from flights;\nüêçpython\n(\n    flights\n        .filter(['year', 'month', 'day', 'dep_time', 'flight', \n                'tailnum', 'origin', 'dest'])\n)\nüü¢ Filtering Operators (WHERE)\nüìó Utilizing ‚ÄòWHERE‚Äô for Equality ( = )\nTo filter all ‚úàÔ∏è flights where the origin is ‚ÄòJFK‚Äô in Pandas, you can use the following code:\nüîçsql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights \n  where origin = 'JFK'\nlimit 10;\nüêçpython\n(   flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest'])\n      .query(\"origin=='JFK'\")\n      .head(10)\n)\nüìó Utilizing ‚ÄòWHERE‚Äô for Equality ( = )\nTo achieve the same filtering in Pandas for specific criteria: * ‚úàÔ∏è Flights departing from JFK, LGA, or EWR.\nüîçsql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights \n  where origin in ( 'JFK', 'LGA', 'EWR' ) \nlimit 10;\nüêçpython\n (  flights\n        .filter(['year', 'month', 'day', 'dep_time', 'flight', \n        'tailnum', 'origin', 'dest'])\n      .query(\"origin in ['JFK', 'EWR', 'LGA']\")\n      .head(10)\n)\nüìó Utilizing ‚ÄòWHERE‚Äô with Inequality ( != )\nTo achieve the same filtering in Pandas for specific criteria:\n‚úàÔ∏è Flights departing from JFK, LGA, or EWR.\n‚úàÔ∏è Flights not destined for Miami (MIA).\nüîçsql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' ) and dest<>'MIA'\nlimit 10;\nüêçpython\n(   flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest'])\n      .query(\"(origin in ['JFK', 'EWR', 'LGA']) and (dest != 'MIA')\")\n   .head(10)\n)\nüìó Utilizing ‚ÄòWHERE‚Äô for Comparisons (>=, <=, <, >)\nTo achieve the same filtering in Pandas for specific criteria:\n‚úàÔ∏è Flights departing from JFK, LGA, or EWR.\n‚úàÔ∏è Flights not destined for Miami (MIA).\n‚úàÔ∏è Flights with a distance less than or equal to 1000 km.\nüîçsql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and distance < = 1000\nlimit 10;\nüêçpython\n( flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight',\n            'tailnum', 'origin', 'dest', 'time_hour', 'distance'])\n      .query(\"(origin in ['JFK', 'EWR', 'LGA']) and (dest != 'MIA') and (distance <= 1000)\")\n      .head(10)\n)\nüìó Utilizing ‚ÄòWHERE‚Äô with between operator\nTo achieve the same filtering in Pandas for specific criteria:\n‚úàÔ∏è Flights departing from JFK, LGA, or EWR.\n‚úàÔ∏è Flights not destined for Miami (MIA).\n‚úàÔ∏è Flights with a distance less than or equal to 1000 km.\n‚úàÔ∏è Flights within the period from September 1, 2013, to September 30, 2013.\n\nüîçsql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and distance < = 1000\n   and time_hour between '2013-09-01' and '2012-09-30'\nlimit 10;\nüêçpython\n(   flights.filter([['year', 'month', 'day', 'dep_time', 'flight', \n          'tailnum', 'origin', 'dest', 'time_hour', 'distance'])\n      .query(\n            \"(origin in ['JFK', 'EWR', 'LGA'])\" \n             \" and (dest != 'MIA')\"\n             \" and (distance <= 1000)\"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n         )\n      .head(10)\n)\nüìó Utilizing ‚ÄòWHERE‚Äô with ‚ÄúLIKE‚Äù Clause\nTo achieve the same filtering in Pandas for specific criteria:\n‚úàÔ∏è Flights departing from JFK, LGA, or EWR.\n‚úàÔ∏è Flights not destined for Miami (MIA).\n‚úàÔ∏è Flights with a distance less than or equal to 1000 km.\n‚úàÔ∏è Flights within the period from September 1, 2013, to September 30, 2013.\n‚úàÔ∏è Flights where the tailnum contains ‚ÄòN5‚Äô in the text.\nYou can use the following code:\nüîçsql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and distance < = 1000\n   and time_hour between '2013-09-01' and '2012-09-30'\n   and tailnum like '%N5%'\nlimit 10;\nüêçpython\n(\n    flights     \n      .filter(['year', 'month', 'day', 'dep_time', 'flight', 'tailnum',\n        'origin', 'dest', 'time_hour'])\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and (dest != 'MIA') \"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n             \" and (tailnum.str.find('N5')>=0)\"\n       )\n      .head(10)\n)\nüìó Utilizing ‚ÄòWHERE‚Äô with Null or Not Null Values\nTo achieve the same filtering in Pandas for specific criteria:\n‚úàÔ∏è Flights departing from JFK, LGA, or EWR.\n‚úàÔ∏è Flights not destined for Miami (MIA).\n‚úàÔ∏è Flights with a distance less than or equal to 1000 km.\n‚úàÔ∏è Flights within the period from September 1, 2013, to September 30, 2013.\n‚úàÔ∏è Flights where the tailnum contains ‚ÄòN5‚Äô in the text.\n‚úàÔ∏è Flights where dep_time is null\nYou can use the following code:\nüîçsql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and distance < = 1000\n   and time_hour between '2013-09-01' and '2012-09-30'\n   and tailnum like '%N5%'\n   and dep_time is null\nlimit 10;\nüêçpython\n(\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest', 'time_hour'])\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and (dest != 'MIA') \"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n             \" and (tailnum.str.find('N5')>=0)\"\n             \" and dep_time.isnull()\"\n       )\n      .head(10)\n)\nüü¢ Order by Statement\nThe .sort_values() methods in Pandas are equivalent to the ORDER BY clause in SQL.\n1Ô∏è‚É£.**.sort_values(['origin','dest'], ascending=False)**: This method sorts the DataFrame based on the ‚Äòorigin‚Äô and ‚Äòdest‚Äô columns in descending order (from highest to lowest).\nIn SQL, this would be similar to the ORDER BY origin DESC, dest DESC clause.\n2Ô∏è‚É£.**.sort_values(['day'], ascending=True)**: This method sorts the DataFrame based on the ‚Äòday‚Äô column in ascending order (lowest to highest).\nIn SQL, this would be similar to the ORDER BY day ASC clause.\nBoth methods allow you to sort your DataFrame according to one or more columns, specifying the sorting direction with the ascending parameter.\nTrue means ascending order, and False means descending order.\nüîçsql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and distance < = 1000\n   and time_hour between '2013-09-01' and '2012-09-30'\n   and tailnum like '%N5%'\n   and dep_time is null\norder by  origin, dest desc\nlimit 10;\nüêçpython\n(\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest', 'time_hour'])\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and (dest != 'MIA') \"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n             \" and (tailnum.str.find('N5')>=0)\"\n             \" and year.notnull()\"\n       )\n      .sort_values(['origin','dest'],ascending=False)\n      .head(10)\n      \n)\nüü¢ Distinct Values: Removing Duplicates from Results\nTo perform a distinct select in pandas, you need to first execute the entire query, and then apply the drop_duplicates() method to eliminate all duplicate rows.\nüîçsql\nselect distinct origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and time_hour between '2013-09-01' and '2012-09-30'\norder by  origin, dest desc;\nüêçpython\n(\n    flights\n      .filter(['origin','dest','time_hour'])\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and (dest != 'MIA') \"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n       )\n      .filter(['origin','dest'])\n      .drop_duplicates()\n      \n)\nüü¢ Adding Calculated Columns\nNow, let‚Äôs introduce a new calculated column called ‚Äúdelay_total,‚Äù where we sum the values from the ‚Äúdep_delay‚Äù and ‚Äúarr_delay‚Äù columns.\nüîçsql\nselect  \n  origin,\n  dest,\n  time_hour,\n  dep_delay,\n  arr_delay,\n  flights.dep_delay + flights.arr_delay as delay_total \nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and time_hour between '2013-09-01' and '2012-09-30';\nüêçpython\n(\n    flights\n      .filter(['origin', 'dest', 'time_hour', 'dep_delay', 'arr_delay'])\n      .assign(delay_total = flights.dep_delay + flights.arr_delay )\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and (dest != 'MIA') \"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n       )\n)\nüü¢ Group by Statement\nTo perform a GROUP BY operation in pandas, we‚Äôll use the groupby method, which operates similarly to its SQL counterpart.\nSimilarly, we can employ common aggregate functions such as sum, max, min, mean (equivalent to avg in SQL), and count.\nBelow is a simple example to illustrate this process:\nüîçsql\nselect \n  year,\n  month,\n  max(dep_delay) as dep_delay,\nfrom flights\ngroup by \n  year,\n  month;\nüêçpython\n(\n    flights\n      .groupby(['year','month'],as_index=False)\n      ['dep_delay'].max()\n)\nüü¢ Group by and Having Statement\nIn the following example, we‚Äôll explore how to implement a HAVING clause in pandas, leveraging the query method, as we‚Äôve done previously for filtering.\nüîçsql\nselect \n  year,\n  month,\n  max(dep_delay) as dep_delay,\nfrom flights\ngroup by \n  year,\n  month\nhaving max(dep_delay)>1000\nüêçpython\n(\n    flights\n      .groupby(['year','month'],as_index=False)['dep_delay']\n      .max()\n      .query('(dep_delay>1000)') # having\n)\nüü¢ Group by with multiple calculations\nWhen working with pandas and needing to perform multiple calculations on the same column or across different columns, the agg function becomes a valuable tool.\nIt allows you to specify a list of calculations to be applied, providing flexibility and efficiency in data analysis.\nConsider the following SQL query:\nüîçsql\nselect \n  year,\n  month,\n  max(dep_delay)  as dep_delay_max,\n  min(dep_delay)  as dep_delay_min,\n  mean(dep_delay) as dep_delay_mean,\n  count(*)        as dep_delay_count,\n  max(arr_delay)  as arr_delay_max,\n  min(arr_delay)  as arr_delay_min,\n  sum(arr_delay)  as arr_delay_sum\nfrom flights\ngroup by \n  year,\n  month\n\nThis query retrieves aggregated information from the ‚Äúflights‚Äù dataset, calculating various statistics like maximum, minimum, mean, count, and sum for both ‚Äúdep_delay‚Äù and ‚Äúarr_delay‚Äù columns.\nTo achieve a similar result in pandas, we use the agg function, which allows us to specify these calculations concisely and efficiently.\nThe resulting DataFrame provides a clear summary of the specified metrics for each combination of ‚Äúyear‚Äù and ‚Äúmonth.‚Äù\n\nüêçpython\nresult = (\n    flights\n      .groupby(['year','month'],as_index=False)\n      .agg({'dep_delay':['max','min','mean','count'], 'arr_delay':['max','min','sum']})     \n)\n\n# Concatenate function names with column names\nresult.columns = result.columns.map('_'.join)\n\n# Print the results\nresult\nüü¢ Union Statement\nTo execute a UNION ALL operation in Pandas, it is necessary to create two DataFrames and concatenate them using the concat method.\nUnlike SQL, a DataFrame in Pandas can be combined to generate additional columns or additional rows.\nTherefore, it is essential to define how the concatenation should be performed:\naxis=1 => Union that appends another dataset to the right, generating more columns.\naxis=0 => Union that appends more rows.\n\nIn our example, we will perform the equivalent of a UNION ALL in SQL, so we will use axis=0.\nüîçsql\nselect  \n  origin,\n  dest,\n  time_hour,\n  dep_delay,\n  arr_delay,\n  flights.dep_delay + flights.arr_delay as delay_total ,\n  'NYC' group\nFROM flights  \n  WHERE origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and time_hour between '2013-09-01' and '2012-09-30'\nORDER BY flights.dep_delay + flights.arr_delay DESC\nLIMIT 3\nUNION ALL\nselect  \n  origin,\n  dest,\n  time_hour,\n  dep_delay,\n  arr_delay,\n  flights.dep_delay + flights.arr_delay as delay_total ,\n  'MIA' group\nFROM flights  \n  WHERE origin in ( 'JFK', 'LGA', 'EWR' ) \n   and time_hour between '2013-07-01' and '2012-09-30'\n  ORDER BY flights.dep_delay + flights.arr_delay DESC\n  LIMIT 2;\nüêçpython\nFlights_NYC = (\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight',\n              'tailnum', 'origin', 'dest', 'time_hour',\n              'dep_delay', 'arr_delay'])\n      .assign(delay_total = flights.dep_delay + flights.arr_delay )\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n       )\n     .assign(group ='NYC')      \n     .sort_values('delay_total',ascending=False)     \n     .head(3)\n)\n\nFlights_MIAMI = (\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest', 'time_hour', \n              'dep_delay', 'arr_delay'])\n      .assign(delay_total = flights.dep_delay + flights.arr_delay )\n      .query(\n             \" (dest in ['MIA', 'OPF', 'FLL'])\"\n             \" and ('2013-07-01' <= time_hour <= '2013-09-30')\"\n       )\n     .assign(group ='MIA') \n     .sort_values('delay_total',ascending=False)     \n     .head(2)\n)\n\n# union all \npd.concat([ Flights_NYC,Flights_MIAMI],axis=0)\nüü¢ CASE WHEN Statement\nTo replicate the CASE WHEN statement, we can use two different methods from NumPy:\n1Ô∏è‚É£.\nIf there are only two conditions, for example, checking if the total delay exceeds 0, then we label it as ‚ÄúDelayed‚Äù; otherwise, we label it as ‚ÄúOn Time‚Äù.\nFor this, the np.where method from NumPy is utilized.\nüîçsql\nselect  \n  origin,\n  dest,\n  time_hour,\n  dep_delay,\n  arr_delay,\n  (case \n    when flights.dep_delay + flights.arr_delay >0 then 'Delayed'\n    else 'On Time' end) as status ,\nFROM flights  \nLIMIT 5;\n\nüêçpython\n(\n    flights\n      .filter(['year', 'month', 'day', 'dep_time',\n              'flight', 'tailnum', 'origin', 'dest', \n              'time_hour', 'dep_delay', 'arr_delay'])\n      .assign(status=np.where((flights['dep_delay'] + flights['arr_delay']) > 0,                                'Delayed',\n                               'On Time'))\n      .head(5)\n)\n \n2Ô∏è‚É£.\nIn case there are more conditions, such as identifying Miami airports and labeling them as ‚ÄúMIA‚Äù, labeling ‚ÄúATL‚Äù airports that they are in Altanta, and for any other cases, using the label ‚ÄúOTHER‚Äù.\nFor this, the np.select method from NumPy is employed.\nCity\nName\nAcronym\nMiami\nMiami International\n(MIA)\nMiami\nOpa-locka Executive\n(OPF)\nMiami\nFort Lauderdale-Hollywood\n(FLL)\nAtlanta\nHartsfield-Jackson Atlanta\n(ATL)\nAtlanta\nDeKalb-Peachtree\n(PDK)\nAtlanta\nFulton County\n(FTY)\nüîçsql\nselect  \n  origin,\n  dest,\n  time_hour,\n  dep_delay,\n  arr_delay,\n  (case \n    when dest in ('ATL','PDK','FTY') then 'ATL'\n    when dest in ('MIA','OPF','FLL') then 'MIA'\n    else 'Other'\n  end) as city ,\nFROM flights  \nLIMIT 10;\nüêçpython\n(\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest', 'time_hour',\n              'dep_delay', 'arr_delay'])\n      .assign( city=np.select([flights['dest'].isin(['ATL','PDK','FTY']), \n                             flights['dest'].isin(['MIA', 'OPF', 'FLL'])],\n                             ['ATL','MIA'],\n                             default='Other')\n              )\n    .head(10)\n)\nüü¢ JOIN Statement\nEntity relationship diagram [DER]\n\nWhen performing a join in Pandas, the merge method should be used.\nüìó Join Types\nHow: Specifies the type of join to be performed.\nAvailable options: {'left', 'right', 'outer', 'inner', 'cross'}\njoinsüìó Join Key\nOn: The key on which the tables will be joined.\nIf more than one column is involved, a list should be provided.\nExamples:\nSingle variable: on='year'\nfligths.merge(planes, how='inner', on='tailnum')\nTwo variables: on=[‚Äòyear‚Äô,‚Äòmonth‚Äô,‚Äòday‚Äô]\nfligths.merge(weather, how='inner', on=['year','month','day'])\nleft_on/right_on: When the columns have different names, these parameters should be used. For example:\nfligths.merge(airports, how='inner', left_on = 'origin', rigth_on='faa')\nHere‚Äôs an example using the airlines and flights tables:\nüîçsql\nselect  \n  f.year,\n  f.month,\n  f.day,\n  f.dep_time,\n  f.flight,\n  f.tailnum,\n  f.origin as airport_origen,\n  f.dest,\n  f.time_hour,\n  f.dep_delay,\n  f.arr_delay,\n  f.carrier,\n  a.name as airline_name\nFROM flights  f\n  left join airlines a on f.carrier = a.carrier\nLIMIT 5;\nüü¢ Rename\n\nThe rename method is used to rename columns, similar to the ‚Äúas‚Äù clause in SQL.\n\nüêçpython\n(\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n        'tailnum', 'origin', 'dest', 'time_hour', 'dep_delay', \n        'arr_delay', 'carrier'])\n      .merge(airlines, how = 'left', on ='carrier')\n      .rename(columns= {'name':'airline_name', 'origin':'airport_origen'})\n      .head(5)\n)\n \n\nYou can find all the code in a üêç python notebook in the following [link]\n\nüìö References\nIf you want to learn more about `Pandas` and `NumPy`‚Ä¶\n- [Pandas]\n- [NumPy]\nOther references:\n- Image preview reference: [Imagen de vectorjuice en Freepik]\n\n\n\n",
    "preview": "posts_en/2023-12-17-sql-to-python-pandas-a-sql-users-quick-guide/preview.jpg",
    "last_modified": "2023-12-24T18:28:45+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-12-10-code-quality-sonarqube/",
    "title": "Code Quality - SonarQube",
    "description": "Code Quality, crucial for robust software, is upheld by tools like SonarQube.  This article explores its significance, implementation, and management.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2023-12-10",
    "categories": [
      "Python"
    ],
    "contents": "\n\nContents\nWhat is a code quality?\nWhat is Clean Code?\nWhat is a SonarQube?\nWhat is a SonarLindt?\nSonarQube vs SonarLindt\nSonarQube Features\nüìú Rules\nüìú Quality Profiles\nüìú Quality Gates\n\nüü£ Using SonarQube with Docker: A Step-by-Step Guide\nüê≥ Install Docker:\nüê≥ Pull the SonarQube Image\nüê≥ Run SonarQube Container:\nüê≥ Check if the container is running\nüöÄ Open the sonarqube web application\nüìÑ Create the script in python\nüìÑ Create a configuration file\nüìÑ Code folder\nüìÅ Create sonarqube project\nüîç Create a quality gate\nüîç Start the scanning\nüîç Scan Results\nüìä Sonarqube Metrics\nüöß SonarQube Issues\n\nüü£ Sonarqube API\nKey Features of the SonarQube API:\nCommon Use Cases:\n\nüü£ SonarQube API: A Step-by-Step Guide\nInstall SonarQubeClient\nConfig the sonarqube client\nGet sonarqube client projects\nGet the project events\n\nüìö References\n\n\nWhat is a code quality?\nCode quality measures the accuracy and reliability of code‚Äîbut being bug-free and portable is not the only measure of code quality.\nIt also includes how developer-friendly the code is.\nCode quality also describes how easy it is to understand, modify, and reuse the code if necessary.\nTestable: A piece of code should be easy to develop tests for and run tests on.\nPortable: You might want it to be easy to take the code from one environment the make it work again in another environment. If so, you can measure portability.\nReusable: High-quality code is modular and designed for reuse.\nsource: Amazon: What is code quality?\n\nImproving code quality involves addressing these factors to create code that is not only technically robust but also user-friendly and conducive to collaborative development.\n\nWhat is Clean Code?\n\n‚ÄúClean Code‚Äù is code that‚Äôs easy to read, maintain, understand and change through structure and consistency yet remains robust and secure to withstand performance demands.\"\nsource: clean code](https://www.sonarsource.com/solutions/clean-code/))\n\nüßπ Clean Code refers to the practice of writing code in a clear, readable and efficient manner, placing a strong emphasis on the understandability and maintainability of the code.\nThe main premise is that the code must not only work correctly, but it must also be easy to understand for any developer who reads it so that it can be reused quickly.\nTo achieve this, it is important to follow good programming practices and adopt conventions that promote clarity.\nAspects to consider:\nüìñ Readability: Code should be written in a way that is easily understandable.\nDescriptive names should be used for variables and functions, and confusing abbreviations should be avoided.\nüöÄ Simplicity: Simplicity is sought in the design and structure of the code.\nAvoiding unnecessary complexity and keeping functions and methods concise helps facilitate understanding.\nüõ†Ô∏è Maintainability: The code must be easy to maintain over time.\nThis involves minimizing code duplication, following sound design principles, and documenting effectively.\nüîÑ Consistency: Consistent coding conventions should be followed throughout the project to improve consistency and make the code easier to read.\nWhat is a SonarQube?\nSonarQube is a self-managed, automatic code review tool that systematically helps you deliver Clean Code.\nAs a core element of our Sonar solution, SonarQube integrates into your existing workflow and detects issues in your code to help you perform continuous code inspections of your projects.\nsource: SonarQube\nWhat is a SonarLindt?\nSonarLint is a free IDE extension that can be used in Visual Code, Visual studio or eclipse.\nThis plugin allows you to identify coding problems in real time, in order to avoid errors, vulnerabilities and code smells while you write your code.\nSonarLint can perform code analysis in JS/TS, Python, PHP, Java, C, C++, Go and IaC.\nSonarQube vs SonarLindt\nThe following is a comparative table in which we compare the functionality and the context in which each of these tools is applied\nSonarQube - SonarLindtimage source:sonarsource docs\nFeature\nSonarQube\nSonarLint\nScope\nServer-based code analysis for entire projects/repositories\nIDE-based code analysis for individual developers\nDeployment\nRequires a centralized server installation\nIntegrated directly into the developer‚Äôs IDE\nReal-time Feedback\nProvides feedback during continuous integration\nOffers real-time feedback within the developer‚Äôs IDE\nCode Analysis Depth\nOffers in-depth static code analysis and metrics\nProvides on-the-fly code analysis with immediate feedback\nIntegration with CI/CD\nIntegrates with CI/CD pipelines for automated analysis\nSupports local analysis as well as CI/CD integration\nRule Configurability\nHighly configurable rules for code quality and security\nLimited rule configuration options within the IDE\nCollaboration\nFacilitates collaboration among development teams\nFocuses on individual developer experience and collaboration\nUse Case\nSuitable for larger projects with centralized management\nIdeal for individual developers or smaller development teams\nSonarQube Features\nüìú Rules\nIn SonarQube, ‚Äúrules‚Äù are definitions that describe code patterns that indicate potential problems, security vulnerabilities, or areas for improvement in code quality.\nThe SonarQube analysis engine uses these rules to scan the source code and highlight potential problems.\nEach rule has a definition that allows a specific pattern to be identified and covers aspects such as: good practices, errors, vulnerability, security, among others.\n\nüìú Quality Profiles\nQuality profiles are a set of specific and organized rules that apply to specific projects.\nThese profiles allow you to customize the rules you want to use to evaluate code quality based on your specific needs and standards.\nTherefore, profiles allow you to customize which rules apply to a project and provide predefined profiles for different programming languages.\n\n\nüìú Quality Gates\nQuality Gates are sets of conditions that are applied to a project after running a static analysis of the code and applying the rules defined in the quality profiles.\nThese conditions allow you to quantify and evaluate whether a project meets specific quality criteria, helping to determine if the code is acceptable for implementation.\nMetric\nDescription\nReliability Rating\nThis indicator evaluates the reliability of the code, which means how prone the code is to contain errors or defects.\nSecurity Rating\nThis indicator evaluates the security level of the code, which means how prone the code is to contain security vulnerabilities.\nSecurity Hotspots Reviewed\nThis indicator evaluates whether all security points identified in the code have been reviewed.\nMaintainability Rating\nThis indicator evaluates the ease with which the code can be maintained and improved in the future\nCoverage\nThis indicator examines the % of code that has been executed.\nDuplicated Lines (%)\nThis flag checks for duplicate lines in the code\n\nüü£ Using SonarQube with Docker: A Step-by-Step Guide\nüê≥ Install Docker:\nEnsure that Docker is installed on your machine.\nYou can download and install Docker from the official website: Docker\nüê≥ Pull the SonarQube Image\nOpen a terminal and run the following command to pull the official SonarQube Docker image from Docker Hub:\ndocker pull sonarqube\nüê≥ Run SonarQube Container:\nExecute the following command in a terminal to run the sonarqube container.\ndocker run \n   -d --name sonarqube \n   -e SONAR_ES_BOOTSTRAP_CHECKS_DISABLE=true \n   -p 9000:9000 \n   sonarqube:latest\nCommand\nDescription\ndocker run\nThis is the command used to run a Docker container.\n-d\nThis is a Docker run option that stands for ‚Äúdetached.‚Äù It runs the container in the background, which means you get your terminal prompt back immediately after starting the container.\n--name sonarqube\nThis option allows you to specify a name for the container. In this case, the name ‚Äúsonarqube‚Äù is given to the container, which makes it easier to reference the container later.\n-e SONAR_ES_BOOTSTRAP_CHECKS_DISABLE=true\nThis option is used to set an environment variable within the container. It disables Elasticsearch bootstrap checks when starting SonarQube.\n-p 9000:9000\nThis option is used to map ports between the host and the container. It specifies that port 9000 on the host should be mapped to port 9000 inside the container, allowing access to SonarQube.\nsonarqube:latest\nThis is the Docker image to run. It specifies the image named ‚Äúsonarqube‚Äù and the ‚Äúlatest‚Äù tag, pulling the latest version from Docker Hub and creating a container from that image.\nüê≥ Check if the container is running\n$ docker ps\nCONTAINER ID   IMAGE              COMMAND                  CREATED          STATUS         PORTS                                       NAMES\nd8e576b6039e   sonarqube:latest   \"/opt/sonarqube/dock‚Ä¶\"   13 seconds ago   Up 8 seconds   0.0.0.0:9000->9000/tcp, :::9000->9000/tcp   sonarqube\nüöÄ Open the sonarqube web application\n‚Äòüîó Link: localhost:9000‚Äô\n\nüîê Initial user and password login: admin password: admin\n\n\nüìÑ Create the script in python\nThis Pythonüêç code was created using chat-gpt intentionally includes some practices that may violate default SonarQube configurations.\nIn addition, we will duplicate this file in the same folder to be able to generate an alert for duplicate code by generating the same file with the name main_bk\nmain.py\n# Code Smell: Unused variable\nunused_variable = 42\n\n# Code Smell: Unused function\ndef unused_function():\n    pass\n\n# Code Smell: Redundant parentheses\nresult = (5 * 3)\n\n# Code Smell: Unused import\nimport unused_module\n\n# Code Smell: Print statement (considered a bad practice)\nprint(\"Hello, World!\")\n\n# Code Smell: Hardcoded values\nmagic_number = 42\n\n# Code Smell: Unused loop variable\nfor _ in range(5):\n    pass\n\n# Code Smell: Assignment in a condition\nif (result == 0):  # Fix the equality check\n    pass\n\n# Code Smell: Using a single underscore as a variable name\n_ = \"Unused variable\"\n\n# Code Smell: Using a mutable default argument in a function\ndef append_item(item, my_list=None):\n    if my_list is None:\n        my_list = []\n    my_list.append(item)\n    return my_list\n\n# Code Smell: Unused variable in an exception block\ntry:\n    value = int(\"text\")\nexcept ValueError as e:\n    unused_exception_variable = e\n\n# Code Smell: Complex lambda function\nsquare = lambda x: x**2 + 2*x + 1\nüìÑ Create a configuration file\nThe following file contains the properties for execute the code quality processes in sonarqube.\nIt is necesary to change the projectkey and the project name, in my case the both name is ‚Äútest‚Äù, this parameters is config when you create the projects in SonarQube web application.\nFilename: ‚Äúsonar-project.properties‚Äù\n# must be unique in a given SonarQube instance\nsonar.projectKey=test\n\n# --- optional properties ---\n\n# defaults to project key\nsonar.projectName=test\n\n# defaults to 'not provided'\nsonar.projectVersion=1.0\n \n# Path is relative to the sonar-project.properties file. Defaults to .\nsonar.sources=.\nsonar.language=python\n\n#----- Default SonarQube server\nsonar.host.url=http://localhost:9000 \n\n# Encoding of the source code. Default is default system encoding\nsonar.sourceEncoding=UTF-8\nüìÑ Code folder\nIn summary, the folder that should contain the following files\nüìÑ main.py\nüìÑ main_bk.py\nüìÑ sonar-project.properties\nüìÅ Create sonarqube project\nThe images below illustrate all the steps necessary to create the project and obtain the token for the scanner later.\n\n\n\n\n\nüîç Create a quality gate\nClick on \"Create\" and define the name of the quality gate.\n\n\nClick on \"Unlock Editing\" to update the condition metrics.\n\nSet the current quality gate as the default\n\nüîç Start the scanning\nTo start the scanning process using the SonarQube CLI, execute the following command after replacing placeholders with your specific information.\nEnsure that this command is run in the terminal where the source path, containing the ‚Äòsonar-project.properties‚Äô file, is located.\nIt‚Äôs crucial to set your token as an environment variable using the following syntax: -e SONAR_LOGIN=‚Äúyour_token_here‚Äù.\nFor example, if your token is ‚Äúsqp_08ad32fcb385e8192b1a4e0aabdc54be3b1ad946‚Äù the corresponding command to be executed would be:\ndocker run --network=host \n -e SONAR_HOST_URL=http://host.docker.internal:9000 \n -e SONAR_LOGIN=\"sqp_08ad32fcb385e8192b1a4e0aabdc54be3b1ad946\" \n -e SONAR_PROJECT_KEY=data-quality \n -it -v \"$(pwd):/usr/src\" \n sonarsource/sonar-scanner-cli\n\nIn the terminal you will see the following log\n\nDigest: sha256:494ecc3b5b1ee1625bd377b3905c4284e4f0cc155cff397805a244dee1c7d575\nStatus: Downloaded newer image for sonarsource/sonar-scanner-cli:latest\nINFO: Scanner configuration file: /opt/sonar-scanner/conf/sonar-scanner.properties\nINFO: Project root configuration file: /usr/src/sonar-project.properties\nINFO: SonarScanner 5.0.1.3006\nINFO: Java 17.0.8 Alpine (64-bit)\nINFO: Linux 5.10.25-linuxkit amd64\nINFO: User cache: /opt/sonar-scanner/.sonar/cache\nINFO: Analyzing on SonarQube server 10.3.0.82913\nINFO: Default locale: \"en_US\", source code encoding: \"UTF-8\"\nINFO: Load global settings\nINFO: Load global settings (done) | time=624ms\nINFO: Server id: 147B411E-AYxB7bsnEO8aoeQvN3oK\nINFO: User cache: /opt/sonar-scanner/.sonar/cache\nINFO: Load/download plugins\nINFO: Load plugins index\nINFO: Load plugins index (done) | time=453ms\nINFO: Load/download plugins (done) | time=5843ms\nINFO: Process project properties\nINFO: Process project properties (done) | time=42ms\nINFO: Execute project builders\nINFO: Execute project builders (done) | time=10ms\nINFO: Project key: data-quality\nINFO: Base dir: /usr/src\nINFO: Working dir: /usr/src/.scannerwork\nINFO: Load project settings for component key: 'data-quality'\nWARN: SCM provider autodetection failed. Please use \"sonar.scm.provider\" to define SCM of your project, or disable the SCM Sensor in the project settings.\nINFO: Load quality profiles\nINFO: Load quality profiles (done) | time=3356ms\nINFO: Load active rules\nwhen the process finish you can see the following log\nINFO: ------------- Run sensors on project\nINFO: Sensor Analysis Warnings import [csharp]\nINFO: Sensor Analysis Warnings import [csharp] (done) | time=3ms\nINFO: Sensor Zero Coverage Sensor\nINFO: Sensor Zero Coverage Sensor (done) | time=14ms\nINFO: SCM Publisher No SCM system was detected. You can use the 'sonar.scm.provider' property to explicitly specify it.\nINFO: CPD Executor Calculating CPD for 0 files\nINFO: CPD Executor CPD calculation finished (done) | time=0ms\nINFO: Analysis report generated in 273ms, dir size=137.8 kB\nINFO: Analysis report compressed in 306ms, zip size=17.4 kB\nINFO: Analysis report uploaded in 412ms\nINFO: ANALYSIS SUCCESSFUL, you can find the results at: http://localhost:9000/dashboard?id=test\nINFO: Note that you will be able to access the updated dashboard once the server has processed the submitted analysis report\nINFO: More about the report processing at http://localhost:9000/api/ce/task?id=AYxTnq5AFtsPP8-M5c1w\nINFO: Analysis total time: 18.832 s\nINFO: ------------------------------------------------------------------------\nINFO: EXECUTION SUCCESS\nINFO: ------------------------------------------------------------------------\nINFO: Total time: 27.819s\nINFO: Final Memory: 21M/80M\nINFO: --------------------------------------------------------\nüîç Scan Results\nUpon completion of the code scan, you can view the results of the code analysis on the web application at localhost:9000\nüìä Sonarqube Metrics\nThe following table presents the metrics defined by SonarQube, which are objective indicators designed to evaluate the quality of the source code.\nThese metrics allow a quantitative evaluation of various critical aspects of the code.\nMetric\nDescription\nReliability\nThe ‚ÄúReliability‚Äù metric refers to the reliability of the code. It measures the number of issues related to software reliability, such as errors and failures.\nMaintainability\nThe ‚ÄúMaintainability‚Äù metric in SonarQube assesses how easy it is to maintain and enhance the code over time. It measures code quality in terms of structure, readability, and ease of maintenance.\nNew Code Smells\nThis metric indicates the number of new ‚Äúcode smells‚Äù introduced in the code recently. ‚ÄúCode smells‚Äù are design patterns or coding practices that may indicate issues with code quality.\nSecurity\nThe ‚ÄúSecurity‚Äù metric in SonarQube evaluates code security for potential vulnerabilities. It measures the code‚Äôs ability to resist attacks and protect data and systems.\nNew Vulnerabilities\nIt indicates the number of new security vulnerabilities introduced in the code recently. Vulnerabilities are weaknesses that can be exploited by attackers.\nSecurity Review\nThis metric in SonarQube assesses the quality of security reviews conducted on the code. It measures the effectiveness of reviews in detecting and correcting security issues.\nNew Security Hotspots\nIt signifies the number of new ‚Äúsecurity hotspots‚Äù introduced in the code recently. Security hotspots are areas of the code that require special attention due to potential security issues.\nCoverage\nThe ‚ÄúCoverage‚Äù metric in SonarQube refers to code coverage. It measures the proportion of code that has been tested through unit tests or automated tests.\nDuplications\nThis metric in SonarQube identifies sections of code that are duplicated in multiple places. Identifying and reducing duplications can improve code quality and maintainability.\nThe result of the scan is the following\n\nüöß SonarQube Issues\n‚ÄúSonarQube Issues‚Äù refer to issues identified by the SonarQube static code analysis process.\nEach issue provides:\nüìç Location of the Issue: Enables identification of where the problem exists in the code.\nü§î Reason for the Issue: Offers a detailed explanation of why it is considered a problem.\nüí¨ Activity: Facilitates collaboration by allowing the addition of comments and discussions about potential solutions.\nüë§ Assignment: Permits the assignment of the issue to a registered user in SonarQube for tracking and resolution.\nüìä Status: Initiates with the ‚ÄúOpen‚Äù status upon creation but may transition to other statuses such as:\n‚ÄúResolved as Fixed‚Äù: Indicates that the problem identified in the issue has been fixed in the source code.\n‚ÄúResolved as False Positive‚Äù: Initiates that the problem initially identified as an ‚Äúissue‚Äù is not actually a problem or does not require correction.\n‚ÄúResolved as Won‚Äôt Fix‚Äù: Indicates that a decision has been made not to address or correct the problem noted in the issue.\n\nüè∑Ô∏è Tags: Allows the addition of tags to enhance the identification and categorization of issues.\n\n\nüü£ Sonarqube API\nThe SonarQube API empowers users to interact with and extract information programmatically from a SonarQube instance.\nThese APIs serve as a tool for developers, administrators, and integrators, enabling the automation of tasks, retrieval of project metrics, and seamless integration of SonarQube data into various workflows.\nKey Features of the SonarQube API:\nü§ñ Automation: The SonarQube API allows for the automation of various tasks related to project analysis, configuration, and management.\nThis includes triggering analyses, retrieving analysis results, and managing quality profiles.\nüìä Data Retrieval: Users can extract a wide range of data from SonarQube, including project metrics, issues, code smells, duplications, and more.\nThis information can be used for reporting, analytics, and custom dashboards.\nüîó Integration: The API facilitates integration with other development tools, continuous integration (CI) systems, and external applications.\nThis enables seamless incorporation of SonarQube‚Äôs code quality and security analysis into existing development pipelines.\n‚öôÔ∏è Configuration Management: The API allows users to manage SonarQube configurations programmatically.\nThis includes creating and updating quality profiles, setting project configurations, and managing global settings.\nCommon Use Cases:\nü§ñ Automated Analysis: Integrate SonarQube analysis into your CI/CD pipeline by triggering analyses automatically after code commits or builds.\nüìä Custom Reporting: Extract specific metrics and data from SonarQube to generate custom reports tailored to your team or organization‚Äôs needs.\nüêõ Issue Tracking: Retrieve information about code issues, security vulnerabilities, and code smells to integrate SonarQube data into your issue tracking or project management system.\nüö¶ Quality Gate Status: Monitor and retrieve the status of Quality Gates for projects to ensure that code meets predefined quality criteria.\nüõ†Ô∏è Configuration as Code: Manage SonarQube configurations using scripts or code, making it easier to replicate configurations across different instances.\nüü£ SonarQube API: A Step-by-Step Guide\nInstall SonarQubeClient\npip install python-sonarqube-api\nConfig the sonarqube client\nfrom sonarqube import SonarQubeClient\nimport pandas as pd\n\nsonar = SonarQubeClient(sonarqube_url=\"http://localhost:9000\", username='admin', password='admin123')\nGet sonarqube client projects\nprojects = sonar.projects.search_projects()\nprojects\n{'paging': {'pageIndex': 1, 'pageSize': 100, 'total': 1},\n 'components': [\n  {'key': 'test',\n   'name': 'test',\n   'qualifier': 'TRK',\n   'visibility': 'public',\n   'lastAnalysisDate': '2023-12-10T13:47:32+0000',\n   'managed': False}]}\nGet the project events\nproject_analyses_and_events = sonar.project_analyses.search_project_analyses_and_events(project=\"test\")\nproject_analyses_and_events = pd.json_normalize(project_analyses_and_events['analyses'])\n\n[item for item in project_analyses_and_events['events'] if item!=[]]```\n\n```bash\n[[{'key': 'AYxT_AnVJaFOvYeUMXMa', 'category': 'VERSION', 'name': '1.0'}],\n [{'key': 'AYxT-xvhJaFOvYeUMXIe',\n   'category': 'QUALITY_GATE',\n   'name': 'Failed',\n   'description': 'Coverage on New Code < 80, New Code Smells > 0'}],\n [{'key': 'AYxT-cgDJaFOvYeUMXFb',\n   'category': 'QUALITY_GATE',\n   'name': 'Passed',\n   'description': ''}],\n [{'key': 'AYxT7MvDJaFOvYeUMW5W',\n   'category': 'QUALITY_GATE',\n   'name': 'Failed',\n   'description': 'Coverage on New Code < 80'}]]\nüìö References\nIf you want to learn‚Ä¶\nSonarqube: Documentation\nSonarLint: IDE\nSonarLint: Documentation\nSonarqube: API\nSonarqube: Python\nOther references:\nImage preview reference: [Imagen de storyset en Freepik]\n\n\n\n",
    "preview": "posts_en/2023-12-10-code-quality-sonarqube/preview.jpg",
    "last_modified": "2023-12-24T18:32:46+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-12-02-rabbitmq-pika/",
    "title": "RabbitMQ-Pika",
    "description": "RabbitMQ allows you to manage message queues between senders and recipients. In the next post we are going to use **Pika** in python for its implementation.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2023-12-02",
    "categories": [
      "Python"
    ],
    "contents": "\n\nContents\nIntroduction: What is RabbitMQ?\nImplementation with Pika in Python üêç\n1Ô∏è‚É£ . Install pika\n2Ô∏è‚É£ . Create send.py üìÑ file\n3Ô∏è‚É£. Create send.py üìÑ file\n4Ô∏è‚É£. MongoDB + Pika\n\n\n\n\nRabbitMQ enables the management of message queues between senders and receivers.\nIn the following post, we will employ Python‚Äôs Pika library for its implementation.\n\nIntroduction: What is RabbitMQ?\nRabbitMQ is an intermediary system designed to facilitate the transfer of messages between producers and consumers through the implementation of queues.\nThis component, essential in distributed systems architecture, is grounded in key concepts:\n1Ô∏è‚É£.\nProducer: The entity responsible for originating and dispatching messages.\n2Ô∏è‚É£.\nQueue: A reservoir where messages are temporarily stored.\n3Ô∏è‚É£.\nConsumer: The receiving instance that processes messages according to the system‚Äôs needs.\nThis introduction aims to provide a clear and concise overview of the fundamental elements of RabbitMQ, paving the way for a deeper understanding of its functioning in messaging environments.\n\nImplementation with Pika in Python üêç\nImplementing in Python with the Pika library involves creating two essential programs: the producer and the consumer.\nPika provides an effective interface for communication with RabbitMQ, leveraging a set of carefully designed objects for this purpose.\nIn our practical example, envision the producer as an application designed to manage food delivery orders üõµ.\nThis application, geared towards optimizing the delivery process, is responsible for sending multiple messagesüìù related to user üì± food orders.\nTo achieve this implementation, we will undertake the following steps:\nSteps\nDescriptions\nProducer:\nDevelop a program that, like an efficient order-taker, generates and sends messagesüìù to the RabbitMQ queue. These messages will contain valuable information about food orders.\nConsumer:\nCreate a program that acts as the receiver of these messages in the queue. The consumer will be responsible for processing these messages according to the system‚Äôs needs, performing relevant actions, such as managing the delivery of orders.\nThis structured and efficient approach ensures a clear and functional implementation, providing a robust foundation for systems managing information flows in dynamic environments.\n1Ô∏è‚É£ . Install pika\n!pip install pika\n2Ô∏è‚É£ . Create send.py üìÑ file\nimport pika\nfrom datetime import datetime\n\nconnection = pika.BlockingConnection(\n    pika.ConnectionParameters(host='localhost'))\nchannel = connection.channel()\n\nchannel.queue_declare(queue='delivery')\n\npedidos=['üçïüçïüçï','üçîüçîüçî','üç∞üç∞üç∞','üç∫üç∫üç∫']\n\nfor i in pedidos:\n    channel.basic_publish(exchange='', routing_key='delivery', body=i)\n    print(\" [x] Se envia pedido!'\"+ i)\n\nconnection.close()\n3Ô∏è‚É£. Create send.py üìÑ file\nimport pika, sys, os\nfrom datetime import datetime\n\n\ndef main(queue='delivery'):\n    connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))\n    channel = connection.channel()\n    channel.queue_declare(queue=queue)\n\n    def callback(ch, method, properties, body):\n        print(\" [x] Received %r\" % body.decode())\n\n    channel.basic_consume(queue='delivery', on_message_callback=callback, auto_ack=True)\n\n    print(' [*] Waiting for messages. To exit press CTRL+C')\n    channel.start_consuming()\n\nif __name__ == '__main__':\n    try:\n        main(queue=queue)\n    except KeyboardInterrupt:\n        print('Interrupted')\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)\nImage description4Ô∏è‚É£. MongoDB + Pika\nIn the following, we will modify the script to enable it to connect to a MongoDB Atlas and perform the insertion of received messages.\nimport pymongo\nimport pika, sys, os\nfrom datetime import datetime\n\n# Crear una conexion con MongoClient\nclient = pymongo.MongoClient(\"mongodb+srv://NombreUser:PasswordUser@clusterName.moczg.mongodb.net/rabbit?retryWrites=true&w=majority\")\n\n# Database\ndb = client[\"rabbit\"]\n\n# Collection\ncollection= db[\"mensajes\"]\n\ndef main(queue='delivery'):\n    connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))\n    channel = connection.channel()\n    channel.queue_declare(queue=queue)\n\n    def callback(ch, method, properties, body):\n        print(\" [x] Received %r\" % body.decode())\n        body_indsert={'fecha':datetime.now(),'queue':queue,'message':body.decode()}\n        db[\"mensajes\"].insert_one(body_indsert)\n\n    channel.basic_consume(queue='hello', on_message_callback=callback, auto_ack=True)\n\n    print(' [*] Waiting for messages. To exit press CTRL+C')\n    channel.start_consuming()\n\nif __name__ == '__main__':\n    try:\n        main(queue=queue)\n    except KeyboardInterrupt:\n        print('Interrupted')\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)\nTo download the code for these two files, you can do so from the following link.\nTo learn more about RabbitMQ, you can visit the following sites:\nüìÑ Oficial Documentation\nüìÑ Rabbit Tutorial\nüêç Pika\nImage preview reference: Imagen de rawpixel.com en Freepik\n\n\n\n",
    "preview": "posts_en/2023-12-02-rabbitmq-pika/preview.jpg",
    "last_modified": "2023-12-24T18:37:01+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-11-18-aws-copilot/",
    "title": "AWS Copilot",
    "description": "In the following article, I explain what AWS Copilot is, how to use this project, and the ease of implementing it.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://r0mymendez.github.io/romymendezblog/"
      }
    ],
    "date": "2023-11-14",
    "categories": [
      "Cloud Computing"
    ],
    "contents": "\n\nContents\nIntroduction\nüí°AWS ECS (Elastic Container Services)\n‚öíÔ∏è ECS with EC2 instances\n‚öíÔ∏è ECS with AWS Fargate (Serverless)\n\nüí°AWS Copilot\nüîé Comparison of Task Responsibilities\nüß© Components\nüöÄ Deployment with AWS Copilot: A 5-Step Guide\nüîé Logs\nüìâ Traffic to production\nüß™Testing\nüí∞ Service Cost\n\nüéØ Key Takeaways\nüìö References\n\n\nIntroduction\nIn this article, I explain the use of the AWS Copilot service.\nHowever, to carry it out, it‚Äôs necessary to start by analyzing what the AWS Elastic Container Service (ECS) is and how it works, along with its deployment methods.\nThis is necessary because AWS Copilot performs the implementation and deployment of an application using ECS\nüí°AWS ECS (Elastic Container Services)\nElastic Container Service ( ECS ) is a scalable container orchestration platform proprietary to AWS.\nIt is designed to run, stop, and manage containers in a cluster.\nTherefore, ECS is AWS‚Äôs Docker container service that handles the orchestration and provisioning of Docker containers.\nThis service includes the following concepts:\n‚òÅÔ∏è Task Definition: Describes how to start a Docker container.\n‚òÅÔ∏è Task: This is a running container with the configuration defined in the task definition.\n‚òÅÔ∏è Service: Defines long-running tasks from the same task definition.\n‚òÅÔ∏è Cluster: A logical group of EC2 instances.\n‚òÅÔ∏è Container Instance: This is just an EC2 instance that is part of an ECS cluster and has Docker installed.\n‚öíÔ∏è ECS with EC2 instances\nIn this model, containers are deployed on EC2 instances (VMs) created for the cluster.\nECS manages them along with the tasks that are part of the task definition\n‚úÖ Advantages\n‚ùå Disadvantages\n- Complete control over the type of EC2 instance used is provided.\n- When working with EC2, it‚Äôs necessary for the administrator of this architecture to handle all security updates and scaling of instances.\n- It allows the use of instances that can be optimized depending on what you want to execute.\n- The cost is based on the type of EC2 instance running within the cluster and the VPC networks.\n‚öíÔ∏è ECS with AWS Fargate (Serverless)\nIn this serverless configuration, the reliance on EC2 instances is eliminated, simplifying the deployment process.\nInstead, you only need to specify the required CPU and memory combination.\nAWS Fargate allows for a fully managed and serverless container deployment experience.\n‚úÖ Advantages\n‚ùå Disadvantages\n- There are no servers to manage.\n- ECS + Fargate supports only one network mode, and this limits control over the network layer.\n- AWS is in charge of the availability and scalability of the containers.\n- Cost is based on the CPU and memory you select. The number of CPU cores and GB determines the cost of running the cluster.\n- Fargate Spot is a new capability that can run ECS tasks that are interruption-tolerant at up to a 70% discount compared to the Fargate price.\n\nüí°AWS Copilot\nAWS Copilot is a tool used through the AWS command line that simplifies the creation, deployment, monitoring, and operation of containers in ECS using a local development environment\nThis tool manages the components required for the deployment and operation of an application, such as VPC, load balancers, deployment pipelines, and storage.\nTherefore, it‚Äôs only necessary to provide an application container and minimal configurations, resulting in a faster deployment and focusing on application development.\nüîé Comparison of Task Responsibilities\nThe services will communicate with each other, so it is necessary to consider the following scenarios:\nActivities\nWithout AWS-copilot\nWith AWS-copilot\nApplication developmen\nüìó Development team\nüìó Development team\nContainer generation\nüìó Development team\nüìó Development team\nVirtual Private Cloud (VPC) Subnets\nüìó Development team\nüìô AWS-Copilot\nLoad balancers\nüìóDevelopment team\nüìô AWS-Copilot\nDeployment flows (ci/cd)\nüìó Development team\nüìô AWS-Copilot\nPersistent storage of your application\nüìó Development team\nüìô AWS-Copilot\nSynchronize deployment across environments\nüìó Development team\nüìô AWS-Copilot\nüß© Components\nThe following table contains the components that are configured when using the AWS Copilot service.\nComponent\nDescription\nApplication\nAn application is a grouping mechanism for the pieces of your system.\nEnviroment\nAn environment is a stage in the deployment of an application.\nService\nA service is a single process of long-running code within a container.\nüöÄ Deployment with AWS Copilot: A 5-Step Guide\nIn just 5 steps we can deploy an application using aws-copilot, as shown in the following image.\nThis allows the development team to only focus on development and not so much on the deployment of the infrastructure.\nThe first application that is deployed in copilot will make a default configuration and the same will be with a serverless container in fargate.\nAs seen in the following image, with only 5 steps we can deploy an application.\nImage descriptionThe steps in the flow are as follows:\nInstall AWS Copilot, which will require AWS client credentials.\nCreate the Dockerfile for our application.\nExecute copilot init in a terminal to initialize.\nWhen running init, some questions will appear to answer, such as the application name, service type, service name, and Dockerfile location.\nIn this final step, a URL will be provided to access the application\nüîé Logs\nTo obtain the logs of the deployed containers, it is necessary to execute the following command:\n$ copilot svc logs- follow\nüìâ Traffic to production\nTo deploy in production it is necessary to be able to generate different environments, so to generate them it is necessary to execute the following command.\n$ copilot env init\nSubsequently, it is important to be able to modify the manifest file that contains all the application configurations and is located in ¬†nombredeaplicacion/manifest.yml\nOnce the environment configuration is complete, it is necessary to deploy it to production (or another environment, but the following example is in production).\n$ copilot svc deploy ‚Äîenv production\nüß™Testing\nIn order to test the deployed application, you can use ApacheBench which allows you to generate traffic to the web application.\nFor this it is necessary to be able to execute the following command in which you want to generate a number of 5000 transactions to my service with a concurrency of 25 requests at a time.\nab -n 5000 -c 25<http://app12345.us-east-1.elb.amazonaws.com>\nIf I do not have the expected response, I can modify my manifest file and horizontally scale the application based on the different environments.\nüí∞ Service Cost\nAWS Copilot is distributed by Amazon under an Apache 2.0 license, making it an open-source application.\nAs an open-source tool, AWS Copilot incurs no additional costs.\nThe pricing is solely determined by the usage of the configured services.\nThis cost-efficient model allows users to leverage the full capabilities of AWS Copilot without incurring any licensing fees.\nüéØ Key Takeaways\nIn conclusion, AWS Copilot stands out for the following features:\nAWS Copilot emerges as a robust, open-source AWS tool that streamlines the deployment of production-ready containers in a mere 5 steps, allowing development teams to concentrate on coding rather than grappling with infrastructure intricacies.\nConfiguration is effortless, demanding only the execution of a few commands and adjustments to the manifest file based on the application‚Äôs resource requirements.\nAddressing horizontal scaling needs is a breeze ‚Äì a simple modification to the manifest file followed by a deployment is all it takes.\nAWS Copilot facilitates the establishment of a CI/CD pipeline for seamless, automatic deployments across various environments.\nEffortlessly generate KPIs, set up alarms, and collect metrics with just a few commands through the user-friendly AWS Copilot service.\nüìö References\nüìöTitle: Presentamos AWS Copilot, Site: Blog de Amazon Web Services (AWS), Author: Nathan Peck,Gabriel Gasca Torres y Jos√© Lorenzo Cu√©ncar, url: <https://aws.amazon.com/es/blogs/aws-spanish/presentamos-aws-copilot/>,\nüìöTitle: Introducci√≥n a Amazon ECS mediante AWS Copilot, Site: Documentaci√≥n oficical de AWS, Author: AWS, url: <https://docs.aws.amazon.com/es_es/AmazonECS/latest/userguide/getting-started-aws-copilot-cli.html>\nüìöTitle: AWS Copilot, Site: AWS, Author: AWS, url: <https://aws.amazon.com/es/containers/copilot/>\nüìöTitle: Gentle Introduction to How AWS ECS Works with Example Tutorial, Site: Medium, Author: Tung Nguyen , Url: <https://medium.com/boltops/gentle-introduction-to-how-aws-ecs-works-with-example-tutorial-cea3d27ce63d>\nImage preview reference: Image by vectorjuice on Freepik\n\n\n\n",
    "preview": "posts_en/2023-11-18-aws-copilot/preview.jpg",
    "last_modified": "2023-12-24T18:38:07+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-11-17-data-quality/",
    "title": "Data Quality",
    "description": "In the following article you will find the definition of data quality, what the domains are and how to quickly implement a solution.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://r0mymendez.github.io/romymendezblog/"
      }
    ],
    "date": "2023-11-12",
    "categories": [
      "Data",
      "Python"
    ],
    "contents": "\n\nContents\nWhat is Data quality?\nData quality dimensions - Use case\nPython Frameworks\nDifferences between Licencia Apache 2.0 y MIT\nDataset\nüü¢ Pandera\nInstall pandera\nImplementation Example\n\nüü† Great Expectations\nInstall great expectation\nImplementation example\n\nIf you want to learn‚Ä¶\n\n\n\nIn the current digital environment, the amount of available data is overwhelming.\nHowever, the true cornerstone for making informed decisions lies in the quality of this data.\nIn this article, we will explore the crucial importance of data quality, analyzing the inherent challenges that organizations face in managing information.\nAlthough often overlooked, data quality plays a fundamental role in the reliability and usefulness of the information that underpins our strategic decisions.\nWhat is Data quality?\nData quality measures how well a dataset complies with the criteria of accuracy, completeness, validity, consistency, uniqueness, timeliness, and fitness for purpose, and is fundamental for all data governance initiatives within an organization.\nData quality standards ensure that companies make decisions based on data to achieve their business objectives.\nsource: IBM\n\nsource: DataCamp cheat sheet\nThe following table highlights the various domains of data quality, from accuracy to fitness, providing an essential guide for assessing and enhancing the robustness of datasets:\nDimensions\nDescription\nüéØ Accuracy\nData accuracy, or how close data is to reality or truth. Accurate data is that which faithfully reflects the information it seeks to represent.\nüß© Completeness\nMeasures the entirety of the data. A complete dataset is one that has no missing values or significant gaps. Data integrity is crucial for gaining a comprehensive and accurate understanding.\n‚úÖ Validity\nIndicates whether the data conforms to defined rules and standards. Valid data complies with the established constraints and criteria for a specific dataset..\nüîÑ Consistency\nRefers to the uniformity of data over time and across different datasets. Consistent data does not exhibit contradictions or discrepancies when compared with each other\nüìá Uniqueness\nEvaluates whether there are no duplicates in the data. Unique data ensures that each entity or element is represented only once in a dataset\n‚åõTimeliness\nRefers to the timeliness of data. Timely information is that which is available when needed, without unnecessary delays.\nüèãÔ∏è Fitness\nThis aspect evaluates the relevance and usefulness of data for the intended purpose. Data should be suitable and applicable to the specific objectives of the organization or analysis being conducted.\nData quality dimensions - Use case\nNext, we provide an example where some issues with an e-commerce-based use case can be observed.\nID Transacci√≥n\nID Cliente\nProducto\nCantidad\nPrecio Unitario\nTotal\n‚ö™ 1\n10234\nLaptop HP\n1\n$800\n$800\nüü£ 2\n\nWireless Headphones\n2\n$50\n$100\nüîµ 3\n10235\nSmartphone\n-1\n$1000\n-$1000\nüü¢ 4\n10236\nWireless Mouse\n3\n$30\n$90\nüü¢ 4\n10237\nWireless Keyboard\n2\n$40\n$80\nüü£ Row 2 (Completeness): Row 2 does not comply with data integrity (Completeness) as the customer ID is missing.\nCustomer information is incomplete, making it challenging to trace the transaction back to a specific customer.\nüîµ Row 3 (Accuracy and Consistency): Row 3 exhibits accuracy (Accuracy) and consistency (Consistency) issues.\nThe quantity of products is negative, which is inaccurate and goes against the expected consistency in a transaction dataset.\nüü¢ Row 4 (Uniqueness): The introduction of a second row with the same transaction ID (Transaction ID = 4) violates the uniqueness principle.\nEach transaction should have a unique identifier, and having two rows with the same Transaction ID creates duplicates, affecting the uniqueness of transactions.\n\nPython Frameworks\nThe following are some of the Python implementations carried out to perform data quality validations:\nFramework\nDescripci√≥n\nGreat Expectations\nGreat Expectations is an open-source library for data validation. It enables the definition, documentation, and validation of expectations about data, ensuring quality and consistency in data science and analysis projects\nPandera\nPandera is a data validation library for data structures in Python, specifically designed to work with pandas DataFrames. It allows you to define schemas and validation rules to ensure data conformity\nDora\nDora is a Python library designed to automate data exploration and perform exploratory data analysis.\nLet‚Äôs analyze some of the metrics that can be observed in their GitHub repositories, taking into account that the metrics were obtained on 2023-11-12.\nMetricas\nGreat Expectations\nPandera\nDora\nüë• Members\n399\n109\n106\n‚ö†Ô∏è Issues: Open\n112\n273\n1\nüü¢ Issues: Close\n1642\n419\n7\n‚≠ê Stars\n9000\n2700\n623\nüì∫ Watching\n78\n17\n42\nüîé Forks\n1400\n226\n63\nüì¨ Open PR\n43\n19\n0\nüêç Version Python\n>=3.8\n>=3.7\nNo especificada\nüìÑ Version Number\n233\n76\n3\nüìÑ Last Version\n0.18.2\n0.17.2\n0.0.3\nüìÜ Last Date Version\n9 Nov 2023\n30 sep 2023\n30 jun 2020\nüìÑ Licence type\nApache-2.0 license\nMIT\nMIT\nüìÑ Languages\nPython 95.1%\nJupyter Notebook 4.3%\nJinja 0.4%\nJavaScript 0.1%\nCSS 0.1%\nHTML 0.0%\n\nPython 99.9%\nMakefile 0.1%\n\nPython100%\n\nDifferences between Licencia Apache 2.0 y MIT\nNotification of Changes:\nApache 2.0: Requires notification of changes made to the source code when distributing the software.\nMIT: Does not require specific notification of changes.\n\nCompatibility:\nApache 2.0: Known to be compatible with more licenses compared to MIT.\nMIT: Also quite compatible with various licenses, but Apache 2.0 License is often chosen in projects seeking greater interoperability with other licenses.\n\nAttribution:\nApache 2.0: Requires attribution and the inclusion of a copyright notice.\nMIT: Requires attribution to the original authorship but may have less strict requirements in terms of how that attribution is displayed.\n\nConsidering these currently analyzed metrics, let‚Äôs proceed with an example implementation using Pandera and Great Expectations.\n\nDataset\nFor the development of this example, we will use the dataset named ‚ÄòTips.‚Äô You can download the dataset from the followinge link.\nThe ‚Äòtips‚Äô dataset contains information about tips given in a restaurant, along with details about the total bill, the gender of the person who paid the bill, whether the customer is a smoker, the day of the week, and the meal‚Äôs time.\nColumn\nDescription\ntotal_bill\nThe total amount of the bill (including the tip).\ntip\nThe amount of tip given.\nsex\nThe gender of the bill payer (male or female).\nsmoker\nWhether the customer is a smoker or not.\nday\nThe day of the week when the meal was made.\ntime\nThe time of day (lunch or dinner).\nsize\nThe size of the group that shared the meal.\nBelow is a table with the first 5 rows of the dataset:\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\nüü¢ Pandera\nNext, we will provide an example of implementing Pandera using the dataset described earlier.\nInstall pandera\npip install pandas pandera \nImplementation Example\nImport pandas and pandera\nimport pandas as pd\nimport pandera as pa\nImport the dataframe file\npath = 'data/tips.csv'\ndata = pd.read_csv(path)\n\nprint(f\"Numero de columnas: {data.shape[1]}, Numero de filas: {data.shape[0]}\")\nprint(f\"Nombre de columnas: {list(data.columns)}\")\ndata.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   total_bill  244 non-null    float64\n 1   tip         244 non-null    float64\n 2   sex         244 non-null    object \n 3   smoker      244 non-null    object \n 4   day         244 non-null    object \n 5   time        244 non-null    object \n 6   size        244 non-null    int64  \ndtypes: float64(2), int64(1), object(4)\nmemory usage: 13.5+ KB\nNow, let‚Äôs create the schema object that contains all the validations we want to perform.\nYou can find additional validations that can be performed at the following link: <https://pandera.readthedocs.io/en/stable/dtype_validation.html>\nschema = pa.DataFrameSchema({\n  \"total_bill\": pa.Column(float, checks=pa.Check.le(50)),\n  \"tip\"       : pa.Column(float, checks=pa.Check.between(0,30)),\n  \"sex\"       : pa.Column(str, checks=[pa.Check.isin(['Female','Male'])]),\n  \"smoker\"    : pa.Column(str, checks=[pa.Check.isin(['No','Yes'])]),\n  \"day\"       : pa.Column(str, checks=[pa.Check.isin(['Sun','Sat'])]),\n  \"time\"      : pa.Column(str, checks=[pa.Check.isin(['Dinner','Lunch'])]),\n  \"size\"      : pa.Column(int, checks=[pa.Check.between(1,4)])\n})\nTo capture the error and subsequently analyze the output, it is necessary to catch it using an exception.\ntry:\n    schema(data).validate()\nexcept Exception as e:\n    print(e)\n    error = e\nSchema None: A total of 3 schema errors were found.\n\nError Counts\n------------\n- SchemaErrorReason.SCHEMA_COMPONENT_CHECK: 3\n\nSchema Error Summary\n--------------------\nschema_context column     check                     failure_cases  n_failure_cases\n                                                   \nColumn         day        isin(['Sun', 'Sat'])      [Thur, Fri]             2\n               size       in_range(1, 4)              [5, 6]                2\n               total_bill less_than_or_equal_to(50)   [50.81]               1\nBelow is a function that allows you to transform the output into a dictionary or a pandas dataframe\ndef get_errors(error, dtype_dict=True):\n    response = []\n\n \n    for item in range(len(error.schema_errors)):\n        error_item = error.schema_errors[item]\n        response.append(\n        {\n            'column'     :error_item.schema.name,\n            'check_error':error_item.schema.checks[0].error,\n            'num_cases'  :error_item.failure_cases.index.shape[0],\n            'check_rows' :error_item.failure_cases.to_dict()\n        })\n    \n    if dtype_dict:\n        return response\n    else:\n        return pd.DataFrame(response)\nget_errors(error,dtype_dict=True)\n[{'column': 'total_bill',\n  'check_error': 'less_than_or_equal_to(50)',\n  'num_cases': 1,\n  'check_rows': {'index': {0: 170}, 'failure_case': {0: 50.81}}},\n {'column': 'day',\n  'check_error': \"isin(['Sun', 'Sat'])\",\n  'num_cases': 81,\n  'check_rows': {'index': {0: 77,\n    1: 78,\n    2: 79,\n    3: 80,\n    4: 81,\n    5: 82,\n    6: 83,\n    7: 84,\n...\n    5: 156,\n    6: 185,\n    7: 187,\n    8: 216},\n   'failure_case': {0: 6, 1: 6, 2: 5, 3: 6, 4: 5, 5: 6, 6: 5, 7: 5, 8: 5}}}]\nüü† Great Expectations\nGreat Expectations is an open-source Python-based library for validating, documenting, and profiling your data.\nIt helps maintain data quality and improve communication about data across teams.\n\nsource : <https://docs.greatexpectations.io/docs/>\nTherefore, we can describe Great Expectations as an open source tool designed to guarantee the quality and reliability of data in various sources, such as databases, tables, files and dataframes.\nIts operation is based on the creation of validation groups that specify the expectations or rules that the data must comply with.\nThe following are the steps that we must define when using this framework:\nDefinition of Expectations: Specify the expectations you have for the data.\nThese expectations can include simple constraints, such as value ranges, or more complex rules about data coherence and quality.\nConnecting to Data Sources: In this step, define the connections you need to make to various data sources, such as databases, tables, files, or dataframes.\nGeneration of Validation Suites: Based on the defined expectations, Great Expectations generates validation suites, which are organized sets of rules to be applied to the data.\nExecution of Validations: Validation suites are applied to the data to verify if they meet the defined expectations.\nThis can be done automatically in a scheduled workflow or interactively as needed.\nGeneration of Analysis and Reports: Great Expectations provides advanced analysis and reporting capabilities.\nThis includes detailed data quality profiles and reports summarizing the overall health of the data based on expectations.\nAlerts and Notifications: If the data does not meet the defined expectations, Great Expectations can generate alerts or notifications, allowing users to take immediate action to address data quality issues.\nTogether, Great Expectations offers a comprehensive solution to ensure data quality over time, facilitating early detection of problems and providing confidence in the integrity and usefulness of data used in analysis and decision-making\nInstall great expectation\n!pip install great_expectations==0.17.22 seaborn matplotlib numpy pandas\nImplementation example\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport re \n\nimport great_expectations as gx\nfrom ruamel.yaml import YAML\nfrom great_expectations.cli.datasource import sanitize_yaml_and_save_datasource\nfrom great_expectations.core.expectation_configuration import ExpectationConfiguration\n\nprint(f\"* great expectations version:{gx.__version__}\")\nprint(f\"* seaborn version:{sns.__version__}\")\nprint(f\"* numpy version:{np.__version__}\")\nprint(f\"* pandas:{pd.__version__}\")\n* great expectations version:0.17.22\n* seaborn version:0.13.0\n* numpy version:1.26.1\n* pandas:2.1.3\nImport dataset using great expectation\npath = 'data/tips.csv'\ndata_gx = gx.read_csv(path)\nList all available expectations by type\nlist_expectations = pd.DataFrame([item for item in dir(data_gx) if item.find('expect_')==0],columns=['expectation'])\nlist_expectations['expectation_type'] = np.select( [\n        list_expectations.expectation.str.find('_table_')>0, \n        list_expectations.expectation.str.find('_column_')>0,  \n        list_expectations.expectation.str.find('_multicolumn_')>0,\n    ],['table','column','multicolumn'],\n    default='other'\n)\n\nplt.figure(figsize=(20,6))\nsns.countplot(x=list_expectations.expectation_type)\nplt.show()\n\n\nIn the image, it can be observed that the available expectations are mainly applied to columns (for example: expect_column_max_to_be_between) and tables (for example: expect_table_columns_to_match_set), although an expectation based on the values of multiple columns can also be applied (for example: expect_multicolumn_values_to_be_unique).\n\nExpectations: Tables\n# The following list contains the columns that the dataframe must have:\ncolumns = ['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size']\ndata_gx.expect_table_columns_to_match_set(column_set = columns)\n{\n  \"success\": true,\n  \"result\": {\n    \"observed_value\": [\n      \"total_bill\",\n      \"tip\",\n      \"sex\",\n      \"smoker\",\n      \"day\",\n      \"time\",\n      \"size\"\n    ]\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\n# Now, we delete two columns, 'time' and 'size,' to validate the outcome\n\ncolumns = = ['total_bill', 'tip', 'sex', 'smoker', 'day']\ndata_gx.expect_table_columns_to_match_set(column_set = columns)\nIf we observe, the result is False, and in the details, they provide information about the columns that the dataframe has in addition to those expected.\n{\n  \"success\": false,\n  \"result\": {\n    \"observed_value\": [\n      \"day\",\n      \"sex\",\n      \"size\",\n      \"smoker\",\n      \"time\",\n      \"tip\",\n      \"total_bill\"\n    ],\n    \"details\": {\n      \"mismatched\": {\n        \"unexpected\": [\n          \"size\",\n          \"time\"\n        ]\n      }\n    }\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\nExpectations: Columns\nLet‚Äôs validate that there is a categorical value within a column\ndata_gx['total_bill_group'] = pd.cut(data_gx['total_bill'],\n                              bins=[0,10,20,30,40,50,float('inf')], \n                              labels=['0-10', '10-20', '20-30', '30-40', '40-50', '>50'],\n                              right=False, \n                              include_lowest=True)\n\n# Now, let's validate if 3 categories exist within the dataset\n\ndata_gx.expect_column_distinct_values_to_contain_set(column='total_bill_group',\n                                                      value_set=['0-10','10-20', '20-30'],\n                                                      result_format='BASIC') \n{\n  \"success\": true,\n  \"result\": {\n    \"observed_value\": [\n      \"0-10\",\n      \"10-20\",\n      \"20-30\",\n      \"30-40\",\n      \"40-50\",\n      \">50\"\n    ],\n    \"element_count\": 244,\n    \"missing_count\": null,\n    \"missing_percent\": null\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\nLet‚Äôs validate that the column does not have null values\ndata_gx.expect_column_values_to_not_be_null('sex')\n{\n  \"success\": true,\n  \"result\": {\n    \"element_count\": 244,\n    \"unexpected_count\": 0,\n    \"unexpected_percent\": 0.0,\n    \"unexpected_percent_total\": 0.0,\n    \"partial_unexpected_list\": []\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\nGreat Expectation Project\nNow, let‚Äôs generate a Great Expectations project to run a group of validations based on one or more datasets.\nInitialize the Great Expectations project:\n !yes Y | great_expectations init\n ___              _     ___                  _        _   _\n / __|_ _ ___ __ _| |_  | __|_ ___ __  ___ __| |_ __ _| |_(_)___ _ _  ___\n| (_ | '_/ -_) _` |  _| | _|\\ \\ / '_ \\/ -_) _|  _/ _` |  _| / _ \\ ' \\(_-<\n \\___|_| \\___\\__,_|\\__| |___/_\\_\\ .__/\\___\\__|\\__\\__,_|\\__|_\\___/_||_/__/\n                                |_|\n             ~ Always know what to expect from your data ~\n\nLet's create a new Data Context to hold your project configuration.\n\nGreat Expectations will create a new directory with the following structure:\n\n    great_expectations\n    |-- great_expectations.yml\n    |-- expectations\n    |-- checkpoints\n    |-- plugins\n    |-- .gitignore\n    |-- uncommitted\n        |-- config_variables.yml\n        |-- data_docs\n        |-- validations\n\nOK to proceed? [Y/n]: \n================================================================================\n\nCongratulations! You are now ready to customize your Great Expectations configuration.\n\nYou can customize your configuration in many ways. Here are some examples:\n\n  Use the CLI to:\n    - Run `great_expectations datasource new` to connect to your data.\n    - Run `great_expectations checkpoint new <checkpoint_name>` to bundle data with Expectation Suite(s) in a Checkpoint for later re-validation.\n    - Run `great_expectations suite --help` to create, edit, list, profile Expectation Suites.\n    - Run `great_expectations docs --help` to build and manage Data Docs sites.\n\n  Edit your configuration in great_expectations.yml to:\n    - Move Stores to the cloud\n    - Add Slack notifications, PagerDuty alerts, etc.\n    - Customize your Data Docs\n\nPlease see our documentation for more configuration options!\nCopy data into the ‚Äògreat_expectations‚Äô folder generated from the project initialization\n!cp -r data gx\n# Let's print the contents of the folder\n\ndef print_directory_structure(directory_path, indent=0):\n    current_dir = os.path.basename(directory_path)\n    print(\"    |\" + \"    \" * indent + f\"-- {current_dir}\")\n    indent += 1\n    with os.scandir(directory_path) as entries:\n        for entry in entries:\n            if entry.is_dir():\n                print_directory_structure(entry.path, indent)\n            else:\n                print(\"    |\" + \"    \" * indent + f\"-- {entry.name}\")\n\n\nprint_directory_structure('gx')\n    |-- gx\n    |    -- great_expectations.yml\n    |    -- plugins\n    |        -- custom_data_docs\n    |            -- renderers\n    |            -- styles\n    |                -- data_docs_custom_styles.css\n    |            -- views\n    |    -- checkpoints\n    |    -- expectations\n    |        -- .ge_store_backend_id\n    |    -- profilers\n    |    -- .gitignore\n    |    -- data\n    |        -- tips.csv\n    |    -- uncommitted\n    |        -- data_docs\n    |        -- config_variables.yml\n    |        -- validations\n    |            -- .ge_store_backend_id\nHere are some clarifications about the files and folders generated in this directory:\nFiles/Folders\nDescription\nüìÑ great_expectations.yml\nThis file contains the main configuration of the project. Details such as storage locations and other configuration parameters are specified here\nüìÇ plugins\ncustom_data_docs:\nüìÑrenderers: It contains custom renderers for data documents.\nüìÑ styles: It includes custom styles for data documents, such as CSS style sheets (data_docs_custom_styles.css).\nüìÑ views: It can contain custom views for data documents.\n\nüìÇ checkpoints\nThis folder could contain definitions of checkpoints, which are points in the data flow where specific validations can be performed.\nüìÇ expectations\nThis is where the expectations defined for the data are stored. This directory may contain various subfolders and files, depending on the project‚Äôs organization.\nüìÇ profilers\nIt can contain configurations for data profiles, which are detailed analyses of data statistics.\nüìÑ .gitignore\nIt is a Git configuration file that specifies files and folders to be ignored when performing tracking and commit operations. (commit)\nüìÇ data\nIt contains the data used in the project, in this case, the file tips.csv.\nüìÇ uncommitted\nüìÇdata_docs: Folder where data documents are generated.\nüìÑconfig_variables.yml: Configuration file that can contain project-specific variables\nüìÇvalidations: It can contain results of validations performed on the data.\n\nConfiguration of datasource and data connectors:\nDataSource: It is the data source used (can be a file, API, database, etc.).\nData Connectors: These are the connectors that facilitate the connection to data sources and where access credentials, location, etc., should be defined.\n\ndatasource_name_file = 'tips.csv'\ndatasource_name = 'datasource_tips'\ndataconnector_name = 'connector_tips'\n# Let's create the configuration for the datasource\n\ncontext = gx.data_context.DataContext()\nmy_datasource_config = f\"\"\"\n    name: {datasource_name}\n    class_name: Datasource\n    execution_engine:\n      class_name: PandasExecutionEngine\n    data_connectors:\n      {dataconnector_name}:\n        class_name: InferredAssetFilesystemDataConnector\n        base_directory: data\n        default_regex:\n          group_names:\n            - data_asset_name\n          pattern: (.*)\n      default_runtime_data_connector_name:\n        class_name: RuntimeDataConnector\n        assets:\n          my_runtime_asset_name:\n            batch_identifiers:\n              - runtime_batch_identifier_name\n\"\"\"\n\nyaml = YAML()\ncontext.add_datasource(**yaml.load(my_datasource_config))\nsanitize_yaml_and_save_datasource(context, my_datasource_config, overwrite_existing=True)\nConfiguration of the expectations\n\nIn the following code snippet, the configuration of three expectations is presented.\nIn particular, the last one includes a parameter called ‚Äòmostly‚Äô with a value of 0.75.\nThis parameter indicates that the expectation can fail in up to 25% of cases, as by default, 100% compliance is expected unless specified otherwise.\nAdditionally, an error message can be specified in markdown format, as shown in the last expectation.\n\nexpectation_configuration_table =  ExpectationConfiguration(\n   expectation_type=\"expect_table_columns_to_match_set\",\n      kwargs= {\n        \"column_set\": ['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size']\n      },\n      meta= {}\n)\n\nexpectation_configuration_total_bill = ExpectationConfiguration(\n      expectation_type= \"expect_column_values_to_be_between\",\n      kwargs= {\n        \"column\": \"total_bill\",\n        \"min_value\": 0,\n        \"max_value\": 100\n      },\n      meta= {}\n)\n\n\nexpectation_configuration_size = ExpectationConfiguration(\n   expectation_type=\"expect_column_values_to_not_be_null\",\n   kwargs={\n      \"column\": \"size\",\n      \"mostly\": 0.75,\n   },\n   meta={\n      \"notes\": {\n         \"format\": \"markdown\",\n         \"content\": \"Expectation to validate column `size` does not have null values.\"\n      }\n   }\n)\nCreation of the expectation suite\nexpectation_suite_name = \"tips_expectation_suite\"\nexpectation_suite = context.create_expectation_suite(\n    expectation_suite_name=expectation_suite_name, \n    overwrite_existing=True\n)\n\n# Add expectations\nexpectation_suite.add_expectation(expectation_configuration=expectation_configuration_table)\nexpectation_suite.add_expectation(expectation_configuration=expectation_configuration_total_bill)\nexpectation_suite.add_expectation(expectation_configuration=expectation_configuration_size)\n\n# save expectation_suite\ncontext.save_expectation_suite(expectation_suite=expectation_suite, \n                               expectation_suite_name=expectation_suite_name)\ndata-quality/gx/expectations/tips_expectation_suite.json\n\nWithin the ‚Äòexpectations‚Äô folder, a JSON file is created with all the expectations generated earlier.\n\nConfiguration of the checkpoints\ncheckpoint_name ='tips_checkpoint'\n\nconfig_checkpoint = f\"\"\"\n    name: {checkpoint_name}\n    config_version: 1\n    class_name: SimpleCheckpoint\n    expectation_suite_name: {expectation_suite_name}\n    validations:\n      - batch_request:\n          datasource_name: {datasource_name}\n          data_connector_name: {dataconnector_name}\n          data_asset_name: {datasource_name_file}\n          batch_spec_passthrough:\n            reader_method: read_csv\n            reader_options: \n              sep: \",\"\n          data_connector_query:\n            index: -1\n        expectation_suite_name: {expectation_suite_name}\n\"\"\"\n\n# Validate if the YAML structure is correct\ncontext.test_yaml_config(config_checkpoint)\n\n# Add the checkpoint to the generated context\ncontext.add_checkpoint(**yaml.load(config_checkpoint)) \nExecute the checkpoint to validate all the configured expectations on the dataset\nresponse = context.run_checkpoint(checkpoint_name=checkpoint_name)\nTo observe the result obtained from the validations, it can be converted to JSON\n response.to_json_dict()\n{'run_id': {'run_name': None, 'run_time': '2023-11-12T20:39:23.346946+01:00'},\n 'run_results': {'ValidationResultIdentifier::tips_expectation_suite/__none__/20231112T193923.346946Z/722b2e93e32fd7222c8ad9339f3e0e1d': {'validation_result': {'success': True,\n    'results': [{'success': True,\n      'expectation_config': {'expectation_type': 'expect_table_columns_to_match_set',\n       'kwargs': {'column_set': ['total_bill',\n         'tip',\n         'sex',\n         'smoker',\n         'day',\n         'time',\n         'size'],\n        'batch_id': '722b2e93e32fd7222c8ad9339f3e0e1d'},\n       'meta': {}},\n      'result': {'observed_value': ['total_bill',\n        'tip',\n        'sex',\n        'smoker',\n        'day',\n        'time',\n        'size']},\n      'meta': {},\n      'exception_info': {'raised_exception': False,\n       'exception_traceback': None,\n       'exception_message': None}},\n     {'success': True,\n...\n  'notify_on': None,\n  'default_validation_id': None,\n  'site_names': None,\n  'profilers': []},\n 'success': True}\nNow, let‚Äôs obtain the results\n context.open_data_docs()\n\nBy executing this code chunk, an HTML file with the results of the validations will open at gx/uncommitted/data_docs/local_site/validations/tips_expectation_suite/__none__/20231112T192529.002401Z/722b2e93e32fd7222c8ad9339f3e0e1d.html\n\n\n\n\nIf you want to learn‚Ä¶\nPandera Documentaci√≥n Oficial\nPandera: Statistical Data Validation of Pandas Dataframes - Researchgate\nGreat Expectation Documentaci√≥n Oficial\nData Quality Fundamentals Book O‚Äôrelly\nGreat Expectation Yoututbe Channel\nImage preview reference: Image by jcomp on Freepik\n\n\n\n",
    "preview": "posts_en/2023-11-17-data-quality/preview.jpg",
    "last_modified": "2023-12-24T18:39:18+01:00",
    "input_file": {}
  }
]
