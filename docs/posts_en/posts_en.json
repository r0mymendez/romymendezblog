[
  {
    "path": "posts_en/2024-01-02-transform-your-pandas-dataframes/",
    "title": "Transform your Pandas Dataframes: Styles, ğŸ¨ Colors, and ğŸ˜ Emojis",
    "description": "In the following article, we will explore a method to add colors and styles to Pandas DataFrames.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2024-01-02",
    "categories": [
      "Python",
      "Data",
      "DataViz"
    ],
    "contents": "\n\nContents\nWhat is Pandas Style?\nğŸŸ£ Pivot Tables\nExample\n\nğŸŸ£ Dataframe: Apple Store apps\nData Schema overview\n\nğŸŸ£ Create Dataframe\nğŸŸ£ Pivot Table\nğŸŸ£ Styling with Pandas\nğŸ¨ Styling: Setting Background Color for Headers\nğŸ¨ Style: Setting the background color for a particular cell\nğŸ¨ Style: Setting the background color for max/min values in the dataframe\nğŸ¨ Style: Color Background Gradients\nğŸ¨ Style: Color Background in columns\nğŸ¨ Style: Color Bar\nğŸ¨ Style: Image in Columns\nğŸ¨ Style: Emoji Representation Based on Percentile Values\n\nğŸ“š References\n\n\n\nIn the following section of this article, we will explore a method to add colors and styles to Pandas DataFrames.\nOur focus will be on the application of colors and emojis, utilizing approaches similar to the popular conditional formatting commonly employed in pivot tables within spreadsheets.\nThrough this strategy, we aim to enhance the presentation of our data, making the exploration and understanding of information not only informative but also visually appealing\nimage generated using partyrockWhat is Pandas Style?\nPandas Styler is a module within the Pandas library that provides methods for creating HTML-styled representations of DataFrames.\nThis feature allows for the customization of the visual appearance of DataFrames during their visualization.\nThe core functionality of Pandas Styler lies in the ability to highlight, color, and format cells based on specific conditions, facilitating the visual identification of patterns and trends in datasets.\nAlso, Pandas Styler stands out for its capability to assist in the design of DataFrames or series by generating visual representations using HTML and CSS.\nThis functionality simplifies the creation of attractive and customized data presentations, enhancing the visualization experience, and enabling a more intuitive interpretation of the information contained in the datasets.\n\nNext we have the code with we are going to create a pivot table using a set of data and from this you will begin to give it different styles and conditional formats such as can be seen in the previous image.\nğŸŸ£ Pivot Tables\n\nThe pivot table is a tabular data structure that provides a summarized overview of information from another table, organizing the data based on one variable and displaying values associated with another variable.\nIn this specific scenario, the pivot table organizes the data according to the â€˜smokerâ€™ column and presents the total sum of tips, categorized by the days on which clients consume in the restaurant\n\nExample\nThe following example shows the pivot_table method with the â€˜tipsâ€™ DataFrame\n\npython code\nimport pandas as pd\nimport seaborn as sns\n\n# create the tips dataframe \ndata = sns.load_dataset('tips')\ndata_pivot = pd.pivot_table(data,\n                    index='smoker',\n                    columns='day',\n                    values='total_bill',\n                    aggfunc='sum').reset_index()\ndata_pivot\nouput\nday\nsmoker\nThur\nFri\nSat\nSun\n0\nYes\n326.24\n252.20\n893.62\n458.28\n1\nNo\n770.09\n73.68\n884.78\n1168.88\nğŸŸ£ Dataframe: Apple Store apps\nIn this analysis, we will use the â€˜ğŸ Apple Store appsâ€™ DataFrame to explore the creation of pivot tables and customization of table styles.\nThis dataset provides detailed insights into Apple App Store applications, covering aspects from app names to specifics like size, price, and ratings.\nOur objective is to efficiently break down the information while applying styles that enhance the presentation and comprehension of data effectively.\nThe dataset was downloaded from Kaggle and it contains more than 7000 Apple iOS mobile application details.\nIt is important to note that the data was collected in July 2017.\nData Schema overview\ncolumn_name\nÂ column description\ntrack_name\nthe column contains the name of the app.\nsize_bytes\nthe column contains the size of the app in bytes.\ncurrency\nthe column contains the currency type.\nprice\nthe column contains the price of the app.\nrating_count_tot\nthe column contains the total number of ratings.\nrating_count_ver\nthe column contains the number of ratings for the current version of the app.\nuser_rating\nthe column contains the average user rating for the app.\nuser_rating_ver\nthe column contains the average user rating for the current version of the app.\nver\nthe column contains the current version of the app.\ncont_rating\nthe column contains the content rating.\nprime_genre\nthe column contains the primary genre.\nsup_devices.num\nthe column contains the number of supported devices.\nipadSc_urls.num\nthe column contains the number of screenshots showed for display.\nlang.num\nthe column contains the number of supported languages.\nvpp_lic\nthe column contains the Vpp Device Based Licensing Enabled.\nğŸŸ£ Create Dataframe\nIn the following code chunk, we will create a DataFrame by reading the CSV file.\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport warnings\n\n# Deactivate pandas warning\nwarnings.filterwarnings('ignore')\n\n\nprint(\"Python Libraries version:\")\nprint('--'*20)\nprint(\"Pandas version: \", pd.__version__)\nprint(\"Numpy version: \", np.__version__)\nprint(\"Matplotlib version: \", plt.matplotlib.__version__)\nPython Libraries version:\n----------------------------------------\nPandas version:  2.1.3\nNumpy version:  1.26.1\nMatplotlib version:  3.8.1\n# Create a dataframe from a csv file\n# You can download the file from the following link https://github.com/r0mymendez/pandas-styles\npath='data/AppleStore.csv'\ndata =pd.read_csv(path,sep=';')\nğŸŸ£ Pivot Table\nIn the next step, we are going to create a pivot table from a DataFrame.\n# Pivot table\n\n# filter the data to keep only the top 15 genres\ntop_genre = data.value_counts('prime_genre')[:15].index.tolist()\ntmp = data.loc[data['prime_genre'].isin(top_genre),['prime_genre','user_rating','price']]\n\n# create a new column with the rating rounded to the nearest integer\ntmp['user_rating'] = [f'rating_{str(math.trunc(item))}' for item in  tmp['user_rating']]\n\n# create a pivot table\ntmp_pivot = (\n        pd.pivot_table(\n            data = tmp,\n            columns='user_rating',\n            index='prime_genre',\n            values='price',\n            aggfunc='mean',\n            fill_value=0\n            ).reset_index().round(2)\n)\n# rename the columns\ntmp_pivot.columns.name=''\n# print the pivot table\ntmp_pivot\n\nğŸŸ£ Styling with Pandas\nNow we will explore the style module in Pandas, that enables us to enhance the visual presentation of DataFrames.\nThe style module provides a differents of options to modify the appearance of the data, allowing us to customize aspects such as:\nColoring Cells: Apply different colors based on cell values or conditions.\nHighlighting: Emphasize specific rows, columns, or values.\nFormatting: Adjust the format of the displayed values, including precision and alignment.\nBar Charts: Represent data with horizontal or vertical bar charts within cells.\nğŸ¨ Styling: Setting Background Color for Headers\nIn this section, we will apply styles to both the titles and the table.\nTherefore we use background colors to highlight the headers and the rest of the table.\n# Styling: Changing Background Color for Column Headers\nheaders = {\n    'selector': 'th.col_heading',\n    'props': 'background-color: #5E17EB; color: white;'\n}\n\nindex_style = {\n    'selector': 'th.index_name',\n    'props': 'background-color: #5E17EB; color: white;'\n}\n\ntmp_pivot_style = (\n    tmp_pivot\n        .style\n            .set_table_styles([headers,index_style])\n            .set_properties(**{'background-color': '#ECE3FF','color': 'black'})\n)\n\ntmp_pivot_style\n\nğŸ¨ Style: Setting the background color for a particular cell\nIn following code snippet illustrates how to set a custom background color for a particular cell in our DataFrame using pandas styling.\n(\n    tmp_pivot\n        .style\n            .set_table_styles([headers, index_style])\n            .set_properties(**{'background-color': '#ECE3FF', 'color': 'black'})\n            .set_properties(**{'background-color': '#FD636B', 'color': 'white'},subset=pd.IndexSlice[4, 'rating_5'])\n)\n\nğŸ¨ Style: Setting the background color for max/min values in the dataframe\nNow, we will focus on highlighting the maximum and minimum values in our DataFrame.\nFor this reason, we will assign distinctive background colors to these extreme values, facilitating a quicker and more intuitive understanding of the dataset.\nThe code snippet below demonstrates how to implement this stylistic enhancement.\n#Â select the columns that start with 'rating_'\ncolumns = tmp_pivot.columns[tmp_pivot.columns.str.startswith('rating_')]\n\n#Â get the max and min values\nmax_value = tmp_pivot[columns].max().max()\nmin_value = tmp_pivot[columns].min().min()\n\n# Establecer el estilo para la celda con el valor mÃ¡ximo\nmax_style = f'border: 4px solid #3BE8B0 !important;'\n\n# Establecer el estilo para la celda con el valor mÃ­nimo\nmin_style = f'background-color: #FF66C4; '\n\n(\n    tmp_pivot\n        .style\n            .set_table_styles([headers, index_style])\n            .set_properties(**{'background-color': '#ECE3FF', 'color': 'black'})\n            .set_properties(**{'background-color': '#FD636B', 'color': 'white'}, subset=pd.IndexSlice[4, 'rating_5'])\n            .applymap(lambda x: max_style if x == max_value else '')\n            .applymap(lambda x: min_style if x == min_value else '', subset=columns)\n)\n\nğŸ¨ Style: Color Background Gradients\nIn the upcoming section, we will delve into the concept of color maps, representing a spectrum of colors arranged in a gradient.\nA colormap, essentially a palette of colors, consists of distinctive denominations, with the most popular ones being [â€˜viridis,â€™ â€˜magma,â€™ â€˜Greens,â€™ â€˜Redsâ€™].\nThe primary objective behind creating these color spectrums is to enhance the visual representation of data.\nEach color in the gradient carries specific nuances, contributing to a more nuanced data visualization experience.\nFor an extensive array of color options, you can explore the matplotlib colormaps link.\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the colormap\nfor cmap_item in ['viridis', 'magma','Greens','Reds']:\n    cmap = plt.get_cmap(cmap_item)\n    # Create a color gradient\n    gradient = np.linspace(0, 1, 256).reshape(1, -1)\n\n    # Display the color palette\n    plt.figure(figsize=(10, 0.2))\n    plt.imshow(gradient, aspect='auto', cmap=cmap)\n    plt.axis('off')\n    plt.title(f'{cmap_item.capitalize()} Color Palette', loc='left', fontsize=9)\n    plt.show()\n\n\nViridis palette\nNow, we will apply a color gradient to our pivot table, allowing you to observe how it is colored using the Viridis palette.\nIn this context, lighter colors signify larger values within the distribution, while darker shades correspond to smaller values in the distribution.\nThis approach provides a visual representation that intuitively conveys the magnitude of the data, making it easier to discern patterns and variations across the dataset.\n\nplt.get_cmap('viridis',lut=20)\n\n(\n    tmp_pivot\n        .style\n            .set_table_styles([headers, index_style])\n            .background_gradient(cmap='viridis',subset=columns)\n)\n\nğŸ¨ Style: Color Background in columns\nIn the next code chunk, we will enhance the visual representation of our pivot table by introducing distinct color backgrounds to specific columns.\nThis technique aids in better highlighting and categorizing data, making it easier to draw insights from the table.\n(\n    tmp_pivot\n        .style\n            .set_table_styles([headers, index_style])\n            .set_properties(**{'background-color': '#FFCFC9','color':'black'},subset=['rating_0','rating_1'])\n            .set_properties(**{'background-color': '#FFF1B0','color':'black'},subset=['rating_2','rating_3'])\n            .set_properties(**{'background-color': '#BEEAE5','color':'black'},subset=['rating_4','rating_5'])\n)\nIğŸ¨ Style: Color Bar\nIn this section, we will implement the style.bar function to introduce a dynamic color bar into our DataFrame.\nThe color bar provides a visual representation of data values, assigning varying colors to different data ranges.\n(\n    tmp_pivot\n        .style\n            .set_table_styles([headers, index_style])\n            .set_properties(**{'background-color': '#ECE3FF', 'color': 'black'})\n            .set_properties(**{'background-color': 'white','color':'black'},subset=columns)\n            .bar(color='#FFCFC9',subset=['rating_0','rating_1'])\n            .bar(color='#FFF1B0',subset=['rating_2','rating_3'])\n            .bar(color='#BEEAE5',subset=['rating_4','rating_5'])\n )\n\nğŸ¨ Style: Image in Columns\nIn this section, we explore the enhancement of data representation by adding an image to an additional column.\nThis approach provides an alternative method to elevate the visual impact of the data being presented.\nThese images can serve as icons, represent brands, or convey additional visual elements to captivate and engage the audience.\n# create a function to add an image to the dataframe depending on the genre\ndef add_image(image_name):\n    img_url = f\"img/icons/img_{image_name}.png\"\n    width   = \"width: 50px\"\n    height  = \"height: 50px\"\n    text_align =\"center\"\n    return f'{width};{height}; content: url({img_url}); text-align:{text_align}'\n\n# apply the function to the dataframe\nstyled_df = (\n    tmp_pivot\n        .head(5)\n        .reset_index()\n        .rename({'index': 'genre'}, axis=1)\n        .style.applymap(add_image, subset=pd.IndexSlice[:, ['genre']])\n        .set_table_styles([headers, index_style])\n        .set_properties(**{'background-color': '#ECE3FF', 'color': 'black'})\n)\n\n# display the dataframe with the images\ndisplay(styled_df)\n\n\nDisclaimer: Issues with Notebook Cache\nDuring the creation of this content, I encountered difficulties related to the notebook cache.\nDespite making changes to the images, the visualization did not update correctly.\nEven after attempting to restart the kernel and clear the cell output, the problem persisted.\nThe only effective solution I found was to change the file names of the images, thus avoiding unexpected cache behavior.\nItâ€™s important to note that these issues may be specific to the Jupyter Notebooks environment and may not reflect inherent limitations in the code or libraries used.\nWhile I tried to address this problem, I did not find a complete solution and opted for an alternative fix by changing the file names.\nIf you have suggestions or additional solutions, I would be delighted to learn and improve this process.\n\nğŸ¨ Style: Emoji Representation Based on Percentile Values\nIn this section, we delve into the creative use of emojis based on percentile values, offering a distinctive approach to elevate data representation.\nBy incorporating diverse emojis, we enhance the visual impact of the data.\nSpecifically, we employ circles and squads as emojis to bring nuanced expressions to our data points.\nIf youâ€™d like to view the code for creating this style, itâ€™s available in my GitHub repository.\nFeel free to check it out and give it a star if you find it helpful!\nâ­ï¸\n\n\n\nğŸ“š References\nIf you want to learnâ€¦ * ğŸ¼ Pandas Style Documentation\nOther references: * Image preview reference: [Imagen de vectorjuice en Freepik]\n\n\n\n",
    "preview": "posts_en/2024-01-02-transform-your-pandas-dataframes/preview.jpg",
    "last_modified": "2024-01-02T23:04:28+01:00",
    "input_file": "transform-your-pandas-dataframes.knit.md"
  },
  {
    "path": "posts_en/2023-12-22-decoding-a-data-model-using-schemaspy-in-snowflake/",
    "title": "Decoding a Data Model - Using SchemaSpy in Snowflake",
    "description": "In following article, we will do on the implementation journey of SchemaPy, leveraging Docker for deployment, and employing a Snowflake database.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2023-12-22",
    "categories": [
      "Python",
      "SQL",
      "Database",
      "Data"
    ],
    "contents": "\n\nContents\nData Model\nConceptual Data Model (CDM)\nLogical Data Model (LDM)\nPhysical data model\n\nâ„ï¸Snowflake SchemaSpy ğŸ³ Docker Compose\nğŸš€ Benefits of SchemaSpy\nPrerequisites\nUsage\nğŸ“ Clone this repository\nâ„ï¸ Snowflake Configuration\nâ„ï¸ Snowflake Account\n\nğŸ™ Build and run the Docker Compose environment\n\nğŸ” Schemapy\nFunctionality Tabs\nSchemaSpy UI\n\nğŸ“š References\n\n\nIn this article, we will delve into a comprehensive exploration of the intricacies of data modeling, spanning from its conceptual inception to its logical definition and eventual physical implementation.\nUnderstanding the life cycle of a data model is crucial for efficiently designing and managing databases.\nFurthermore, in real-world scenarios, existing databases often necessitate reverse engineering to unveil and comprehend their underlying structures.\nIn following sections, we will do on the implementation journey of SchemaPy, leveraging Docker for deployment, and employing a Snowflake database.\n\n\n\n\n\n\n\nData Model\nData modeling is a fundamental task that provides us with a clear understanding of data and the most efficient way to store it.\nThis approach involves representing entities and relationships based on business processes, facilitating documentation and the efficient exploration of data.\nThe ability to generate these documents and understand how data is stored is essential knowledge for any data team.\nIn this context, we will delve into the significance and process of data modeling, emphasizing how this practice becomes a valuable tool for the effective management and exploitation of information.\n\nConceptual Data Model (CDM)\nA conceptual data model (CDM) operates at a high level and offers an organizationâ€™s data needs.\nIt defines a broad and simplified view of the data and the purpose of conceptual data modeling is to establish a shared understanding of the business by capturing the essential concepts of a business process.\nThe focus is on abstracting and representing key entities, relationships, and their interdependencies, fostering a common understanding among stakeholders about the fundamental aspects of the business and its data requirements.\nBelow is an example of a CDM\n\nLogical Data Model (LDM)\nA logical data model (LDM) extends the conceptual data model by providing a complete definition of relationships, incorporating details and the structure of essential entities.\nIn summary the LDM encompasses specific attributes for each entity and the relationships between entities.\nBelow is an example of a LDM\n\nPhysical data model\nA Physical Data Model (PDM) outlines how the model will be translated into an actual database.\nThe PDM incorporates all necessary physical specifications essential for constructing a database, providing a comprehensive guide for database construction, including tables, columns, data types, indexes, constraints, and other implementation details tailored to the chosen database management system.\n\nâ„ï¸Snowflake SchemaSpy ğŸ³ Docker Compose\nIn this following example, we will implement of SchemaSpy through a Docker image in a Snowflake database.\nItâ€™s important to note that the implementation can be extended to other databases such as: mysql, PostgreSQL, Oracle, SQL Server, DB2, H2, HSQLDB, SQLite, Firebird, Redshift and Informix.\nSchemaSpy is a tool that generates an HTML representation of a database schemaâ€™s relationships, providing insights into the database structure.\nSnowflake is a cloud-based data storage and processing service that provides a highly scalable and fully managed data storage environment.\nIts architecture is built on the separation of storage and computing, allowing elastic scalability and optimal performance.\nğŸš€ Benefits of SchemaSpy\nVisual representation of the database schema.\nRelationship insights between tables.\nHTML report for easy sharing and documentation.\nPrerequisites\nBefore you begin, ensure that you have the following installed:\nğŸ³ Docker\nğŸ™ Docker Compose\nâ„ï¸ Snowflake account (You can create an account with a 30-day free trial which includes $400 worth of free usage.)\nUsage\nğŸ“ Clone this repository\nsource:r0mymendez/schemaspy-snowflakegit clone https://github.com/r0mymendez/schemaspy-snowflake.git\ncd schemaspy-snowflake\nâ„ï¸ Snowflake Configuration\nAt this stage, you need to configure the configuration file located at the following path: config/snowflake.properties.\nThis configuration will be used to establish the connection to Snowflake, so it is necessary to modify the account, role, warehouse, schema, and database settings.\nschemaspy.t=snowflake\nschemaspy.u=<user>\nschemaspy.p=<password>\nschemaspy.account=<account>\nschemaspy.role=<role\nschemaspy.warehouse=<warehouse>\nschemaspy.db=<database>\nschemaspy.s=<schema>\nâ„ï¸ Snowflake Account\nBelow is an example of an account URL.\nHowever, you only need to use a portion of it in the configuration:\nFull account URL: https://%60nl1111.eu-west-3.aws`.snowflakecomputing.com\nAccount to use in the configuration: nl1111.eu-west-3.aws\nImage descriptionğŸ™ Build and run the Docker Compose environment\nThis command will build the Docker image and start the container.\ndocker-compose -f docker-compose-snowflake.yaml up\nğŸ” Schemapy\nAfter executing Docker Compose, youâ€™ll find a web site in the output folder.\nThis page features multiple tabs, each offering distinct functionalities, and below, we will explain them.\nFunctionality Tabs\nTables: Provides an overview of all tables in the database schema.\nColumns: Displays detailed information about columns within each table.\nConstraints: Offers insights into constraints applied to the database.\nRelationships: Visualizes the relationships between different tables.\nOrphan Tables: Identifies tables without established relationships.\nAnomalies: Highlights any irregularities or anomalies in the schema.\nRoutines: Presents information about stored routines or procedures.\nSchemaSpy UI\nIn the provided example, we showcase a demo utilizing a synthetic database called Synthea.\nSynthea is a state-of-the-art synthetic data generation tool designed for creating realistic, yet entirely fictitious, patient records and healthcare data.\nIt enables the simulation of diverse medical scenarios, making it a valuable resource for testing and development purposes in the healthcare domain.\nVisit the following ğŸ‘‰link to access a demoğŸ‘ˆ.\n\n\nğŸ“š References\nIf you want to learnâ€¦\nSnowflake\nSnowflake Free courses\nSchemaSpy\nSchemaSpy: Docker Image\nSynthea Project\nOther references:\n- Image preview reference: [Imagen de vectorjuice en Freepik]\n\n\n\n",
    "preview": "posts_en/2023-12-22-decoding-a-data-model-using-schemaspy-in-snowflake/preview.jpg",
    "last_modified": "2023-12-24T18:31:23+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-12-17-sql-to-python-pandas-a-sql-users-quick-guide/",
    "title": "SQL to Python Pandas: A SQL User's Quick Guide",
    "description": "Unlock the essentials of translating your code from SQL to Python with this quick guide tailored for SQL users. Dive into key insights and streamline your data manipulation process.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2023-12-17",
    "categories": [
      "Python",
      "SQL",
      "Database",
      "Data"
    ],
    "contents": "\n\nContents\nNew York Flights âœˆï¸ ğŸ§³ ğŸ—½\nEntity-relationship diagram [DER]\nInstallation: Setting Up nycflights13\n\nğŸŸ¢ Pandas, NumPy, and nycflights13 for Data Analysis in Python\nğŸŸ¢ SELECT and FROM Statements\nğŸ“— SELECT: All Columns\nğŸ“— SELECT: Specific Columns\n\nğŸŸ¢ Filtering Operators (WHERE)\nğŸ“— Utilizing â€˜WHEREâ€™ for Equality ( = )\nğŸ“— Utilizing â€˜WHEREâ€™ for Equality ( = )\nğŸ“— Utilizing â€˜WHEREâ€™ with Inequality ( != )\nğŸ“— Utilizing â€˜WHEREâ€™ for Comparisons (>=, <=, <, >)\nğŸ“— Utilizing â€˜WHEREâ€™ with between operator\nğŸ“— Utilizing â€˜WHEREâ€™ with â€œLIKEâ€ Clause\nğŸ“— Utilizing â€˜WHEREâ€™ with Null or Not Null Values\n\nğŸŸ¢ Order by Statement\nğŸŸ¢ Distinct Values: Removing Duplicates from Results\nğŸŸ¢ Adding Calculated Columns\nğŸŸ¢ Group by Statement\nğŸŸ¢ Group by and Having Statement\nğŸŸ¢ Group by with multiple calculations\nğŸŸ¢ Union Statement\nğŸŸ¢ CASE WHEN Statement\nğŸŸ¢ JOIN Statement\nğŸ“— Join Types\nğŸ“— Join Key\n\nğŸŸ¢ Rename\nğŸ“š References\n\n\nIn this post, we will compare the implementation of Pandas and SQL for data queries.\nWeâ€™ll explore how to use Pandas in a manner similar to SQL by translating SQL queries into Pandas operations.\nItâ€™s important to note that there are various ways to achieve similar results, and the translation of SQL queries to Pandas will be done by employing some of its core methods.\nNew York Flights âœˆï¸ ğŸ§³ ğŸ—½\n\nsource image Image by upklyak on Freepik\nWe aim to explore the diverse Python Pandas methods, focusing on their application through the nycflights13 datasets.\nThis datasets offer comprehensive information about airlines, airports, weather conditions, and aircraft for all flights passing through New York airports in 2013.\nThrough this exercise, weâ€™ll not only explore Pandas functionality but also learn to apply fundamental SQL concepts in a Python data manipulation environment.\nEntity-relationship diagram [DER]\n\nThe nycflights13 library contains tables with flight data from New York airports in 2023.\nBelow, you can find a high-level representation of an entity-relationship diagram with its five tables.\n\n\nInstallation: Setting Up nycflights13\nTo install the nycflights13 library, you can use the following command:\n!pip install nycflights13\nThis library provides datasets containing comprehensive information about flights from New York airports in 2023.\nOnce installed, you can easily access and analyze this flight data using various tools and functionalities provided by the nycflights13 package.\nğŸŸ¢ Pandas, NumPy, and nycflights13 for Data Analysis in Python\nIn the next code snippet, we are importing essential Python libraries for data analysis.\n* ğŸ“— Pandas is a library for data manipulation and analysis, * ğŸ“— Numpy provides support for numerical operations * ğŸ“— Nycflights13 is a specialized library containing datasets related to flights from New York airports in 2023.\nimport pandas as pd\nimport numpy as np\nimport nycflights13 as nyc\nIn the following lines of code, we are assigning two specific datasets from the nycflights13 library to variables.\nflights = nyc.flights\nairlines = nyc.airlines\nğŸŸ¢ SELECT and FROM Statements\nğŸ“— SELECT: All Columns\nThe following SQL query retrieves all columns and rows from the â€œğŸ›©ï¸ flightsâ€ table.\nIn Pandas, the equivalent is simply writing the DataFrame name, in this case, â€œflights.â€ For example:\nğŸ”sql\n  SELECT * FROM flights;\nğŸpython\nflights\nğŸ“— SELECT: Specific Columns\nTo select specific columns from a Pandas DataFrame, you can use the following syntax:\nğŸ”sql\n  select \n    year, \n    month, \n    day, \n    dep_time, \n    flight, \n    tailnum, \n    origin, \n    dest \n  from flights;\nğŸpython\n(\n    flights\n        .filter(['year', 'month', 'day', 'dep_time', 'flight', \n                'tailnum', 'origin', 'dest'])\n)\nğŸŸ¢ Filtering Operators (WHERE)\nğŸ“— Utilizing â€˜WHEREâ€™ for Equality ( = )\nTo filter all âœˆï¸ flights where the origin is â€˜JFKâ€™ in Pandas, you can use the following code:\nğŸ”sql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights \n  where origin = 'JFK'\nlimit 10;\nğŸpython\n(   flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest'])\n      .query(\"origin=='JFK'\")\n      .head(10)\n)\nğŸ“— Utilizing â€˜WHEREâ€™ for Equality ( = )\nTo achieve the same filtering in Pandas for specific criteria: * âœˆï¸ Flights departing from JFK, LGA, or EWR.\nğŸ”sql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights \n  where origin in ( 'JFK', 'LGA', 'EWR' ) \nlimit 10;\nğŸpython\n (  flights\n        .filter(['year', 'month', 'day', 'dep_time', 'flight', \n        'tailnum', 'origin', 'dest'])\n      .query(\"origin in ['JFK', 'EWR', 'LGA']\")\n      .head(10)\n)\nğŸ“— Utilizing â€˜WHEREâ€™ with Inequality ( != )\nTo achieve the same filtering in Pandas for specific criteria:\nâœˆï¸ Flights departing from JFK, LGA, or EWR.\nâœˆï¸ Flights not destined for Miami (MIA).\nğŸ”sql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' ) and dest<>'MIA'\nlimit 10;\nğŸpython\n(   flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest'])\n      .query(\"(origin in ['JFK', 'EWR', 'LGA']) and (dest != 'MIA')\")\n   .head(10)\n)\nğŸ“— Utilizing â€˜WHEREâ€™ for Comparisons (>=, <=, <, >)\nTo achieve the same filtering in Pandas for specific criteria:\nâœˆï¸ Flights departing from JFK, LGA, or EWR.\nâœˆï¸ Flights not destined for Miami (MIA).\nâœˆï¸ Flights with a distance less than or equal to 1000 km.\nğŸ”sql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and distance < = 1000\nlimit 10;\nğŸpython\n( flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight',\n            'tailnum', 'origin', 'dest', 'time_hour', 'distance'])\n      .query(\"(origin in ['JFK', 'EWR', 'LGA']) and (dest != 'MIA') and (distance <= 1000)\")\n      .head(10)\n)\nğŸ“— Utilizing â€˜WHEREâ€™ with between operator\nTo achieve the same filtering in Pandas for specific criteria:\nâœˆï¸ Flights departing from JFK, LGA, or EWR.\nâœˆï¸ Flights not destined for Miami (MIA).\nâœˆï¸ Flights with a distance less than or equal to 1000 km.\nâœˆï¸ Flights within the period from September 1, 2013, to September 30, 2013.\n\nğŸ”sql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and distance < = 1000\n   and time_hour between '2013-09-01' and '2012-09-30'\nlimit 10;\nğŸpython\n(   flights.filter([['year', 'month', 'day', 'dep_time', 'flight', \n          'tailnum', 'origin', 'dest', 'time_hour', 'distance'])\n      .query(\n            \"(origin in ['JFK', 'EWR', 'LGA'])\" \n             \" and (dest != 'MIA')\"\n             \" and (distance <= 1000)\"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n         )\n      .head(10)\n)\nğŸ“— Utilizing â€˜WHEREâ€™ with â€œLIKEâ€ Clause\nTo achieve the same filtering in Pandas for specific criteria:\nâœˆï¸ Flights departing from JFK, LGA, or EWR.\nâœˆï¸ Flights not destined for Miami (MIA).\nâœˆï¸ Flights with a distance less than or equal to 1000 km.\nâœˆï¸ Flights within the period from September 1, 2013, to September 30, 2013.\nâœˆï¸ Flights where the tailnum contains â€˜N5â€™ in the text.\nYou can use the following code:\nğŸ”sql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and distance < = 1000\n   and time_hour between '2013-09-01' and '2012-09-30'\n   and tailnum like '%N5%'\nlimit 10;\nğŸpython\n(\n    flights     \n      .filter(['year', 'month', 'day', 'dep_time', 'flight', 'tailnum',\n        'origin', 'dest', 'time_hour'])\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and (dest != 'MIA') \"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n             \" and (tailnum.str.find('N5')>=0)\"\n       )\n      .head(10)\n)\nğŸ“— Utilizing â€˜WHEREâ€™ with Null or Not Null Values\nTo achieve the same filtering in Pandas for specific criteria:\nâœˆï¸ Flights departing from JFK, LGA, or EWR.\nâœˆï¸ Flights not destined for Miami (MIA).\nâœˆï¸ Flights with a distance less than or equal to 1000 km.\nâœˆï¸ Flights within the period from September 1, 2013, to September 30, 2013.\nâœˆï¸ Flights where the tailnum contains â€˜N5â€™ in the text.\nâœˆï¸ Flights where dep_time is null\nYou can use the following code:\nğŸ”sql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and distance < = 1000\n   and time_hour between '2013-09-01' and '2012-09-30'\n   and tailnum like '%N5%'\n   and dep_time is null\nlimit 10;\nğŸpython\n(\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest', 'time_hour'])\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and (dest != 'MIA') \"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n             \" and (tailnum.str.find('N5')>=0)\"\n             \" and dep_time.isnull()\"\n       )\n      .head(10)\n)\nğŸŸ¢ Order by Statement\nThe .sort_values() methods in Pandas are equivalent to the ORDER BY clause in SQL.\n1ï¸âƒ£.**.sort_values(['origin','dest'], ascending=False)**: This method sorts the DataFrame based on the â€˜originâ€™ and â€˜destâ€™ columns in descending order (from highest to lowest).\nIn SQL, this would be similar to the ORDER BY origin DESC, dest DESC clause.\n2ï¸âƒ£.**.sort_values(['day'], ascending=True)**: This method sorts the DataFrame based on the â€˜dayâ€™ column in ascending order (lowest to highest).\nIn SQL, this would be similar to the ORDER BY day ASC clause.\nBoth methods allow you to sort your DataFrame according to one or more columns, specifying the sorting direction with the ascending parameter.\nTrue means ascending order, and False means descending order.\nğŸ”sql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and distance < = 1000\n   and time_hour between '2013-09-01' and '2012-09-30'\n   and tailnum like '%N5%'\n   and dep_time is null\norder by  origin, dest desc\nlimit 10;\nğŸpython\n(\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest', 'time_hour'])\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and (dest != 'MIA') \"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n             \" and (tailnum.str.find('N5')>=0)\"\n             \" and year.notnull()\"\n       )\n      .sort_values(['origin','dest'],ascending=False)\n      .head(10)\n      \n)\nğŸŸ¢ Distinct Values: Removing Duplicates from Results\nTo perform a distinct select in pandas, you need to first execute the entire query, and then apply the drop_duplicates() method to eliminate all duplicate rows.\nğŸ”sql\nselect distinct origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and time_hour between '2013-09-01' and '2012-09-30'\norder by  origin, dest desc;\nğŸpython\n(\n    flights\n      .filter(['origin','dest','time_hour'])\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and (dest != 'MIA') \"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n       )\n      .filter(['origin','dest'])\n      .drop_duplicates()\n      \n)\nğŸŸ¢ Adding Calculated Columns\nNow, letâ€™s introduce a new calculated column called â€œdelay_total,â€ where we sum the values from the â€œdep_delayâ€ and â€œarr_delayâ€ columns.\nğŸ”sql\nselect  \n  origin,\n  dest,\n  time_hour,\n  dep_delay,\n  arr_delay,\n  flights.dep_delay + flights.arr_delay as delay_total \nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and time_hour between '2013-09-01' and '2012-09-30';\nğŸpython\n(\n    flights\n      .filter(['origin', 'dest', 'time_hour', 'dep_delay', 'arr_delay'])\n      .assign(delay_total = flights.dep_delay + flights.arr_delay )\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and (dest != 'MIA') \"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n       )\n)\nğŸŸ¢ Group by Statement\nTo perform a GROUP BY operation in pandas, weâ€™ll use the groupby method, which operates similarly to its SQL counterpart.\nSimilarly, we can employ common aggregate functions such as sum, max, min, mean (equivalent to avg in SQL), and count.\nBelow is a simple example to illustrate this process:\nğŸ”sql\nselect \n  year,\n  month,\n  max(dep_delay) as dep_delay,\nfrom flights\ngroup by \n  year,\n  month;\nğŸpython\n(\n    flights\n      .groupby(['year','month'],as_index=False)\n      ['dep_delay'].max()\n)\nğŸŸ¢ Group by and Having Statement\nIn the following example, weâ€™ll explore how to implement a HAVING clause in pandas, leveraging the query method, as weâ€™ve done previously for filtering.\nğŸ”sql\nselect \n  year,\n  month,\n  max(dep_delay) as dep_delay,\nfrom flights\ngroup by \n  year,\n  month\nhaving max(dep_delay)>1000\nğŸpython\n(\n    flights\n      .groupby(['year','month'],as_index=False)['dep_delay']\n      .max()\n      .query('(dep_delay>1000)') # having\n)\nğŸŸ¢ Group by with multiple calculations\nWhen working with pandas and needing to perform multiple calculations on the same column or across different columns, the agg function becomes a valuable tool.\nIt allows you to specify a list of calculations to be applied, providing flexibility and efficiency in data analysis.\nConsider the following SQL query:\nğŸ”sql\nselect \n  year,\n  month,\n  max(dep_delay)  as dep_delay_max,\n  min(dep_delay)  as dep_delay_min,\n  mean(dep_delay) as dep_delay_mean,\n  count(*)        as dep_delay_count,\n  max(arr_delay)  as arr_delay_max,\n  min(arr_delay)  as arr_delay_min,\n  sum(arr_delay)  as arr_delay_sum\nfrom flights\ngroup by \n  year,\n  month\n\nThis query retrieves aggregated information from the â€œflightsâ€ dataset, calculating various statistics like maximum, minimum, mean, count, and sum for both â€œdep_delayâ€ and â€œarr_delayâ€ columns.\nTo achieve a similar result in pandas, we use the agg function, which allows us to specify these calculations concisely and efficiently.\nThe resulting DataFrame provides a clear summary of the specified metrics for each combination of â€œyearâ€ and â€œmonth.â€\n\nğŸpython\nresult = (\n    flights\n      .groupby(['year','month'],as_index=False)\n      .agg({'dep_delay':['max','min','mean','count'], 'arr_delay':['max','min','sum']})     \n)\n\n# Concatenate function names with column names\nresult.columns = result.columns.map('_'.join)\n\n# Print the results\nresult\nğŸŸ¢ Union Statement\nTo execute a UNION ALL operation in Pandas, it is necessary to create two DataFrames and concatenate them using the concat method.\nUnlike SQL, a DataFrame in Pandas can be combined to generate additional columns or additional rows.\nTherefore, it is essential to define how the concatenation should be performed:\naxis=1 => Union that appends another dataset to the right, generating more columns.\naxis=0 => Union that appends more rows.\n\nIn our example, we will perform the equivalent of a UNION ALL in SQL, so we will use axis=0.\nğŸ”sql\nselect  \n  origin,\n  dest,\n  time_hour,\n  dep_delay,\n  arr_delay,\n  flights.dep_delay + flights.arr_delay as delay_total ,\n  'NYC' group\nFROM flights  \n  WHERE origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and time_hour between '2013-09-01' and '2012-09-30'\nORDER BY flights.dep_delay + flights.arr_delay DESC\nLIMIT 3\nUNION ALL\nselect  \n  origin,\n  dest,\n  time_hour,\n  dep_delay,\n  arr_delay,\n  flights.dep_delay + flights.arr_delay as delay_total ,\n  'MIA' group\nFROM flights  \n  WHERE origin in ( 'JFK', 'LGA', 'EWR' ) \n   and time_hour between '2013-07-01' and '2012-09-30'\n  ORDER BY flights.dep_delay + flights.arr_delay DESC\n  LIMIT 2;\nğŸpython\nFlights_NYC = (\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight',\n              'tailnum', 'origin', 'dest', 'time_hour',\n              'dep_delay', 'arr_delay'])\n      .assign(delay_total = flights.dep_delay + flights.arr_delay )\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n       )\n     .assign(group ='NYC')      \n     .sort_values('delay_total',ascending=False)     \n     .head(3)\n)\n\nFlights_MIAMI = (\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest', 'time_hour', \n              'dep_delay', 'arr_delay'])\n      .assign(delay_total = flights.dep_delay + flights.arr_delay )\n      .query(\n             \" (dest in ['MIA', 'OPF', 'FLL'])\"\n             \" and ('2013-07-01' <= time_hour <= '2013-09-30')\"\n       )\n     .assign(group ='MIA') \n     .sort_values('delay_total',ascending=False)     \n     .head(2)\n)\n\n# union all \npd.concat([ Flights_NYC,Flights_MIAMI],axis=0)\nğŸŸ¢ CASE WHEN Statement\nTo replicate the CASE WHEN statement, we can use two different methods from NumPy:\n1ï¸âƒ£.\nIf there are only two conditions, for example, checking if the total delay exceeds 0, then we label it as â€œDelayedâ€; otherwise, we label it as â€œOn Timeâ€.\nFor this, the np.where method from NumPy is utilized.\nğŸ”sql\nselect  \n  origin,\n  dest,\n  time_hour,\n  dep_delay,\n  arr_delay,\n  (case \n    when flights.dep_delay + flights.arr_delay >0 then 'Delayed'\n    else 'On Time' end) as status ,\nFROM flights  \nLIMIT 5;\n\nğŸpython\n(\n    flights\n      .filter(['year', 'month', 'day', 'dep_time',\n              'flight', 'tailnum', 'origin', 'dest', \n              'time_hour', 'dep_delay', 'arr_delay'])\n      .assign(status=np.where((flights['dep_delay'] + flights['arr_delay']) > 0,                                'Delayed',\n                               'On Time'))\n      .head(5)\n)\n \n2ï¸âƒ£.\nIn case there are more conditions, such as identifying Miami airports and labeling them as â€œMIAâ€, labeling â€œATLâ€ airports that they are in Altanta, and for any other cases, using the label â€œOTHERâ€.\nFor this, the np.select method from NumPy is employed.\nCity\nName\nAcronym\nMiami\nMiami International\n(MIA)\nMiami\nOpa-locka Executive\n(OPF)\nMiami\nFort Lauderdale-Hollywood\n(FLL)\nAtlanta\nHartsfield-Jackson Atlanta\n(ATL)\nAtlanta\nDeKalb-Peachtree\n(PDK)\nAtlanta\nFulton County\n(FTY)\nğŸ”sql\nselect  \n  origin,\n  dest,\n  time_hour,\n  dep_delay,\n  arr_delay,\n  (case \n    when dest in ('ATL','PDK','FTY') then 'ATL'\n    when dest in ('MIA','OPF','FLL') then 'MIA'\n    else 'Other'\n  end) as city ,\nFROM flights  \nLIMIT 10;\nğŸpython\n(\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest', 'time_hour',\n              'dep_delay', 'arr_delay'])\n      .assign( city=np.select([flights['dest'].isin(['ATL','PDK','FTY']), \n                             flights['dest'].isin(['MIA', 'OPF', 'FLL'])],\n                             ['ATL','MIA'],\n                             default='Other')\n              )\n    .head(10)\n)\nğŸŸ¢ JOIN Statement\nEntity relationship diagram [DER]\n\nWhen performing a join in Pandas, the merge method should be used.\nğŸ“— Join Types\nHow: Specifies the type of join to be performed.\nAvailable options: {'left', 'right', 'outer', 'inner', 'cross'}\njoinsğŸ“— Join Key\nOn: The key on which the tables will be joined.\nIf more than one column is involved, a list should be provided.\nExamples:\nSingle variable: on='year'\nfligths.merge(planes, how='inner', on='tailnum')\nTwo variables: on=[â€˜yearâ€™,â€˜monthâ€™,â€˜dayâ€™]\nfligths.merge(weather, how='inner', on=['year','month','day'])\nleft_on/right_on: When the columns have different names, these parameters should be used. For example:\nfligths.merge(airports, how='inner', left_on = 'origin', rigth_on='faa')\nHereâ€™s an example using the airlines and flights tables:\nğŸ”sql\nselect  \n  f.year,\n  f.month,\n  f.day,\n  f.dep_time,\n  f.flight,\n  f.tailnum,\n  f.origin as airport_origen,\n  f.dest,\n  f.time_hour,\n  f.dep_delay,\n  f.arr_delay,\n  f.carrier,\n  a.name as airline_name\nFROM flights  f\n  left join airlines a on f.carrier = a.carrier\nLIMIT 5;\nğŸŸ¢ Rename\n\nThe rename method is used to rename columns, similar to the â€œasâ€ clause in SQL.\n\nğŸpython\n(\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n        'tailnum', 'origin', 'dest', 'time_hour', 'dep_delay', \n        'arr_delay', 'carrier'])\n      .merge(airlines, how = 'left', on ='carrier')\n      .rename(columns= {'name':'airline_name', 'origin':'airport_origen'})\n      .head(5)\n)\n \n\nYou can find all the code in a ğŸ python notebook in the following [link]\n\nğŸ“š References\nIf you want to learn more about `Pandas` and `NumPy`â€¦\n- [Pandas]\n- [NumPy]\nOther references:\n- Image preview reference: [Imagen de vectorjuice en Freepik]\n\n\n\n",
    "preview": "posts_en/2023-12-17-sql-to-python-pandas-a-sql-users-quick-guide/preview.jpg",
    "last_modified": "2023-12-24T18:28:45+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-12-10-code-quality-sonarqube/",
    "title": "Code Quality - SonarQube",
    "description": "Code Quality, crucial for robust software, is upheld by tools like SonarQube.  This article explores its significance, implementation, and management.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2023-12-10",
    "categories": [
      "Python"
    ],
    "contents": "\n\nContents\nWhat is a code quality?\nWhat is Clean Code?\nWhat is a SonarQube?\nWhat is a SonarLindt?\nSonarQube vs SonarLindt\nSonarQube Features\nğŸ“œ Rules\nğŸ“œ Quality Profiles\nğŸ“œ Quality Gates\n\nğŸŸ£ Using SonarQube with Docker: A Step-by-Step Guide\nğŸ³ Install Docker:\nğŸ³ Pull the SonarQube Image\nğŸ³ Run SonarQube Container:\nğŸ³ Check if the container is running\nğŸš€ Open the sonarqube web application\nğŸ“„ Create the script in python\nğŸ“„ Create a configuration file\nğŸ“„ Code folder\nğŸ“ Create sonarqube project\nğŸ” Create a quality gate\nğŸ” Start the scanning\nğŸ” Scan Results\nğŸ“Š Sonarqube Metrics\nğŸš§ SonarQube Issues\n\nğŸŸ£ Sonarqube API\nKey Features of the SonarQube API:\nCommon Use Cases:\n\nğŸŸ£ SonarQube API: A Step-by-Step Guide\nInstall SonarQubeClient\nConfig the sonarqube client\nGet sonarqube client projects\nGet the project events\n\nğŸ“š References\n\n\nWhat is a code quality?\nCode quality measures the accuracy and reliability of codeâ€”but being bug-free and portable is not the only measure of code quality.\nIt also includes how developer-friendly the code is.\nCode quality also describes how easy it is to understand, modify, and reuse the code if necessary.\nTestable: A piece of code should be easy to develop tests for and run tests on.\nPortable: You might want it to be easy to take the code from one environment the make it work again in another environment. If so, you can measure portability.\nReusable: High-quality code is modular and designed for reuse.\nsource: Amazon: What is code quality?\n\nImproving code quality involves addressing these factors to create code that is not only technically robust but also user-friendly and conducive to collaborative development.\n\nWhat is Clean Code?\n\nâ€œClean Codeâ€ is code thatâ€™s easy to read, maintain, understand and change through structure and consistency yet remains robust and secure to withstand performance demands.\"\nsource: clean code](https://www.sonarsource.com/solutions/clean-code/))\n\nğŸ§¹ Clean Code refers to the practice of writing code in a clear, readable and efficient manner, placing a strong emphasis on the understandability and maintainability of the code.\nThe main premise is that the code must not only work correctly, but it must also be easy to understand for any developer who reads it so that it can be reused quickly.\nTo achieve this, it is important to follow good programming practices and adopt conventions that promote clarity.\nAspects to consider:\nğŸ“– Readability: Code should be written in a way that is easily understandable.\nDescriptive names should be used for variables and functions, and confusing abbreviations should be avoided.\nğŸš€ Simplicity: Simplicity is sought in the design and structure of the code.\nAvoiding unnecessary complexity and keeping functions and methods concise helps facilitate understanding.\nğŸ› ï¸ Maintainability: The code must be easy to maintain over time.\nThis involves minimizing code duplication, following sound design principles, and documenting effectively.\nğŸ”„ Consistency: Consistent coding conventions should be followed throughout the project to improve consistency and make the code easier to read.\nWhat is a SonarQube?\nSonarQube is a self-managed, automatic code review tool that systematically helps you deliver Clean Code.\nAs a core element of our Sonar solution, SonarQube integrates into your existing workflow and detects issues in your code to help you perform continuous code inspections of your projects.\nsource: SonarQube\nWhat is a SonarLindt?\nSonarLint is a free IDE extension that can be used in Visual Code, Visual studio or eclipse.\nThis plugin allows you to identify coding problems in real time, in order to avoid errors, vulnerabilities and code smells while you write your code.\nSonarLint can perform code analysis in JS/TS, Python, PHP, Java, C, C++, Go and IaC.\nSonarQube vs SonarLindt\nThe following is a comparative table in which we compare the functionality and the context in which each of these tools is applied\nSonarQube - SonarLindtimage source:sonarsource docs\nFeature\nSonarQube\nSonarLint\nScope\nServer-based code analysis for entire projects/repositories\nIDE-based code analysis for individual developers\nDeployment\nRequires a centralized server installation\nIntegrated directly into the developerâ€™s IDE\nReal-time Feedback\nProvides feedback during continuous integration\nOffers real-time feedback within the developerâ€™s IDE\nCode Analysis Depth\nOffers in-depth static code analysis and metrics\nProvides on-the-fly code analysis with immediate feedback\nIntegration with CI/CD\nIntegrates with CI/CD pipelines for automated analysis\nSupports local analysis as well as CI/CD integration\nRule Configurability\nHighly configurable rules for code quality and security\nLimited rule configuration options within the IDE\nCollaboration\nFacilitates collaboration among development teams\nFocuses on individual developer experience and collaboration\nUse Case\nSuitable for larger projects with centralized management\nIdeal for individual developers or smaller development teams\nSonarQube Features\nğŸ“œ Rules\nIn SonarQube, â€œrulesâ€ are definitions that describe code patterns that indicate potential problems, security vulnerabilities, or areas for improvement in code quality.\nThe SonarQube analysis engine uses these rules to scan the source code and highlight potential problems.\nEach rule has a definition that allows a specific pattern to be identified and covers aspects such as: good practices, errors, vulnerability, security, among others.\n\nğŸ“œ Quality Profiles\nQuality profiles are a set of specific and organized rules that apply to specific projects.\nThese profiles allow you to customize the rules you want to use to evaluate code quality based on your specific needs and standards.\nTherefore, profiles allow you to customize which rules apply to a project and provide predefined profiles for different programming languages.\n\n\nğŸ“œ Quality Gates\nQuality Gates are sets of conditions that are applied to a project after running a static analysis of the code and applying the rules defined in the quality profiles.\nThese conditions allow you to quantify and evaluate whether a project meets specific quality criteria, helping to determine if the code is acceptable for implementation.\nMetric\nDescription\nReliability Rating\nThis indicator evaluates the reliability of the code, which means how prone the code is to contain errors or defects.\nSecurity Rating\nThis indicator evaluates the security level of the code, which means how prone the code is to contain security vulnerabilities.\nSecurity Hotspots Reviewed\nThis indicator evaluates whether all security points identified in the code have been reviewed.\nMaintainability Rating\nThis indicator evaluates the ease with which the code can be maintained and improved in the future\nCoverage\nThis indicator examines the % of code that has been executed.\nDuplicated Lines (%)\nThis flag checks for duplicate lines in the code\n\nğŸŸ£ Using SonarQube with Docker: A Step-by-Step Guide\nğŸ³ Install Docker:\nEnsure that Docker is installed on your machine.\nYou can download and install Docker from the official website: Docker\nğŸ³ Pull the SonarQube Image\nOpen a terminal and run the following command to pull the official SonarQube Docker image from Docker Hub:\ndocker pull sonarqube\nğŸ³ Run SonarQube Container:\nExecute the following command in a terminal to run the sonarqube container.\ndocker run \n   -d --name sonarqube \n   -e SONAR_ES_BOOTSTRAP_CHECKS_DISABLE=true \n   -p 9000:9000 \n   sonarqube:latest\nCommand\nDescription\ndocker run\nThis is the command used to run a Docker container.\n-d\nThis is a Docker run option that stands for â€œdetached.â€ It runs the container in the background, which means you get your terminal prompt back immediately after starting the container.\n--name sonarqube\nThis option allows you to specify a name for the container. In this case, the name â€œsonarqubeâ€ is given to the container, which makes it easier to reference the container later.\n-e SONAR_ES_BOOTSTRAP_CHECKS_DISABLE=true\nThis option is used to set an environment variable within the container. It disables Elasticsearch bootstrap checks when starting SonarQube.\n-p 9000:9000\nThis option is used to map ports between the host and the container. It specifies that port 9000 on the host should be mapped to port 9000 inside the container, allowing access to SonarQube.\nsonarqube:latest\nThis is the Docker image to run. It specifies the image named â€œsonarqubeâ€ and the â€œlatestâ€ tag, pulling the latest version from Docker Hub and creating a container from that image.\nğŸ³ Check if the container is running\n$ docker ps\nCONTAINER ID   IMAGE              COMMAND                  CREATED          STATUS         PORTS                                       NAMES\nd8e576b6039e   sonarqube:latest   \"/opt/sonarqube/dockâ€¦\"   13 seconds ago   Up 8 seconds   0.0.0.0:9000->9000/tcp, :::9000->9000/tcp   sonarqube\nğŸš€ Open the sonarqube web application\nâ€˜ğŸ”— Link: localhost:9000â€™\n\nğŸ” Initial user and password login: admin password: admin\n\n\nğŸ“„ Create the script in python\nThis PythonğŸ code was created using chat-gpt intentionally includes some practices that may violate default SonarQube configurations.\nIn addition, we will duplicate this file in the same folder to be able to generate an alert for duplicate code by generating the same file with the name main_bk\nmain.py\n# Code Smell: Unused variable\nunused_variable = 42\n\n# Code Smell: Unused function\ndef unused_function():\n    pass\n\n# Code Smell: Redundant parentheses\nresult = (5 * 3)\n\n# Code Smell: Unused import\nimport unused_module\n\n# Code Smell: Print statement (considered a bad practice)\nprint(\"Hello, World!\")\n\n# Code Smell: Hardcoded values\nmagic_number = 42\n\n# Code Smell: Unused loop variable\nfor _ in range(5):\n    pass\n\n# Code Smell: Assignment in a condition\nif (result == 0):  # Fix the equality check\n    pass\n\n# Code Smell: Using a single underscore as a variable name\n_ = \"Unused variable\"\n\n# Code Smell: Using a mutable default argument in a function\ndef append_item(item, my_list=None):\n    if my_list is None:\n        my_list = []\n    my_list.append(item)\n    return my_list\n\n# Code Smell: Unused variable in an exception block\ntry:\n    value = int(\"text\")\nexcept ValueError as e:\n    unused_exception_variable = e\n\n# Code Smell: Complex lambda function\nsquare = lambda x: x**2 + 2*x + 1\nğŸ“„ Create a configuration file\nThe following file contains the properties for execute the code quality processes in sonarqube.\nIt is necesary to change the projectkey and the project name, in my case the both name is â€œtestâ€, this parameters is config when you create the projects in SonarQube web application.\nFilename: â€œsonar-project.propertiesâ€\n# must be unique in a given SonarQube instance\nsonar.projectKey=test\n\n# --- optional properties ---\n\n# defaults to project key\nsonar.projectName=test\n\n# defaults to 'not provided'\nsonar.projectVersion=1.0\n \n# Path is relative to the sonar-project.properties file. Defaults to .\nsonar.sources=.\nsonar.language=python\n\n#----- Default SonarQube server\nsonar.host.url=http://localhost:9000 \n\n# Encoding of the source code. Default is default system encoding\nsonar.sourceEncoding=UTF-8\nğŸ“„ Code folder\nIn summary, the folder that should contain the following files\nğŸ“„ main.py\nğŸ“„ main_bk.py\nğŸ“„ sonar-project.properties\nğŸ“ Create sonarqube project\nThe images below illustrate all the steps necessary to create the project and obtain the token for the scanner later.\n\n\n\n\n\nğŸ” Create a quality gate\nClick on \"Create\" and define the name of the quality gate.\n\n\nClick on \"Unlock Editing\" to update the condition metrics.\n\nSet the current quality gate as the default\n\nğŸ” Start the scanning\nTo start the scanning process using the SonarQube CLI, execute the following command after replacing placeholders with your specific information.\nEnsure that this command is run in the terminal where the source path, containing the â€˜sonar-project.propertiesâ€™ file, is located.\nItâ€™s crucial to set your token as an environment variable using the following syntax: -e SONAR_LOGIN=â€œyour_token_hereâ€.\nFor example, if your token is â€œsqp_08ad32fcb385e8192b1a4e0aabdc54be3b1ad946â€ the corresponding command to be executed would be:\ndocker run --network=host \n -e SONAR_HOST_URL=http://host.docker.internal:9000 \n -e SONAR_LOGIN=\"sqp_08ad32fcb385e8192b1a4e0aabdc54be3b1ad946\" \n -e SONAR_PROJECT_KEY=data-quality \n -it -v \"$(pwd):/usr/src\" \n sonarsource/sonar-scanner-cli\n\nIn the terminal you will see the following log\n\nDigest: sha256:494ecc3b5b1ee1625bd377b3905c4284e4f0cc155cff397805a244dee1c7d575\nStatus: Downloaded newer image for sonarsource/sonar-scanner-cli:latest\nINFO: Scanner configuration file: /opt/sonar-scanner/conf/sonar-scanner.properties\nINFO: Project root configuration file: /usr/src/sonar-project.properties\nINFO: SonarScanner 5.0.1.3006\nINFO: Java 17.0.8 Alpine (64-bit)\nINFO: Linux 5.10.25-linuxkit amd64\nINFO: User cache: /opt/sonar-scanner/.sonar/cache\nINFO: Analyzing on SonarQube server 10.3.0.82913\nINFO: Default locale: \"en_US\", source code encoding: \"UTF-8\"\nINFO: Load global settings\nINFO: Load global settings (done) | time=624ms\nINFO: Server id: 147B411E-AYxB7bsnEO8aoeQvN3oK\nINFO: User cache: /opt/sonar-scanner/.sonar/cache\nINFO: Load/download plugins\nINFO: Load plugins index\nINFO: Load plugins index (done) | time=453ms\nINFO: Load/download plugins (done) | time=5843ms\nINFO: Process project properties\nINFO: Process project properties (done) | time=42ms\nINFO: Execute project builders\nINFO: Execute project builders (done) | time=10ms\nINFO: Project key: data-quality\nINFO: Base dir: /usr/src\nINFO: Working dir: /usr/src/.scannerwork\nINFO: Load project settings for component key: 'data-quality'\nWARN: SCM provider autodetection failed. Please use \"sonar.scm.provider\" to define SCM of your project, or disable the SCM Sensor in the project settings.\nINFO: Load quality profiles\nINFO: Load quality profiles (done) | time=3356ms\nINFO: Load active rules\nwhen the process finish you can see the following log\nINFO: ------------- Run sensors on project\nINFO: Sensor Analysis Warnings import [csharp]\nINFO: Sensor Analysis Warnings import [csharp] (done) | time=3ms\nINFO: Sensor Zero Coverage Sensor\nINFO: Sensor Zero Coverage Sensor (done) | time=14ms\nINFO: SCM Publisher No SCM system was detected. You can use the 'sonar.scm.provider' property to explicitly specify it.\nINFO: CPD Executor Calculating CPD for 0 files\nINFO: CPD Executor CPD calculation finished (done) | time=0ms\nINFO: Analysis report generated in 273ms, dir size=137.8 kB\nINFO: Analysis report compressed in 306ms, zip size=17.4 kB\nINFO: Analysis report uploaded in 412ms\nINFO: ANALYSIS SUCCESSFUL, you can find the results at: http://localhost:9000/dashboard?id=test\nINFO: Note that you will be able to access the updated dashboard once the server has processed the submitted analysis report\nINFO: More about the report processing at http://localhost:9000/api/ce/task?id=AYxTnq5AFtsPP8-M5c1w\nINFO: Analysis total time: 18.832 s\nINFO: ------------------------------------------------------------------------\nINFO: EXECUTION SUCCESS\nINFO: ------------------------------------------------------------------------\nINFO: Total time: 27.819s\nINFO: Final Memory: 21M/80M\nINFO: --------------------------------------------------------\nğŸ” Scan Results\nUpon completion of the code scan, you can view the results of the code analysis on the web application at localhost:9000\nğŸ“Š Sonarqube Metrics\nThe following table presents the metrics defined by SonarQube, which are objective indicators designed to evaluate the quality of the source code.\nThese metrics allow a quantitative evaluation of various critical aspects of the code.\nMetric\nDescription\nReliability\nThe â€œReliabilityâ€ metric refers to the reliability of the code. It measures the number of issues related to software reliability, such as errors and failures.\nMaintainability\nThe â€œMaintainabilityâ€ metric in SonarQube assesses how easy it is to maintain and enhance the code over time. It measures code quality in terms of structure, readability, and ease of maintenance.\nNew Code Smells\nThis metric indicates the number of new â€œcode smellsâ€ introduced in the code recently. â€œCode smellsâ€ are design patterns or coding practices that may indicate issues with code quality.\nSecurity\nThe â€œSecurityâ€ metric in SonarQube evaluates code security for potential vulnerabilities. It measures the codeâ€™s ability to resist attacks and protect data and systems.\nNew Vulnerabilities\nIt indicates the number of new security vulnerabilities introduced in the code recently. Vulnerabilities are weaknesses that can be exploited by attackers.\nSecurity Review\nThis metric in SonarQube assesses the quality of security reviews conducted on the code. It measures the effectiveness of reviews in detecting and correcting security issues.\nNew Security Hotspots\nIt signifies the number of new â€œsecurity hotspotsâ€ introduced in the code recently. Security hotspots are areas of the code that require special attention due to potential security issues.\nCoverage\nThe â€œCoverageâ€ metric in SonarQube refers to code coverage. It measures the proportion of code that has been tested through unit tests or automated tests.\nDuplications\nThis metric in SonarQube identifies sections of code that are duplicated in multiple places. Identifying and reducing duplications can improve code quality and maintainability.\nThe result of the scan is the following\n\nğŸš§ SonarQube Issues\nâ€œSonarQube Issuesâ€ refer to issues identified by the SonarQube static code analysis process.\nEach issue provides:\nğŸ“ Location of the Issue: Enables identification of where the problem exists in the code.\nğŸ¤” Reason for the Issue: Offers a detailed explanation of why it is considered a problem.\nğŸ’¬ Activity: Facilitates collaboration by allowing the addition of comments and discussions about potential solutions.\nğŸ‘¤ Assignment: Permits the assignment of the issue to a registered user in SonarQube for tracking and resolution.\nğŸ“Š Status: Initiates with the â€œOpenâ€ status upon creation but may transition to other statuses such as:\nâ€œResolved as Fixedâ€: Indicates that the problem identified in the issue has been fixed in the source code.\nâ€œResolved as False Positiveâ€: Initiates that the problem initially identified as an â€œissueâ€ is not actually a problem or does not require correction.\nâ€œResolved as Wonâ€™t Fixâ€: Indicates that a decision has been made not to address or correct the problem noted in the issue.\n\nğŸ·ï¸ Tags: Allows the addition of tags to enhance the identification and categorization of issues.\n\n\nğŸŸ£ Sonarqube API\nThe SonarQube API empowers users to interact with and extract information programmatically from a SonarQube instance.\nThese APIs serve as a tool for developers, administrators, and integrators, enabling the automation of tasks, retrieval of project metrics, and seamless integration of SonarQube data into various workflows.\nKey Features of the SonarQube API:\nğŸ¤– Automation: The SonarQube API allows for the automation of various tasks related to project analysis, configuration, and management.\nThis includes triggering analyses, retrieving analysis results, and managing quality profiles.\nğŸ“Š Data Retrieval: Users can extract a wide range of data from SonarQube, including project metrics, issues, code smells, duplications, and more.\nThis information can be used for reporting, analytics, and custom dashboards.\nğŸ”— Integration: The API facilitates integration with other development tools, continuous integration (CI) systems, and external applications.\nThis enables seamless incorporation of SonarQubeâ€™s code quality and security analysis into existing development pipelines.\nâš™ï¸ Configuration Management: The API allows users to manage SonarQube configurations programmatically.\nThis includes creating and updating quality profiles, setting project configurations, and managing global settings.\nCommon Use Cases:\nğŸ¤– Automated Analysis: Integrate SonarQube analysis into your CI/CD pipeline by triggering analyses automatically after code commits or builds.\nğŸ“Š Custom Reporting: Extract specific metrics and data from SonarQube to generate custom reports tailored to your team or organizationâ€™s needs.\nğŸ› Issue Tracking: Retrieve information about code issues, security vulnerabilities, and code smells to integrate SonarQube data into your issue tracking or project management system.\nğŸš¦ Quality Gate Status: Monitor and retrieve the status of Quality Gates for projects to ensure that code meets predefined quality criteria.\nğŸ› ï¸ Configuration as Code: Manage SonarQube configurations using scripts or code, making it easier to replicate configurations across different instances.\nğŸŸ£ SonarQube API: A Step-by-Step Guide\nInstall SonarQubeClient\npip install python-sonarqube-api\nConfig the sonarqube client\nfrom sonarqube import SonarQubeClient\nimport pandas as pd\n\nsonar = SonarQubeClient(sonarqube_url=\"http://localhost:9000\", username='admin', password='admin123')\nGet sonarqube client projects\nprojects = sonar.projects.search_projects()\nprojects\n{'paging': {'pageIndex': 1, 'pageSize': 100, 'total': 1},\n 'components': [\n  {'key': 'test',\n   'name': 'test',\n   'qualifier': 'TRK',\n   'visibility': 'public',\n   'lastAnalysisDate': '2023-12-10T13:47:32+0000',\n   'managed': False}]}\nGet the project events\nproject_analyses_and_events = sonar.project_analyses.search_project_analyses_and_events(project=\"test\")\nproject_analyses_and_events = pd.json_normalize(project_analyses_and_events['analyses'])\n\n[item for item in project_analyses_and_events['events'] if item!=[]]```\n\n```bash\n[[{'key': 'AYxT_AnVJaFOvYeUMXMa', 'category': 'VERSION', 'name': '1.0'}],\n [{'key': 'AYxT-xvhJaFOvYeUMXIe',\n   'category': 'QUALITY_GATE',\n   'name': 'Failed',\n   'description': 'Coverage on New Code < 80, New Code Smells > 0'}],\n [{'key': 'AYxT-cgDJaFOvYeUMXFb',\n   'category': 'QUALITY_GATE',\n   'name': 'Passed',\n   'description': ''}],\n [{'key': 'AYxT7MvDJaFOvYeUMW5W',\n   'category': 'QUALITY_GATE',\n   'name': 'Failed',\n   'description': 'Coverage on New Code < 80'}]]\nğŸ“š References\nIf you want to learnâ€¦\nSonarqube: Documentation\nSonarLint: IDE\nSonarLint: Documentation\nSonarqube: API\nSonarqube: Python\nOther references:\nImage preview reference: [Imagen de storyset en Freepik]\n\n\n\n",
    "preview": "posts_en/2023-12-10-code-quality-sonarqube/preview.jpg",
    "last_modified": "2023-12-24T18:32:46+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-12-02-rabbitmq-pika/",
    "title": "RabbitMQ-Pika",
    "description": "RabbitMQ allows you to manage message queues between senders and recipients. In the next post we are going to use **Pika** in python for its implementation.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2023-12-02",
    "categories": [
      "Python"
    ],
    "contents": "\n\nContents\nIntroduction: What is RabbitMQ?\nImplementation with Pika in Python ğŸ\n1ï¸âƒ£ . Install pika\n2ï¸âƒ£ . Create send.py ğŸ“„ file\n3ï¸âƒ£. Create send.py ğŸ“„ file\n4ï¸âƒ£. MongoDB + Pika\n\n\n\n\nRabbitMQ enables the management of message queues between senders and receivers.\nIn the following post, we will employ Pythonâ€™s Pika library for its implementation.\n\nIntroduction: What is RabbitMQ?\nRabbitMQ is an intermediary system designed to facilitate the transfer of messages between producers and consumers through the implementation of queues.\nThis component, essential in distributed systems architecture, is grounded in key concepts:\n1ï¸âƒ£.\nProducer: The entity responsible for originating and dispatching messages.\n2ï¸âƒ£.\nQueue: A reservoir where messages are temporarily stored.\n3ï¸âƒ£.\nConsumer: The receiving instance that processes messages according to the systemâ€™s needs.\nThis introduction aims to provide a clear and concise overview of the fundamental elements of RabbitMQ, paving the way for a deeper understanding of its functioning in messaging environments.\n\nImplementation with Pika in Python ğŸ\nImplementing in Python with the Pika library involves creating two essential programs: the producer and the consumer.\nPika provides an effective interface for communication with RabbitMQ, leveraging a set of carefully designed objects for this purpose.\nIn our practical example, envision the producer as an application designed to manage food delivery orders ğŸ›µ.\nThis application, geared towards optimizing the delivery process, is responsible for sending multiple messagesğŸ“ related to user ğŸ“± food orders.\nTo achieve this implementation, we will undertake the following steps:\nSteps\nDescriptions\nProducer:\nDevelop a program that, like an efficient order-taker, generates and sends messagesğŸ“ to the RabbitMQ queue. These messages will contain valuable information about food orders.\nConsumer:\nCreate a program that acts as the receiver of these messages in the queue. The consumer will be responsible for processing these messages according to the systemâ€™s needs, performing relevant actions, such as managing the delivery of orders.\nThis structured and efficient approach ensures a clear and functional implementation, providing a robust foundation for systems managing information flows in dynamic environments.\n1ï¸âƒ£ . Install pika\n!pip install pika\n2ï¸âƒ£ . Create send.py ğŸ“„ file\nimport pika\nfrom datetime import datetime\n\nconnection = pika.BlockingConnection(\n    pika.ConnectionParameters(host='localhost'))\nchannel = connection.channel()\n\nchannel.queue_declare(queue='delivery')\n\npedidos=['ğŸ•ğŸ•ğŸ•','ğŸ”ğŸ”ğŸ”','ğŸ°ğŸ°ğŸ°','ğŸºğŸºğŸº']\n\nfor i in pedidos:\n    channel.basic_publish(exchange='', routing_key='delivery', body=i)\n    print(\" [x] Se envia pedido!'\"+ i)\n\nconnection.close()\n3ï¸âƒ£. Create send.py ğŸ“„ file\nimport pika, sys, os\nfrom datetime import datetime\n\n\ndef main(queue='delivery'):\n    connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))\n    channel = connection.channel()\n    channel.queue_declare(queue=queue)\n\n    def callback(ch, method, properties, body):\n        print(\" [x] Received %r\" % body.decode())\n\n    channel.basic_consume(queue='delivery', on_message_callback=callback, auto_ack=True)\n\n    print(' [*] Waiting for messages. To exit press CTRL+C')\n    channel.start_consuming()\n\nif __name__ == '__main__':\n    try:\n        main(queue=queue)\n    except KeyboardInterrupt:\n        print('Interrupted')\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)\nImage description4ï¸âƒ£. MongoDB + Pika\nIn the following, we will modify the script to enable it to connect to a MongoDB Atlas and perform the insertion of received messages.\nimport pymongo\nimport pika, sys, os\nfrom datetime import datetime\n\n# Crear una conexion con MongoClient\nclient = pymongo.MongoClient(\"mongodb+srv://NombreUser:PasswordUser@clusterName.moczg.mongodb.net/rabbit?retryWrites=true&w=majority\")\n\n# Database\ndb = client[\"rabbit\"]\n\n# Collection\ncollection= db[\"mensajes\"]\n\ndef main(queue='delivery'):\n    connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))\n    channel = connection.channel()\n    channel.queue_declare(queue=queue)\n\n    def callback(ch, method, properties, body):\n        print(\" [x] Received %r\" % body.decode())\n        body_indsert={'fecha':datetime.now(),'queue':queue,'message':body.decode()}\n        db[\"mensajes\"].insert_one(body_indsert)\n\n    channel.basic_consume(queue='hello', on_message_callback=callback, auto_ack=True)\n\n    print(' [*] Waiting for messages. To exit press CTRL+C')\n    channel.start_consuming()\n\nif __name__ == '__main__':\n    try:\n        main(queue=queue)\n    except KeyboardInterrupt:\n        print('Interrupted')\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)\nTo download the code for these two files, you can do so from the following link.\nTo learn more about RabbitMQ, you can visit the following sites:\nğŸ“„ Oficial Documentation\nğŸ“„ Rabbit Tutorial\nğŸ Pika\nImage preview reference: Imagen de rawpixel.com en Freepik\n\n\n\n",
    "preview": "posts_en/2023-12-02-rabbitmq-pika/preview.jpg",
    "last_modified": "2023-12-24T18:37:01+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-11-18-aws-copilot/",
    "title": "AWS Copilot",
    "description": "In the following article, I explain what AWS Copilot is, how to use this project, and the ease of implementing it.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://r0mymendez.github.io/romymendezblog/"
      }
    ],
    "date": "2023-11-14",
    "categories": [
      "Cloud Computing"
    ],
    "contents": "\n\nContents\nIntroduction\nğŸ’¡AWS ECS (Elastic Container Services)\nâš’ï¸ ECS with EC2 instances\nâš’ï¸ ECS with AWS Fargate (Serverless)\n\nğŸ’¡AWS Copilot\nğŸ” Comparison of Task Responsibilities\nğŸ§© Components\nğŸš€ Deployment with AWS Copilot: A 5-Step Guide\nğŸ” Logs\nğŸ“‰ Traffic to production\nğŸ§ªTesting\nğŸ’° Service Cost\n\nğŸ¯ Key Takeaways\nğŸ“š References\n\n\nIntroduction\nIn this article, I explain the use of the AWS Copilot service.\nHowever, to carry it out, itâ€™s necessary to start by analyzing what the AWS Elastic Container Service (ECS) is and how it works, along with its deployment methods.\nThis is necessary because AWS Copilot performs the implementation and deployment of an application using ECS\nğŸ’¡AWS ECS (Elastic Container Services)\nElastic Container Service ( ECS ) is a scalable container orchestration platform proprietary to AWS.\nIt is designed to run, stop, and manage containers in a cluster.\nTherefore, ECS is AWSâ€™s Docker container service that handles the orchestration and provisioning of Docker containers.\nThis service includes the following concepts:\nâ˜ï¸ Task Definition: Describes how to start a Docker container.\nâ˜ï¸ Task: This is a running container with the configuration defined in the task definition.\nâ˜ï¸ Service: Defines long-running tasks from the same task definition.\nâ˜ï¸ Cluster: A logical group of EC2 instances.\nâ˜ï¸ Container Instance: This is just an EC2 instance that is part of an ECS cluster and has Docker installed.\nâš’ï¸ ECS with EC2 instances\nIn this model, containers are deployed on EC2 instances (VMs) created for the cluster.\nECS manages them along with the tasks that are part of the task definition\nâœ… Advantages\nâŒ Disadvantages\n- Complete control over the type of EC2 instance used is provided.\n- When working with EC2, itâ€™s necessary for the administrator of this architecture to handle all security updates and scaling of instances.\n- It allows the use of instances that can be optimized depending on what you want to execute.\n- The cost is based on the type of EC2 instance running within the cluster and the VPC networks.\nâš’ï¸ ECS with AWS Fargate (Serverless)\nIn this serverless configuration, the reliance on EC2 instances is eliminated, simplifying the deployment process.\nInstead, you only need to specify the required CPU and memory combination.\nAWS Fargate allows for a fully managed and serverless container deployment experience.\nâœ… Advantages\nâŒ Disadvantages\n- There are no servers to manage.\n- ECS + Fargate supports only one network mode, and this limits control over the network layer.\n- AWS is in charge of the availability and scalability of the containers.\n- Cost is based on the CPU and memory you select. The number of CPU cores and GB determines the cost of running the cluster.\n- Fargate Spot is a new capability that can run ECS tasks that are interruption-tolerant at up to a 70% discount compared to the Fargate price.\n\nğŸ’¡AWS Copilot\nAWS Copilot is a tool used through the AWS command line that simplifies the creation, deployment, monitoring, and operation of containers in ECS using a local development environment\nThis tool manages the components required for the deployment and operation of an application, such as VPC, load balancers, deployment pipelines, and storage.\nTherefore, itâ€™s only necessary to provide an application container and minimal configurations, resulting in a faster deployment and focusing on application development.\nğŸ” Comparison of Task Responsibilities\nThe services will communicate with each other, so it is necessary to consider the following scenarios:\nActivities\nWithout AWS-copilot\nWith AWS-copilot\nApplication developmen\nğŸ“— Development team\nğŸ“— Development team\nContainer generation\nğŸ“— Development team\nğŸ“— Development team\nVirtual Private Cloud (VPC) Subnets\nğŸ“— Development team\nğŸ“™ AWS-Copilot\nLoad balancers\nğŸ“—Development team\nğŸ“™ AWS-Copilot\nDeployment flows (ci/cd)\nğŸ“— Development team\nğŸ“™ AWS-Copilot\nPersistent storage of your application\nğŸ“— Development team\nğŸ“™ AWS-Copilot\nSynchronize deployment across environments\nğŸ“— Development team\nğŸ“™ AWS-Copilot\nğŸ§© Components\nThe following table contains the components that are configured when using the AWS Copilot service.\nComponent\nDescription\nApplication\nAn application is a grouping mechanism for the pieces of your system.\nEnviroment\nAn environment is a stage in the deployment of an application.\nService\nA service is a single process of long-running code within a container.\nğŸš€ Deployment with AWS Copilot: A 5-Step Guide\nIn just 5 steps we can deploy an application using aws-copilot, as shown in the following image.\nThis allows the development team to only focus on development and not so much on the deployment of the infrastructure.\nThe first application that is deployed in copilot will make a default configuration and the same will be with a serverless container in fargate.\nAs seen in the following image, with only 5 steps we can deploy an application.\nImage descriptionThe steps in the flow are as follows:\nInstall AWS Copilot, which will require AWS client credentials.\nCreate the Dockerfile for our application.\nExecute copilot init in a terminal to initialize.\nWhen running init, some questions will appear to answer, such as the application name, service type, service name, and Dockerfile location.\nIn this final step, a URL will be provided to access the application\nğŸ” Logs\nTo obtain the logs of the deployed containers, it is necessary to execute the following command:\n$ copilot svc logs- follow\nğŸ“‰ Traffic to production\nTo deploy in production it is necessary to be able to generate different environments, so to generate them it is necessary to execute the following command.\n$ copilot env init\nSubsequently, it is important to be able to modify the manifest file that contains all the application configurations and is located in Â nombredeaplicacion/manifest.yml\nOnce the environment configuration is complete, it is necessary to deploy it to production (or another environment, but the following example is in production).\n$ copilot svc deploy â€”env production\nğŸ§ªTesting\nIn order to test the deployed application, you can use ApacheBench which allows you to generate traffic to the web application.\nFor this it is necessary to be able to execute the following command in which you want to generate a number of 5000 transactions to my service with a concurrency of 25 requests at a time.\nab -n 5000 -c 25<http://app12345.us-east-1.elb.amazonaws.com>\nIf I do not have the expected response, I can modify my manifest file and horizontally scale the application based on the different environments.\nğŸ’° Service Cost\nAWS Copilot is distributed by Amazon under an Apache 2.0 license, making it an open-source application.\nAs an open-source tool, AWS Copilot incurs no additional costs.\nThe pricing is solely determined by the usage of the configured services.\nThis cost-efficient model allows users to leverage the full capabilities of AWS Copilot without incurring any licensing fees.\nğŸ¯ Key Takeaways\nIn conclusion, AWS Copilot stands out for the following features:\nAWS Copilot emerges as a robust, open-source AWS tool that streamlines the deployment of production-ready containers in a mere 5 steps, allowing development teams to concentrate on coding rather than grappling with infrastructure intricacies.\nConfiguration is effortless, demanding only the execution of a few commands and adjustments to the manifest file based on the applicationâ€™s resource requirements.\nAddressing horizontal scaling needs is a breeze â€“ a simple modification to the manifest file followed by a deployment is all it takes.\nAWS Copilot facilitates the establishment of a CI/CD pipeline for seamless, automatic deployments across various environments.\nEffortlessly generate KPIs, set up alarms, and collect metrics with just a few commands through the user-friendly AWS Copilot service.\nğŸ“š References\nğŸ“šTitle: Presentamos AWS Copilot, Site: Blog de Amazon Web Services (AWS), Author: Nathan Peck,Gabriel Gasca Torres y JosÃ© Lorenzo CuÃ©ncar, url: <https://aws.amazon.com/es/blogs/aws-spanish/presentamos-aws-copilot/>,\nğŸ“šTitle: IntroducciÃ³n a Amazon ECS mediante AWS Copilot, Site: DocumentaciÃ³n oficical de AWS, Author: AWS, url: <https://docs.aws.amazon.com/es_es/AmazonECS/latest/userguide/getting-started-aws-copilot-cli.html>\nğŸ“šTitle: AWS Copilot, Site: AWS, Author: AWS, url: <https://aws.amazon.com/es/containers/copilot/>\nğŸ“šTitle: Gentle Introduction to How AWS ECS Works with Example Tutorial, Site: Medium, Author: Tung Nguyen , Url: <https://medium.com/boltops/gentle-introduction-to-how-aws-ecs-works-with-example-tutorial-cea3d27ce63d>\nImage preview reference: Image by vectorjuice on Freepik\n\n\n\n",
    "preview": "posts_en/2023-11-18-aws-copilot/preview.jpg",
    "last_modified": "2023-12-24T18:38:07+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-11-17-data-quality/",
    "title": "Data Quality",
    "description": "In the following article you will find the definition of data quality, what the domains are and how to quickly implement a solution.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://r0mymendez.github.io/romymendezblog/"
      }
    ],
    "date": "2023-11-12",
    "categories": [
      "Data",
      "Python"
    ],
    "contents": "\n\nContents\nWhat is Data quality?\nData quality dimensions - Use case\nPython Frameworks\nDifferences between Licencia Apache 2.0 y MIT\nDataset\nğŸŸ¢ Pandera\nInstall pandera\nImplementation Example\n\nğŸŸ  Great Expectations\nInstall great expectation\nImplementation example\n\nIf you want to learnâ€¦\n\n\n\nIn the current digital environment, the amount of available data is overwhelming.\nHowever, the true cornerstone for making informed decisions lies in the quality of this data.\nIn this article, we will explore the crucial importance of data quality, analyzing the inherent challenges that organizations face in managing information.\nAlthough often overlooked, data quality plays a fundamental role in the reliability and usefulness of the information that underpins our strategic decisions.\nWhat is Data quality?\nData quality measures how well a dataset complies with the criteria of accuracy, completeness, validity, consistency, uniqueness, timeliness, and fitness for purpose, and is fundamental for all data governance initiatives within an organization.\nData quality standards ensure that companies make decisions based on data to achieve their business objectives.\nsource: IBM\n\nsource: DataCamp cheat sheet\nThe following table highlights the various domains of data quality, from accuracy to fitness, providing an essential guide for assessing and enhancing the robustness of datasets:\nDimensions\nDescription\nğŸ¯ Accuracy\nData accuracy, or how close data is to reality or truth. Accurate data is that which faithfully reflects the information it seeks to represent.\nğŸ§© Completeness\nMeasures the entirety of the data. A complete dataset is one that has no missing values or significant gaps. Data integrity is crucial for gaining a comprehensive and accurate understanding.\nâœ… Validity\nIndicates whether the data conforms to defined rules and standards. Valid data complies with the established constraints and criteria for a specific dataset..\nğŸ”„ Consistency\nRefers to the uniformity of data over time and across different datasets. Consistent data does not exhibit contradictions or discrepancies when compared with each other\nğŸ“‡ Uniqueness\nEvaluates whether there are no duplicates in the data. Unique data ensures that each entity or element is represented only once in a dataset\nâŒ›Timeliness\nRefers to the timeliness of data. Timely information is that which is available when needed, without unnecessary delays.\nğŸ‹ï¸ Fitness\nThis aspect evaluates the relevance and usefulness of data for the intended purpose. Data should be suitable and applicable to the specific objectives of the organization or analysis being conducted.\nData quality dimensions - Use case\nNext, we provide an example where some issues with an e-commerce-based use case can be observed.\nID TransacciÃ³n\nID Cliente\nProducto\nCantidad\nPrecio Unitario\nTotal\nâšª 1\n10234\nLaptop HP\n1\n$800\n$800\nğŸŸ£ 2\n\nWireless Headphones\n2\n$50\n$100\nğŸ”µ 3\n10235\nSmartphone\n-1\n$1000\n-$1000\nğŸŸ¢ 4\n10236\nWireless Mouse\n3\n$30\n$90\nğŸŸ¢ 4\n10237\nWireless Keyboard\n2\n$40\n$80\nğŸŸ£ Row 2 (Completeness): Row 2 does not comply with data integrity (Completeness) as the customer ID is missing.\nCustomer information is incomplete, making it challenging to trace the transaction back to a specific customer.\nğŸ”µ Row 3 (Accuracy and Consistency): Row 3 exhibits accuracy (Accuracy) and consistency (Consistency) issues.\nThe quantity of products is negative, which is inaccurate and goes against the expected consistency in a transaction dataset.\nğŸŸ¢ Row 4 (Uniqueness): The introduction of a second row with the same transaction ID (Transaction ID = 4) violates the uniqueness principle.\nEach transaction should have a unique identifier, and having two rows with the same Transaction ID creates duplicates, affecting the uniqueness of transactions.\n\nPython Frameworks\nThe following are some of the Python implementations carried out to perform data quality validations:\nFramework\nDescripciÃ³n\nGreat Expectations\nGreat Expectations is an open-source library for data validation. It enables the definition, documentation, and validation of expectations about data, ensuring quality and consistency in data science and analysis projects\nPandera\nPandera is a data validation library for data structures in Python, specifically designed to work with pandas DataFrames. It allows you to define schemas and validation rules to ensure data conformity\nDora\nDora is a Python library designed to automate data exploration and perform exploratory data analysis.\nLetâ€™s analyze some of the metrics that can be observed in their GitHub repositories, taking into account that the metrics were obtained on 2023-11-12.\nMetricas\nGreat Expectations\nPandera\nDora\nğŸ‘¥ Members\n399\n109\n106\nâš ï¸ Issues: Open\n112\n273\n1\nğŸŸ¢ Issues: Close\n1642\n419\n7\nâ­ Stars\n9000\n2700\n623\nğŸ“º Watching\n78\n17\n42\nğŸ” Forks\n1400\n226\n63\nğŸ“¬ Open PR\n43\n19\n0\nğŸ Version Python\n>=3.8\n>=3.7\nNo especificada\nğŸ“„ Version Number\n233\n76\n3\nğŸ“„ Last Version\n0.18.2\n0.17.2\n0.0.3\nğŸ“† Last Date Version\n9 Nov 2023\n30 sep 2023\n30 jun 2020\nğŸ“„ Licence type\nApache-2.0 license\nMIT\nMIT\nğŸ“„ Languages\nPython 95.1%\nJupyter Notebook 4.3%\nJinja 0.4%\nJavaScript 0.1%\nCSS 0.1%\nHTML 0.0%\n\nPython 99.9%\nMakefile 0.1%\n\nPython100%\n\nDifferences between Licencia Apache 2.0 y MIT\nNotification of Changes:\nApache 2.0: Requires notification of changes made to the source code when distributing the software.\nMIT: Does not require specific notification of changes.\n\nCompatibility:\nApache 2.0: Known to be compatible with more licenses compared to MIT.\nMIT: Also quite compatible with various licenses, but Apache 2.0 License is often chosen in projects seeking greater interoperability with other licenses.\n\nAttribution:\nApache 2.0: Requires attribution and the inclusion of a copyright notice.\nMIT: Requires attribution to the original authorship but may have less strict requirements in terms of how that attribution is displayed.\n\nConsidering these currently analyzed metrics, letâ€™s proceed with an example implementation using Pandera and Great Expectations.\n\nDataset\nFor the development of this example, we will use the dataset named â€˜Tips.â€™ You can download the dataset from the followinge link.\nThe â€˜tipsâ€™ dataset contains information about tips given in a restaurant, along with details about the total bill, the gender of the person who paid the bill, whether the customer is a smoker, the day of the week, and the mealâ€™s time.\nColumn\nDescription\ntotal_bill\nThe total amount of the bill (including the tip).\ntip\nThe amount of tip given.\nsex\nThe gender of the bill payer (male or female).\nsmoker\nWhether the customer is a smoker or not.\nday\nThe day of the week when the meal was made.\ntime\nThe time of day (lunch or dinner).\nsize\nThe size of the group that shared the meal.\nBelow is a table with the first 5 rows of the dataset:\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\nğŸŸ¢ Pandera\nNext, we will provide an example of implementing Pandera using the dataset described earlier.\nInstall pandera\npip install pandas pandera \nImplementation Example\nImport pandas and pandera\nimport pandas as pd\nimport pandera as pa\nImport the dataframe file\npath = 'data/tips.csv'\ndata = pd.read_csv(path)\n\nprint(f\"Numero de columnas: {data.shape[1]}, Numero de filas: {data.shape[0]}\")\nprint(f\"Nombre de columnas: {list(data.columns)}\")\ndata.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   total_bill  244 non-null    float64\n 1   tip         244 non-null    float64\n 2   sex         244 non-null    object \n 3   smoker      244 non-null    object \n 4   day         244 non-null    object \n 5   time        244 non-null    object \n 6   size        244 non-null    int64  \ndtypes: float64(2), int64(1), object(4)\nmemory usage: 13.5+ KB\nNow, letâ€™s create the schema object that contains all the validations we want to perform.\nYou can find additional validations that can be performed at the following link: <https://pandera.readthedocs.io/en/stable/dtype_validation.html>\nschema = pa.DataFrameSchema({\n  \"total_bill\": pa.Column(float, checks=pa.Check.le(50)),\n  \"tip\"       : pa.Column(float, checks=pa.Check.between(0,30)),\n  \"sex\"       : pa.Column(str, checks=[pa.Check.isin(['Female','Male'])]),\n  \"smoker\"    : pa.Column(str, checks=[pa.Check.isin(['No','Yes'])]),\n  \"day\"       : pa.Column(str, checks=[pa.Check.isin(['Sun','Sat'])]),\n  \"time\"      : pa.Column(str, checks=[pa.Check.isin(['Dinner','Lunch'])]),\n  \"size\"      : pa.Column(int, checks=[pa.Check.between(1,4)])\n})\nTo capture the error and subsequently analyze the output, it is necessary to catch it using an exception.\ntry:\n    schema(data).validate()\nexcept Exception as e:\n    print(e)\n    error = e\nSchema None: A total of 3 schema errors were found.\n\nError Counts\n------------\n- SchemaErrorReason.SCHEMA_COMPONENT_CHECK: 3\n\nSchema Error Summary\n--------------------\nschema_context column     check                     failure_cases  n_failure_cases\n                                                   \nColumn         day        isin(['Sun', 'Sat'])      [Thur, Fri]             2\n               size       in_range(1, 4)              [5, 6]                2\n               total_bill less_than_or_equal_to(50)   [50.81]               1\nBelow is a function that allows you to transform the output into a dictionary or a pandas dataframe\ndef get_errors(error, dtype_dict=True):\n    response = []\n\n \n    for item in range(len(error.schema_errors)):\n        error_item = error.schema_errors[item]\n        response.append(\n        {\n            'column'     :error_item.schema.name,\n            'check_error':error_item.schema.checks[0].error,\n            'num_cases'  :error_item.failure_cases.index.shape[0],\n            'check_rows' :error_item.failure_cases.to_dict()\n        })\n    \n    if dtype_dict:\n        return response\n    else:\n        return pd.DataFrame(response)\nget_errors(error,dtype_dict=True)\n[{'column': 'total_bill',\n  'check_error': 'less_than_or_equal_to(50)',\n  'num_cases': 1,\n  'check_rows': {'index': {0: 170}, 'failure_case': {0: 50.81}}},\n {'column': 'day',\n  'check_error': \"isin(['Sun', 'Sat'])\",\n  'num_cases': 81,\n  'check_rows': {'index': {0: 77,\n    1: 78,\n    2: 79,\n    3: 80,\n    4: 81,\n    5: 82,\n    6: 83,\n    7: 84,\n...\n    5: 156,\n    6: 185,\n    7: 187,\n    8: 216},\n   'failure_case': {0: 6, 1: 6, 2: 5, 3: 6, 4: 5, 5: 6, 6: 5, 7: 5, 8: 5}}}]\nğŸŸ  Great Expectations\nGreat Expectations is an open-source Python-based library for validating, documenting, and profiling your data.\nIt helps maintain data quality and improve communication about data across teams.\n\nsource : <https://docs.greatexpectations.io/docs/>\nTherefore, we can describe Great Expectations as an open source tool designed to guarantee the quality and reliability of data in various sources, such as databases, tables, files and dataframes.\nIts operation is based on the creation of validation groups that specify the expectations or rules that the data must comply with.\nThe following are the steps that we must define when using this framework:\nDefinition of Expectations: Specify the expectations you have for the data.\nThese expectations can include simple constraints, such as value ranges, or more complex rules about data coherence and quality.\nConnecting to Data Sources: In this step, define the connections you need to make to various data sources, such as databases, tables, files, or dataframes.\nGeneration of Validation Suites: Based on the defined expectations, Great Expectations generates validation suites, which are organized sets of rules to be applied to the data.\nExecution of Validations: Validation suites are applied to the data to verify if they meet the defined expectations.\nThis can be done automatically in a scheduled workflow or interactively as needed.\nGeneration of Analysis and Reports: Great Expectations provides advanced analysis and reporting capabilities.\nThis includes detailed data quality profiles and reports summarizing the overall health of the data based on expectations.\nAlerts and Notifications: If the data does not meet the defined expectations, Great Expectations can generate alerts or notifications, allowing users to take immediate action to address data quality issues.\nTogether, Great Expectations offers a comprehensive solution to ensure data quality over time, facilitating early detection of problems and providing confidence in the integrity and usefulness of data used in analysis and decision-making\nInstall great expectation\n!pip install great_expectations==0.17.22 seaborn matplotlib numpy pandas\nImplementation example\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport re \n\nimport great_expectations as gx\nfrom ruamel.yaml import YAML\nfrom great_expectations.cli.datasource import sanitize_yaml_and_save_datasource\nfrom great_expectations.core.expectation_configuration import ExpectationConfiguration\n\nprint(f\"* great expectations version:{gx.__version__}\")\nprint(f\"* seaborn version:{sns.__version__}\")\nprint(f\"* numpy version:{np.__version__}\")\nprint(f\"* pandas:{pd.__version__}\")\n* great expectations version:0.17.22\n* seaborn version:0.13.0\n* numpy version:1.26.1\n* pandas:2.1.3\nImport dataset using great expectation\npath = 'data/tips.csv'\ndata_gx = gx.read_csv(path)\nList all available expectations by type\nlist_expectations = pd.DataFrame([item for item in dir(data_gx) if item.find('expect_')==0],columns=['expectation'])\nlist_expectations['expectation_type'] = np.select( [\n        list_expectations.expectation.str.find('_table_')>0, \n        list_expectations.expectation.str.find('_column_')>0,  \n        list_expectations.expectation.str.find('_multicolumn_')>0,\n    ],['table','column','multicolumn'],\n    default='other'\n)\n\nplt.figure(figsize=(20,6))\nsns.countplot(x=list_expectations.expectation_type)\nplt.show()\n\n\nIn the image, it can be observed that the available expectations are mainly applied to columns (for example: expect_column_max_to_be_between) and tables (for example: expect_table_columns_to_match_set), although an expectation based on the values of multiple columns can also be applied (for example: expect_multicolumn_values_to_be_unique).\n\nExpectations: Tables\n# The following list contains the columns that the dataframe must have:\ncolumns = ['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size']\ndata_gx.expect_table_columns_to_match_set(column_set = columns)\n{\n  \"success\": true,\n  \"result\": {\n    \"observed_value\": [\n      \"total_bill\",\n      \"tip\",\n      \"sex\",\n      \"smoker\",\n      \"day\",\n      \"time\",\n      \"size\"\n    ]\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\n# Now, we delete two columns, 'time' and 'size,' to validate the outcome\n\ncolumns = = ['total_bill', 'tip', 'sex', 'smoker', 'day']\ndata_gx.expect_table_columns_to_match_set(column_set = columns)\nIf we observe, the result is False, and in the details, they provide information about the columns that the dataframe has in addition to those expected.\n{\n  \"success\": false,\n  \"result\": {\n    \"observed_value\": [\n      \"day\",\n      \"sex\",\n      \"size\",\n      \"smoker\",\n      \"time\",\n      \"tip\",\n      \"total_bill\"\n    ],\n    \"details\": {\n      \"mismatched\": {\n        \"unexpected\": [\n          \"size\",\n          \"time\"\n        ]\n      }\n    }\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\nExpectations: Columns\nLetâ€™s validate that there is a categorical value within a column\ndata_gx['total_bill_group'] = pd.cut(data_gx['total_bill'],\n                              bins=[0,10,20,30,40,50,float('inf')], \n                              labels=['0-10', '10-20', '20-30', '30-40', '40-50', '>50'],\n                              right=False, \n                              include_lowest=True)\n\n# Now, let's validate if 3 categories exist within the dataset\n\ndata_gx.expect_column_distinct_values_to_contain_set(column='total_bill_group',\n                                                      value_set=['0-10','10-20', '20-30'],\n                                                      result_format='BASIC') \n{\n  \"success\": true,\n  \"result\": {\n    \"observed_value\": [\n      \"0-10\",\n      \"10-20\",\n      \"20-30\",\n      \"30-40\",\n      \"40-50\",\n      \">50\"\n    ],\n    \"element_count\": 244,\n    \"missing_count\": null,\n    \"missing_percent\": null\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\nLetâ€™s validate that the column does not have null values\ndata_gx.expect_column_values_to_not_be_null('sex')\n{\n  \"success\": true,\n  \"result\": {\n    \"element_count\": 244,\n    \"unexpected_count\": 0,\n    \"unexpected_percent\": 0.0,\n    \"unexpected_percent_total\": 0.0,\n    \"partial_unexpected_list\": []\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\nGreat Expectation Project\nNow, letâ€™s generate a Great Expectations project to run a group of validations based on one or more datasets.\nInitialize the Great Expectations project:\n !yes Y | great_expectations init\n ___              _     ___                  _        _   _\n / __|_ _ ___ __ _| |_  | __|_ ___ __  ___ __| |_ __ _| |_(_)___ _ _  ___\n| (_ | '_/ -_) _` |  _| | _|\\ \\ / '_ \\/ -_) _|  _/ _` |  _| / _ \\ ' \\(_-<\n \\___|_| \\___\\__,_|\\__| |___/_\\_\\ .__/\\___\\__|\\__\\__,_|\\__|_\\___/_||_/__/\n                                |_|\n             ~ Always know what to expect from your data ~\n\nLet's create a new Data Context to hold your project configuration.\n\nGreat Expectations will create a new directory with the following structure:\n\n    great_expectations\n    |-- great_expectations.yml\n    |-- expectations\n    |-- checkpoints\n    |-- plugins\n    |-- .gitignore\n    |-- uncommitted\n        |-- config_variables.yml\n        |-- data_docs\n        |-- validations\n\nOK to proceed? [Y/n]: \n================================================================================\n\nCongratulations! You are now ready to customize your Great Expectations configuration.\n\nYou can customize your configuration in many ways. Here are some examples:\n\n  Use the CLI to:\n    - Run `great_expectations datasource new` to connect to your data.\n    - Run `great_expectations checkpoint new <checkpoint_name>` to bundle data with Expectation Suite(s) in a Checkpoint for later re-validation.\n    - Run `great_expectations suite --help` to create, edit, list, profile Expectation Suites.\n    - Run `great_expectations docs --help` to build and manage Data Docs sites.\n\n  Edit your configuration in great_expectations.yml to:\n    - Move Stores to the cloud\n    - Add Slack notifications, PagerDuty alerts, etc.\n    - Customize your Data Docs\n\nPlease see our documentation for more configuration options!\nCopy data into the â€˜great_expectationsâ€™ folder generated from the project initialization\n!cp -r data gx\n# Let's print the contents of the folder\n\ndef print_directory_structure(directory_path, indent=0):\n    current_dir = os.path.basename(directory_path)\n    print(\"    |\" + \"    \" * indent + f\"-- {current_dir}\")\n    indent += 1\n    with os.scandir(directory_path) as entries:\n        for entry in entries:\n            if entry.is_dir():\n                print_directory_structure(entry.path, indent)\n            else:\n                print(\"    |\" + \"    \" * indent + f\"-- {entry.name}\")\n\n\nprint_directory_structure('gx')\n    |-- gx\n    |    -- great_expectations.yml\n    |    -- plugins\n    |        -- custom_data_docs\n    |            -- renderers\n    |            -- styles\n    |                -- data_docs_custom_styles.css\n    |            -- views\n    |    -- checkpoints\n    |    -- expectations\n    |        -- .ge_store_backend_id\n    |    -- profilers\n    |    -- .gitignore\n    |    -- data\n    |        -- tips.csv\n    |    -- uncommitted\n    |        -- data_docs\n    |        -- config_variables.yml\n    |        -- validations\n    |            -- .ge_store_backend_id\nHere are some clarifications about the files and folders generated in this directory:\nFiles/Folders\nDescription\nğŸ“„ great_expectations.yml\nThis file contains the main configuration of the project. Details such as storage locations and other configuration parameters are specified here\nğŸ“‚ plugins\ncustom_data_docs:\nğŸ“„renderers: It contains custom renderers for data documents.\nğŸ“„ styles: It includes custom styles for data documents, such as CSS style sheets (data_docs_custom_styles.css).\nğŸ“„ views: It can contain custom views for data documents.\n\nğŸ“‚ checkpoints\nThis folder could contain definitions of checkpoints, which are points in the data flow where specific validations can be performed.\nğŸ“‚ expectations\nThis is where the expectations defined for the data are stored. This directory may contain various subfolders and files, depending on the projectâ€™s organization.\nğŸ“‚ profilers\nIt can contain configurations for data profiles, which are detailed analyses of data statistics.\nğŸ“„ .gitignore\nIt is a Git configuration file that specifies files and folders to be ignored when performing tracking and commit operations. (commit)\nğŸ“‚ data\nIt contains the data used in the project, in this case, the file tips.csv.\nğŸ“‚ uncommitted\nğŸ“‚data_docs: Folder where data documents are generated.\nğŸ“„config_variables.yml: Configuration file that can contain project-specific variables\nğŸ“‚validations: It can contain results of validations performed on the data.\n\nConfiguration of datasource and data connectors:\nDataSource: It is the data source used (can be a file, API, database, etc.).\nData Connectors: These are the connectors that facilitate the connection to data sources and where access credentials, location, etc., should be defined.\n\ndatasource_name_file = 'tips.csv'\ndatasource_name = 'datasource_tips'\ndataconnector_name = 'connector_tips'\n# Let's create the configuration for the datasource\n\ncontext = gx.data_context.DataContext()\nmy_datasource_config = f\"\"\"\n    name: {datasource_name}\n    class_name: Datasource\n    execution_engine:\n      class_name: PandasExecutionEngine\n    data_connectors:\n      {dataconnector_name}:\n        class_name: InferredAssetFilesystemDataConnector\n        base_directory: data\n        default_regex:\n          group_names:\n            - data_asset_name\n          pattern: (.*)\n      default_runtime_data_connector_name:\n        class_name: RuntimeDataConnector\n        assets:\n          my_runtime_asset_name:\n            batch_identifiers:\n              - runtime_batch_identifier_name\n\"\"\"\n\nyaml = YAML()\ncontext.add_datasource(**yaml.load(my_datasource_config))\nsanitize_yaml_and_save_datasource(context, my_datasource_config, overwrite_existing=True)\nConfiguration of the expectations\n\nIn the following code snippet, the configuration of three expectations is presented.\nIn particular, the last one includes a parameter called â€˜mostlyâ€™ with a value of 0.75.\nThis parameter indicates that the expectation can fail in up to 25% of cases, as by default, 100% compliance is expected unless specified otherwise.\nAdditionally, an error message can be specified in markdown format, as shown in the last expectation.\n\nexpectation_configuration_table =  ExpectationConfiguration(\n   expectation_type=\"expect_table_columns_to_match_set\",\n      kwargs= {\n        \"column_set\": ['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size']\n      },\n      meta= {}\n)\n\nexpectation_configuration_total_bill = ExpectationConfiguration(\n      expectation_type= \"expect_column_values_to_be_between\",\n      kwargs= {\n        \"column\": \"total_bill\",\n        \"min_value\": 0,\n        \"max_value\": 100\n      },\n      meta= {}\n)\n\n\nexpectation_configuration_size = ExpectationConfiguration(\n   expectation_type=\"expect_column_values_to_not_be_null\",\n   kwargs={\n      \"column\": \"size\",\n      \"mostly\": 0.75,\n   },\n   meta={\n      \"notes\": {\n         \"format\": \"markdown\",\n         \"content\": \"Expectation to validate column `size` does not have null values.\"\n      }\n   }\n)\nCreation of the expectation suite\nexpectation_suite_name = \"tips_expectation_suite\"\nexpectation_suite = context.create_expectation_suite(\n    expectation_suite_name=expectation_suite_name, \n    overwrite_existing=True\n)\n\n# Add expectations\nexpectation_suite.add_expectation(expectation_configuration=expectation_configuration_table)\nexpectation_suite.add_expectation(expectation_configuration=expectation_configuration_total_bill)\nexpectation_suite.add_expectation(expectation_configuration=expectation_configuration_size)\n\n# save expectation_suite\ncontext.save_expectation_suite(expectation_suite=expectation_suite, \n                               expectation_suite_name=expectation_suite_name)\ndata-quality/gx/expectations/tips_expectation_suite.json\n\nWithin the â€˜expectationsâ€™ folder, a JSON file is created with all the expectations generated earlier.\n\nConfiguration of the checkpoints\ncheckpoint_name ='tips_checkpoint'\n\nconfig_checkpoint = f\"\"\"\n    name: {checkpoint_name}\n    config_version: 1\n    class_name: SimpleCheckpoint\n    expectation_suite_name: {expectation_suite_name}\n    validations:\n      - batch_request:\n          datasource_name: {datasource_name}\n          data_connector_name: {dataconnector_name}\n          data_asset_name: {datasource_name_file}\n          batch_spec_passthrough:\n            reader_method: read_csv\n            reader_options: \n              sep: \",\"\n          data_connector_query:\n            index: -1\n        expectation_suite_name: {expectation_suite_name}\n\"\"\"\n\n# Validate if the YAML structure is correct\ncontext.test_yaml_config(config_checkpoint)\n\n# Add the checkpoint to the generated context\ncontext.add_checkpoint(**yaml.load(config_checkpoint)) \nExecute the checkpoint to validate all the configured expectations on the dataset\nresponse = context.run_checkpoint(checkpoint_name=checkpoint_name)\nTo observe the result obtained from the validations, it can be converted to JSON\n response.to_json_dict()\n{'run_id': {'run_name': None, 'run_time': '2023-11-12T20:39:23.346946+01:00'},\n 'run_results': {'ValidationResultIdentifier::tips_expectation_suite/__none__/20231112T193923.346946Z/722b2e93e32fd7222c8ad9339f3e0e1d': {'validation_result': {'success': True,\n    'results': [{'success': True,\n      'expectation_config': {'expectation_type': 'expect_table_columns_to_match_set',\n       'kwargs': {'column_set': ['total_bill',\n         'tip',\n         'sex',\n         'smoker',\n         'day',\n         'time',\n         'size'],\n        'batch_id': '722b2e93e32fd7222c8ad9339f3e0e1d'},\n       'meta': {}},\n      'result': {'observed_value': ['total_bill',\n        'tip',\n        'sex',\n        'smoker',\n        'day',\n        'time',\n        'size']},\n      'meta': {},\n      'exception_info': {'raised_exception': False,\n       'exception_traceback': None,\n       'exception_message': None}},\n     {'success': True,\n...\n  'notify_on': None,\n  'default_validation_id': None,\n  'site_names': None,\n  'profilers': []},\n 'success': True}\nNow, letâ€™s obtain the results\n context.open_data_docs()\n\nBy executing this code chunk, an HTML file with the results of the validations will open at gx/uncommitted/data_docs/local_site/validations/tips_expectation_suite/__none__/20231112T192529.002401Z/722b2e93e32fd7222c8ad9339f3e0e1d.html\n\n\n\n\nIf you want to learnâ€¦\nPandera DocumentaciÃ³n Oficial\nPandera: Statistical Data Validation of Pandas Dataframes - Researchgate\nGreat Expectation DocumentaciÃ³n Oficial\nData Quality Fundamentals Book Oâ€™relly\nGreat Expectation Yoututbe Channel\nImage preview reference: Image by jcomp on Freepik\n\n\n\n",
    "preview": "posts_en/2023-11-17-data-quality/preview.jpg",
    "last_modified": "2023-12-24T18:39:18+01:00",
    "input_file": {}
  }
]
