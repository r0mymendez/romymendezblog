[
  {
    "path": "posts_en/2024-08-20-my-preparation-journey-for-github-fundamentals-certification/",
    "title": "My Preparation Journey for GitHub Fundamentals Certification",
    "description": "This month, I achieved the GitHub Fundamentals certification, and in this article, I want to share how I did it, what new things I learned, and what I didn’t know before but wish I had known earlier so I could have implemented it sooner.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2024-08-20",
    "categories": [
      "DevOps",
      "GIT",
      "Methodologies"
    ],
    "contents": "\n\nContents\nWhat is the GitHub Platform?\n🟠 GitHub Functionalities & Products\n🟠 GitHub Foundations Learning Path\nPractices with GitHub Actions\nInteractive Console\nKnowledge Check\n\n🟠 Overview of Units\n🟠 Code Management\n📁 Repository\n🔄 Pull Requests\n🔧 Issues\n💬 Discussions\n\n📁 Gists\n🟠 Accounts\n\n🟠 Methodologies\n🟠 Communication\n📝 Markdown\n\n🟠 Products\n🚀 GitHub Copilot\n🚀 GitHub Codespaces\n\n🟠 Project\n📋 View\n📊 Insights\n🤖 Task Automation\n\n🟠 Security\n🔐 Authentication & Access Controls\n⚠️ Vulnerability Alerts\n📊 Reporting\n🚀 Best Practices\n\n🟠 Types of Search on GitHub\nSearch Examples\n\n🟠 Summary\n📚 References\n\n\n\nThis month, I achieved the “GitHub Fundamentals” certification, and in this article, I want to share how I did it, what new things I learned, and what I didn’t know before but wish I had known earlier so I could have implemented it sooner.\nWhat is the GitHub Platform?\nThe GitHub platform is a cloud-based solution built on Git, a version control system.\nIt facilitates collaboration and project management through a range of features and AI-powered tools, enabling efficient software development, while ensuring quality and secure deployment.\n\nSome features we can highlight are as follows:\n🤖 AI: GitHub uses products like Copilot, which will be described later, to help develop and solve problems and perform automated security reviews. This enhances collaboration, productivity, and security.\n🤝 Collaboration: GitHub facilitates efficient collaboration with tools such as Repositories, Issues, and Pull Requests. These features allow multidisciplinary teams to work together quickly, reduce approval times, and accelerate project delivery.\n⚙️ Productivity: Automation through GitHub Actions enables developers to focus on creating solutions and avoid repetitive manual tasks.\n🔒 Security: GitHub includes features like Dependabot and automated security reviews, ensuring that code remains private and protected within the organization.\n🌍 Scalability: GitHub boasts the largest developer community in the world, with over 100 million developers and more than 330 million repositories.\n🔗 Integration: Additionally, GitHub offers an API that allows for integrations and customization of workflows, as well as automation of tasks.\n🟠 GitHub Functionalities & Products\nAs mentioned initially, this post is intended to discuss what you need to know in order to pass the certification exam.\nBelow, we have an image that represents each of the described entities and the topics you should study to successfully pass the certification exam, which consists of ✏️ 75 multiple-choice questions.\nThis mental map represents how I organized the key concepts needed to prepare for the certification, following the GitHub Foundation Learning Path, which I will explain next.\n\n🟠 GitHub Foundations Learning Path\nIn order to effectively study and understand all the essential content for the certification, I completed the ‘GitHub Foundations Learning Path’ provided by Microsoft Learn.\nThis path consists of 15 modules, totaling approximately ⌛ 10 hours, with each module averaging about ⌛ 35 minutes.\nThe journey combines 📚 theoretical knowledge with ✏️hands-on exercises, enabling me to effectively learn and apply the concepts.\nPractices with GitHub Actions\nThe modules include ✏️ hands-on exercises solved in stages, using GitHub Actions to validate each step of the process.\nThis inspired me to develop similar tutorials for future posts.\nInteractive Console\nSome modules, such as the first one, offer an interactive console that facilitates executing typical Git commands, further enhancing the learning experience.\nKnowledge Check\nEach unit includes a set of ✏️questions to validate what has been learned, similar to those found on the exam.\n> 🙌 I recommend completing and reviewing these questions to consolidate the knowledge acquired.\n🟠 Overview of Units\nWith the foundational concepts established, I will now delve into a detailed description of each unit.\nBelow, you will find explanations of the key elements and tools I studied, and how I organized them to effectively prepare for the certification.\n🟠 Code Management\nCode management on GitHub is essential for productivity and effective collaboration within development teams.\nBelow are the key tools that facilitate this management:\n\n📁 Repository\nThe repository is the core component of a project on GitHub, which is where the source code, along with its version history, documentation, and other relevant resources for the project, is stored.\n🔄 Pull Requests\nPull Requests (PRs) are a fundamental tool for collaboration on GitHub.\nThey allow developers to propose changes to the source code, which can then be reviewed and discussed by other team members before being merged into the base code.\nPull Request States It is important to understand the possible states of a Pull Request (PR) on GitHub, which can be:\nDraft: PRs in draft status are still in development and not yet ready for formal review.\nThis state allows collaborators to work on a proposal before requesting official feedback.\nOpen: Open PRs are ready and available for review and discussion by project collaborators.\nThis is where proposed changes are evaluated before deciding whether to integrate them into the base code.\nClosed: Closed PRs have not been accepted and therefore will not be merged into the project’s base code.\nThis state may indicate that the proposal was rejected or that the author decided not to proceed with it.\nMerged: Merged PRs have been accepted, and the proposed changes have been successfully integrated with the main branch of the project.\nThis is the final state of a successful PR.\n🔧 Issues\nIssues are tracking tickets used to report bugs, suggest improvements, or discuss new features.\nThey are essential for planning and organizing work within a project, allowing teams to prioritize tasks and track progress.\n💬 Discussions\nDiscussions on GitHub provide a space where collaborators can engage in broader conversations about the project.\nThese discussions can be organized into several categories:\n📢 Announcements: For communicating important updates about the project.\n💬 General: For general conversations that don’t fit into any other category.\n💡 Ideas: For proposing and debating new ideas or improvements.\n🗳️ Polls: For conducting surveys and gathering community opinions.\n🙏 Q&A: For questions and answers about the project.\n🙌 Show and Tell: For sharing achievements, demos, or use cases of the project.\n📁 Gists\nGists are a simple way to share snippets of code or text.\nThey can be public, accessible to anyone, or private, visible only to the creator or those with whom the link is shared.\nGists are useful for quickly sharing code examples, notes, or scripts among collaborators or with the broader community.\n🟠 Accounts\nThis is one of the topics where you may encounter questions, and within the recommended learning path, you can find three types of accounts:\n👥Personal Account: This is the basic account for individual users, allowing them to own and manage repositories, packages, and projects.\n🏢 Organization Account: Allows multiple users to collaborate on shared projects.\n💼 Enterprise Account: Designed for large companies, it enables centralized management of policies and billing for multiple organizations.\n\nAdditionally, the existing plans are:\nGitHub Free: Provides the essentials for individuals and organizations, with limited storage and GitHub Actions minutes.\nGitHub Pro: Extends the features of GitHub Free with more GitHub Actions minutes, storage, and advanced tools for private repositories, ideal for individual developers.\nGitHub Team: Similar to GitHub Pro but optimized for organizations, adding advanced team collaboration capabilities.\nGitHub Enterprise: Offers premium support, enhanced security, compliance, and advanced authentication options, along with significantly increased resources and storage.\nThere are two versions of this account type:\nCloud (hosted by GitHub)\nServer (self-managed)\n\n🟠 Methodologies\nI’ve titled this section “Methodologies” because throughout different chapters of the learning path, I encountered several methodologies that you need to learn.\nTherefore, I decided to group them all under the same concept.\n\n⚠️ Remember that this is my approach to studying for the certification, and you might find that the learning path or other resources refer to these methodologies differently.\n\n\n🔀 GitHub Flow: A workflow that utilizes specific components (branches, commits, and pull requests) to collaborate and experiment with new features or fixes in a software project.\n🔄 InnerSource: Involves adopting open-source practices and principles, such as transparency and collaboration, within an organization.\nThis means projects are shared internally, allowing any team member to access, modify, and contribute to the code, similar to an open-source project but within the organization.\n🛠️ Contributing to Projects: I consider this a methodology because GitHub enables active participation in open-source projects through its platform.\nThis practice is not only fundamental for collaborative software development but also promotes learning, innovation, and community building among developers.\nKey considerations include:\n📢 Identification and Communication: Before making changes, it’s important to communicate your intentions with the project’s maintainers, whether you are addressing an existing issue or proposing a new feature.\n🔄 Creating a Pull Request: Once changes are made, a pull request is created on GitHub, allowing others to review, discuss, and eventually merge it with the main codebase.\n🔎Review and Adjustments: The pull request is reviewed by the project maintainers, who may request adjustments. It’s essential to be open to feedback and willing to make necessary changes for your contribution to be accepted.\n👥 Iteration and Ongoing Collaboration: The review and discussion process around a pull request is iterative and collaborative, allowing for improved software quality and ensuring each contribution aligns with the project’s standards and goals.\n\n❤️ Supporting Projects Financially: In addition to contributing code and feedback, you can also support open-source projects financially.\nFor this reason, GitHub Sponsors allows you to fund projects and individuals to help them continue their open-source work, while giving them the recognition they deserve.\nTo support a project:\nIdentify Projects: Explore and select projects you are passionate about.\nChoose a Sponsorship Level: Pick a tier that fits your budget and desired level of support, with varying benefits such as exclusive content or early access to new features.\nSupport and Recognition: Find the Sponsor button on a project’s main page if it’s eligible for sponsorship. You can select the sponsorship tier and decide if you want your contribution to be public.\n\n\n\nsource: Identify where you can help\nsource: Identify where you can help\n🟠 Communication\nCommunication within a repository is essential and encompasses the detailed documentation of objectives, changes, methodologies, and rules to be applied.\nBelow are the main tools and practices that facilitate this communication:\n📄 README File: The README file is the first line of communication in a project.\nIt should contain essential information about the project’s purpose, how to use it, how to set it up, and any other relevant details that users and collaborators need to know.\n📚 Wiki: This section is designed to host more extensive and detailed documentation.\nUnlike the README, which is brief and direct, the Wiki is ideal for sharing content such as use cases, examples, design principles, technical details, and any other aspects that require a more in-depth explanation.\n\n📝 Markdown\nMarkdown is a markup language that provides a simple and efficient way to format and style content in issues, README files, wikis, and any other textual documentation within a repository.\nIf you’re not familiar with Markdown, I recommend checking out the DataCamp CheatSheet, which summarizes everything you need to get started.\nAdditionally, within the chapter, there’s an exercise titled “Exercise - Communicate using Markdown” that will allow you to apply and reinforce your practice using markdown.\nsource: DataCamp CheatSheet🟠 Products\nI have titled this section “Products” as it includes two chapters dedicated to two key products.\nOne of these products leverages AI, while the other focuses on enhancing productivity.\n\n🚀 GitHub Copilot\nGitHub Copilot is an AI-powered assistance tool for developers.\nIts main features include:\n💬 GitHub Copilot Chat: Provides a chat within the editor for code analysis, generating unit tests, and fixing bugs.\n🔄 Copilot for Pull Requests: Automatically adds labels to pull request descriptions using AI to facilitate review.\n📄 AI-Generated Documentation Answers: Delivers AI-generated responses to questions about documentation and technologies.\n⚙️ Copilot for CLI: Assists in creating complex commands and loops in the command line.\n🚀 GitHub Codespaces\nGitHub Codespaces provides configurable development environments in the cloud, built on Docker container.\nTherefore every each Codespace operates within a 🐋 Docker container hosted on GitHub.\nAlso, this container includes all the necessary tools and settings for your project, ensuring a consistent and reproducible environment for all project contributors.\nHere’s an overview of the key functionalities and states of a Codespace:\n🛠️ Creating Codespaces: You can create a Codespace from templates, repository branches, open pull requests, or historical commits. By default, each Codespace is based on an Ubuntu Linux image, but you can customize it with any Linux distribution to meet your specific needs. This provides a complete development environment running inside a Docker container on a virtual machine. You can create multiple Codespaces per repository or branch, though there is a limit on the number of simultaneous Codespaces.\n📥 Saving Changes: Changes made within a Codespace are automatically saved when connected through the web. To avoid losing work if the Codespace is deleted, it is crucial to commit and push changes to a remote repository.\n🔄 Reopening and Using Codespaces: You can reopen active or stopped Codespaces from GitHub.com, Visual Studio Code, JetBrains IDE, or GitHub CLI. When connected, you access the Docker container of the Codespace, ensuring a consistent development environment accessible from anywhere.\n⏱️ Timeout and Internet Connection: Codespaces automatically stop after 30 minutes of inactivity. They require an internet connection, and any uncommitted changes are saved to prevent data loss.\n🔧 Closing, Stopping, or Rebuilding a Codespace: You can stop or restart a Codespace to apply configuration changes or troubleshoot issues. You can also rebuild the Codespace to update the Docker container configuration.\n🗑️ Deleting a Codespace: You can delete a Codespace after committing your changes. Inactive Codespaces are automatically deleted after 30 days, though this period can be adjusted. Using Docker containers ensures that your development environment is consistent and reproducible, making it easier to collaborate and maintain projects.\n🟠 Project\nGitHub has introduced new types of projects that significantly enhance productivity and project management.\nWhile previously existing projects focused primarily on code management, the new GitHub Projects integrate key elements that allow you to manage not only the code but also the broader aspects of project management within a single tool.\nThis new approach aims to streamline workflows, provide more comprehensive tracking, and improve collaboration.\nIt’s important to note that there may be questions about the differences between the previous project types and these new ones.\nIn the following sections, I’ll highlight the functionalities and benefits of the new project types, helping you understand how they differ from the old ones and how they can be utilized to optimize project management.\n\n📋 View\nIn GitHub Projects, tasks are represented as issues and can include key details such as title, description, assignee, start date, and complexity.\nThese issues can be customized with various field types like text, number, date, single selection (for dropdown lists), or iteration to better suit your needs.\nThe types of views that can be generated are:\n📄 Table: A tabular view that organizes issues in a table format. This view is primarily used to view and manage detailed information on multiple issues simultaneously, facilitating comparison and bulk editing of data.\n\n📋 Board: A card-based view where issues are organized into columns, typically by status or category. This view is primarily used to visualize the workflow of issues through different stages, aiding task management and progress tracking.\n\n⌛ Roadmap: A graphical view that shows a timeline of issues and the associated milestones. This view is primarily used for planning and visualizing the long-term evolution of projects, helping to identify dependencies and delivery schedules.\n\n📊 Insights\nInsights in Projects allows you to visualize, create, and customize charts using the elements added to your Project as data sources.\nWhen creating a chart, you can define filters, the type of chart, and the information to be displayed.\nThe generated charts are available to anyone with access to the Project.\nThese charts enable you to analyze and manage the project, enhancing your ability to plan, control, and assess the project’s progress.\n\n🤖 Task Automation\nTask automation on GitHub is important for improving efficiency and consistency in project management.\nThe following are the key tools that facilitate this automation:\nWorkflows: Workflows are the simplest way to automate project management on GitHub. These workflows allow you to automate repetitive processes such as updating task statuses, running tests, deployments, and other actions defined in YAML files. Integrated workflows in GitHub Projects streamline automation without the need for complex configurations.\nGraphQL API: The GraphQL API offers more granular control over project automation. With GraphQL, you can customize how you interact with your project data, enabling advanced automations tailored to your team’s specific needs.\nGitHub Actions: GitHub Actions provides a powerful platform for further customizing automation in your projects. With GitHub Actions, you can create pipelines that respond to specific events, such as the creation of issues or changes in task statuses, ensuring that all activities in your project are executed consistently and automatically.\n🟠 Security\nSecurity is a fundamental aspect within GitHub, which provides a range of tools and best practices to enhance security in software development.\n\n🔐 Authentication & Access Controls\nAccess to your GitHub account can be enhanced with advanced authentication methods that validate user identity more securely.\n* SAML SSO Authentication: Allows user authentication through a centralized Identity Provider (IdP), enhancing security and access control.\n* Multifactor Authentication (2FA): Adds an extra layer of security through two-step authentication, which may include physical security keys and time-based one-time password (TOTP) applications.\n⚠️ Vulnerability Alerts\nGitHub offers vulnerability alerts to help you identify and address security issues in your repositories.\nDependabot Alerts: Detects and notifies you about vulnerable dependencies in your project, helping you keep your libraries and packages up to date.\n > source:Automated security\nCode Scanning: Analyzes your code for vulnerabilities and errors. It can be customized using CodeQL, allowing you to detect and fix security issues before they become risks. Integration with other tools like SonarQube or Policy Validator for Terraform by Amazon Web Services is also possible.\nSecret Scanning: Automatically scans the repository for exposed credentials or secrets, helping to prevent misuse of sensitive information.\n📊 Reporting\nSecurity Policy: The security policy for your project should be documented in a SECURITY.md file, detailing how to responsibly report vulnerabilities.\n🚀 Best Practices\nKeep Sensitive Information Secure: It’s crucial to protect sensitive information by ensuring it is not included in commits. Use .gitignore to exclude critical files and carefully review changes before committing them.\nBranch Protection Rules: Set up protection rules for key branches like master or develop, requiring approved reviews before allowing merges. This ensures that only secure and reviewed code is integrated into the main branches.\n🟠 Types of Search on GitHub\nOn GitHub, you can use advanced search to efficiently navigate repositories, find issues, review pull requests, and join discussions.\nHere are some practical examples to enhance your search capabilities and collaboration on projects:\n\nSearch Examples\n🔎 Repositories\nDescription: Finds repositories related to “machine learning” with more than 1000 stars.\nUse: Ideal for locating popular projects in a specific area of interest.\ntopic:machine-learning stars:>1000\n🔎 Issues in a Specific Repository\nDescription: Searches for closed issues within the r0mymendez/datapp repository.\nUse: Useful for reviewing the history of resolved issues in a particular repository.\nrepo:r0mymendez/datapp is:issue is:closed\n🔎 Issues with Labels and Status\nDescription: Finds open issues tagged with “help-wanted” in projects using Python.\nUse: Perfect for finding opportunities to contribute to Python projects where maintainers are seeking help.\nis:issue language:python is:open label:help-wanted\n🔎 Pull Requests\nDescription: Shows all pull requests where a review has been requested from you.\nUse: Ideal for managing your code review responsibilities.\nis:pr review-requested:@m\n🔎 Discussions\nDescription: Finds open discussions in repositories related to the Python language.\nUse: Useful for participating in relevant conversations or searching for open topics in a specific programming language.\nin:discussions is:open language:python\nIssues with Specific Combinations\nDescription: Searches for open issues in repositories related to “machine learning” with more than 1000 stars, tagged with “help-wanted,” and using Python.\nUse: Ideal for finding open issues in popular machine learning projects that are seeking help in Python.\ntopic:machine-learning stars:>1000 is:issue label:help-wanted language:python is:open\n🟠 Summary\n\n📚 References\nThe following references were used to create this article, with the first being the primary resource and the other materials were utilized to practice and simulate the exam.\nGitHub Foundations Learning Path: GitHub Foundations Learning Path\nGitHub Foundations Practice Test: Contains a series of multiple-choice questions for exam simulation. GitHub Foundations Practice Test\nStudy Guide GitHub Foundations: A PDF guide for studying and preparing for the exam. Study Guide GitHub Foundations\nGitHub Foundations Certification – Exam Prep Guide: This is a post in FreeCodeCamp blog and it is content very helpful, and this post provides a good summary of everything you should know for exam preparation. GitHub Foundations Certification – Exam Prep Guide\n\n\n\n\n",
    "preview": "posts_en/2024-08-20-my-preparation-journey-for-github-fundamentals-certification/preview.png",
    "last_modified": "2025-01-02T02:47:05+01:00",
    "input_file": "my-preparation-journey-for-github-fundamentals-certification.knit.md",
    "preview_width": 3386,
    "preview_height": 1301
  },
  {
    "path": "posts_en/2024-08-07-employing-aws-comprehend-medical-for-medical-data-extraction-in-healthcare-analytics/",
    "title": "Employing AWS Comprehend Medical for Medical Data Extraction in Healthcare Analytics",
    "description": "The goal of this tutorial is to provide a guide on how to use Amazon Comprehend Medical for identifying medical entities and extracting information, including RxNorm codes, SNOMED CT concepts, and other attributes.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2024-08-07",
    "categories": [
      "Python",
      "Data",
      "Cloud Computing",
      "AI"
    ],
    "contents": "\n\nContents\n🩺 Amazon Comprehend Medical\n🛡️ What is HIPAA?\n📚 What are the vocabularies?\n🩺 AWS Comprehend Medical: Methods for Data Extraction\n🩺 AWS Comprehend Medical: Detect Entities\n🩺 AWS Comprehend Medical: RxNorm\n🩺 AWS Comprehend Medical: SNOMED CT (Clinical Terms)\n\n🩺 Dataset\n🔧 Prerequisites\n🟣 Data Extraction and Preparation\n🟣 Real-Time Processing with AWS Comprehend Medical\n🟣 Batch Processing with AWS Comprehend Medical\n\n📚 References\n📚 Other references:\n\n\n\nA Step-by-Step Guide to Using Entities, RxNorm and SNOMED CT\nThe goal of this tutorial is to provide a guide on how to use Amazon Comprehend Medical for identifying medical entities and extracting information, including RxNorm codes, SNOMED CT concepts, and other attributes.\nWe will cover the following key topics:\n🏷️ Dataset: We will use a dataset from Kaggle related to the USMLE® Step 2 Clinical Skills examination.\n🏷️ Data Extraction and Preparation: Preparation of the dataset to extract entities.\n🏷️ Real-Time Processing with AWS Comprehend Medical: Explore how to use Amazon Comprehend Medical in real-time to analyze and extract medical information from unstructured text data.\n🏷️ Batch Processing with AWS Comprehend Medical: Discover how to set up and execute batch processing jobs with Amazon Comprehend Medical.\n🩺 Amazon Comprehend Medical\nAmazon Comprehend Medical is a service designed to extract information from unstructured medical texts using natural language processing model (NLP) while ensuring compliance with HIPAA requirements.\nThis service provide the following outputs: * Entities: Key medical elements identified in the text, such as medications, diagnoses, symptoms, and procedures.\n* RxNorm Codes: These codes are derived from a medical ontology that provides normalized names for medications and drugs, ensuring consistent identification and categorization of medication-related information.\n* SNOMED CT: This code set originates from a comprehensive medical ontology that represents clinical concepts such as diseases, procedures, and diagnoses, facilitating precise and interoperable health data.\n\nAt the time of writing this article, only English texts can be processed usign this service.\n\n\n🛡️ What is HIPAA?\nThe HIPAA (Health Insurance Portability and Accountability Act) privacy rule sets national standards for the protection of individually identifiable health information in the United States.\nThis refers to data, including demographic information, that relates to: * The individual’s past, present, or future physical or mental health or condition.\n* The provision of healthcare to the individual.\n* The past, present, or future payment for healthcare provided to the individual, and that identifies the individual or can reasonably be used to identify them, where this includes common identifiers such as name, address, date of birth, and Social Security number.\n📚 What are the vocabularies?\n“Vocabularies” refer to structured sets of standardized terms and codes used to capture, classify, and analyze patient data.\nThese include controlled vocabularies, terminologies, hierarchies, and ontologies, and are essential for interoperability between healthcare systems, enabling data exchange and facilitating global research.\nThis practice dates back to the 1660s, as shown in the image below.\n\n“Medical vocabularies date back to the Bills of Mortality in medieval London to manage outbreaks of plague and other diseases.” The Book Of Ohdsi\n\n Image source: The Book Of Ohdsi\n🩺 AWS Comprehend Medical: Methods for Data Extraction\nAfter understanding the importance of medical vocabularies, we can explore how AWS Comprehend Medical leverages these vocabularies to extract and standardize medical data.\nIn the following sections, we will describe the specific methods used by AWS Comprehend Medical to process and analyze medical texts.\n🩺 AWS Comprehend Medical: Detect Entities\nThe detect_entities_v2 method from AWS Comprehend Medical identifies and classifies various categories of medical information within a text.\nBelow is an image illustrating the categories detected by this method.\n\nFor each of these classes, not only are the categories to which the entity belongs detected, but also other key values.\nThese values include:\nType: The specific type of entity within a category.\nAttribute: Relevant information about the entity, such as the dosage of a medication.\nTrait: Additional aspects that Amazon Comprehend Medical understands about an entity based on context, such as the NEGATION trait if a medication is not being administered to the patient.\nBelow, you can see the additional data that can be obtained for each category.\n\n\n\n\n\n\n\n🩺 AWS Comprehend Medical: RxNorm\nRxNorm is a standardized medical ontology that provides normalized names for clinical medications, and also It serves as a comprehensive resource for identifying and categorizing drugs and their various forms.\nRxNorm links these standardized names to many other drug vocabularies, ensuring consistency and interoperability across different healthcare systems.\nBelow is an example with a medication and the related concepts in RxNorm.\n\n🩺 AWS Comprehend Medical: SNOMED CT (Clinical Terms)\nSNOMED CT (Systematized Nomenclature of Medicine – Clinical Terms) is a comprehensive multilingual health terminology system.\nIt provides a standardized set of codes, concepts, and synonyms to represent clinical information, including diseases, procedures, and diagnoses.\n\nSNOMED CT facilitates semantic interoperability by allowing mapping between different health vocabularies, such as ICD-9 and ICD-10.\n\n\n\n🩺 Dataset\nFor this tutorial, we will use a dataset from Kaggle that is associated with the USMLE® Step 2 Clinical Skills examination, this licensing exam evaluates the examinee’s ability to recognize pertinent clinical facts during interactions with standardized patients.\nWe will use select medical notes from this dataset to process and analyze the results obtained using the AWS Comprehend Medical service.\n\nsource: NBME - Score Clinical Patient Notes\n🔧 Prerequisites\nTo complete this tutorial, you need to meet the following prerequisites:\nAWS Credentials: You must configure the AWS_ACCESS_KEY and AWS_SECRET_KEY credentials. These are crucial for authenticating and authorizing access to AWS services.\nS3 Bucket: Create an S3 bucket to store your data. In this example, we will use a bucket named dev-medical-notes located in the us-east-1 region.\nPermissions: Check the IAM folder in this repository for the necessary policies and permissions to apply.\n\n⚠️ If you’re not familiar with creating AWS credentials or setting up an S3 bucket, you can follow this guide: Create a Bucket.\n\n🔐 AWS Credentials\nFor this tutorial, AWS credentials (AWS_ACCESS_KEY and AWS_SECRET_KEY) are required, these credentials are essential for authenticating and authorizing access to AWS services and they can be generated using the IAM (Identity and Access Management) service.\n\n⚠️ Remember to keep your credentials secure and avoid sharing them to prevent unauthorized access to your AWS account.\n\n🔐 IAM Policies and Role\nFor this tutorial, you need a role and a user with specific policies applied.\nIn the GitHub repository, you’ll find a folder containing the policies that need to be applied.\n📦 AWS Libraries\nThe main libraries we will use are:\nboto3: This library allows us to connect programmatically to Amazon Web Services (AWS) services.\nawswrangler: This open-source Python library integrates pandas with AWS, enabling seamless data manipulation and analysis within AWS services.\n🔧 Configuration Setup\nTo manage AWS credentials, we will use the python-dotenv library to handle environment variables.\nYou need to create a file named .env in the root of the project and configure your AWS credentials there.\nBelow, you will find the format for the file.\n\nFile Name: .env\n\nAWS_SECRET_KEY='mySecretKey'\nAWS_ACCESS_KEY='myAccessKey'\nAWS_ROLE='arn:aws:iam::xxxx:role/role-name'\n⚠️ Considerations\nTo simplify this tutorial and reduce the complexity of implementing a solution, two classes were created, which are as follows:\n📦S3bucket Class\nTo simplify the explanation of this tutorial and manage the files stored in an S3 bucket, I have created a class named S3bucket.\nThis class will enable us to perform various common operations such as listing the files in a bucket, writing a JSON file, writing a Parquet file, and reading a JSON file.\n📦ComprehendMedical Class\nTo make it easier to use AWS Comprehend Medical and create DataFrames from the processed data, I have developed a class named ComprehendMedical.\nThis class is designed to streamline interactions with the service’s methods, including detect_entities_v2, infer_rx_norm, and infer_snomed_ct.\nBelow are the primary methods of this class and their functionalities:\nget_entities: This method uses the detect_entities_v2 function from AWS Comprehend Medical to identify medical entities in a given text.\nget_rxnorm: This method employs the infer_rx_norm function to extract medication-related information from the text.\nget_snomed: This method uses the infer_snomedct function to identify and obtain information related to standardized medical terms in the SNOMED CT system.\n\n⚒️ Methods to Generate DataFrames Each of the above methods also has a version that returns the results in DataFrame format using pandas.\nThese DataFrames are then saved in Parquet format, which is efficient for storage and querying, and facilitates integration with other data processing tools.\nThe Parquet files are stored in a new 📁 folder named “stage” within the same Amazon S3 bucket.\n\n# libraries for data processing\nimport json, os,io,re,uuid\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom pprint import pprint\nimport pandas as pd \nimport numpy as np\n\n# libraries for loading environment variables\nfrom dotenv import load_dotenv\n\n# aws libraries\nimport boto3\nimport awswrangler as wr\n\n# libraries for data visualization\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport matplotlib as mpl\n\nload_dotenv()\nAWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\nAWS_SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")\nAWS_ROLE = os.getenv(\"AWS_ROLE\")\nAWS_REGION_NAME = 'us-east-1'\nBUCKET_NAME = 'dev-medical-notes'\n\nHere we are creating an object from the class called S3bucket and you can see the complete code in the following link of this class\n\n# create an object of the class S3bucket for the bucket 'dev-medical-notes'\ns3 = S3bucket(BUCKET_NAME, AWS_ACCESS_KEY, AWS_SECRET_KEY, AWS_REGION_NAME)\n🟣 Data Extraction and Preparation\nThe first step in the process is to extract a subset of medical notes and upload them to Amazon S3 in JSON {} format.\nTo facilitate the organization and management of these files, they will be stored in a 📁 folder named “raw,” which will be preceded by a 📅 date prefix (dt).\nThe 📁 “raw” folder will serve as the container for the original, unprocessed files, while the date prefix will help classify and manage the files based on when they were uploaded.\ndata = pd.read_csv('data/patient_notes.csv')\ndata['pn_history_len']=data['pn_history'].str.len()\n\n# Plotting the distribution of the number of characters in patient notes\nmpl.rcParams['font.family'] = 'serif'\nplt.figure(figsize=(12,4))\nsns.histplot(data['pn_history_len'], color=sns.color_palette('pastel')[4])\nplt.title('Distribution number of characters in patient notes')\nplt.xlabel('Number of characters')\nplt.ylabel('Frequency')\nplt.show()\n\n🔍 Analysis of Clinical Note Lengths In summary, there is significant variability in the length of clinical notes.\nHowever, most notes typically fall within a certain range.\nBelow are the key points and future considerations for this analysis:\n📊 Distribution of Note Lengths\nThe majority of notes are between 800 and 1000 characters.\nSome notes are shorter, with less than 200 characters.\n\n🧹 Data Cleaning\nIt is important to ensure that the notes do not contain unnecessary characters, such as repetitions or sequences of special symbols. It is recommended to perform a preliminary cleaning of the notes to remove these characters before processing.\nIt is recommended to perform a preliminary cleaning of the notes to remove these characters before processing.\n\n🔍 Future Research Questions  Although some of these questions cannot be answered with our dataset, these are some questions we could consider for analyzing a similar dataset:\nType of Patients: What types of patients have the shortest and longest notes?\nRelation to Severity: Is there a relationship between the length of the note and the severity of the patient's condition?\nTemporal Evolution: How has the average length of the notes changed over time?\n\nAdditionally, since AWS Comprehend Medical processes notes up to 10,000 characters, performing this analysis is ideal for optimizing the usage of this service.\nSelecting Random Notes\n# seelcting random notes to test the function\nrandom_notes = [42141, 39049, 40593, 38851, 41068, 39457, 39152, 39665, 37830, 41717]\n\n# selecting the notes from the data\ndata_test = data.reset_index().rename({'index':'id'},axis=1).loc[random_notes,:].to_dict('records')\n# see the first 3 notes\ndata_test[:3]\n[{'id': 42141,\n  'pn_num': 95330,\n  'case_num': 9,\n  'pn_history': 'Ms. Madden is a 20 yo female presenting w/ the worst HA of her life, unlike anything that she has had before. It is a dull + constant pain, it has gotten progressively worse, started yesterday morn. It is a diffuse pain felt around her head and is nonpulsating. She has photophobia but no phonophobia, has nausea, and vomited 3x yesterday. No sick contacts. Felt warm earlier. No chills, fatigue, CP, SOB, abd pain, or rashes. No sx before the onset of this HA. Ibuprofen, tylenol, sleep have not helped. Walking + bending over makes the pain worse. She has had HA before once or twice a yr but they are usually very mild. \\r\\nMeds: OCPs\\r\\nFH: mother w/ migraines, dad w/ HPL\\r\\nSocial alcohol use, 3 or 4 marijuana joints per week, no tobacco use\\r\\nPMH: none significant',\n  'pn_history_len': 765},\n {'id': 39049,\n  'pn_num': 92131,\n  'case_num': 9,\n  'pn_history': '20 yo F, c/o headaches.\\r\\n- started yesterday, right after she woke up. 8/10, dull, constant headache, getting worse. \\r\\n- nothing makes it better. exacerbated by leaning forward and walking.\\r\\n- nausea and vomiting. vomited 3 times, green fluids, no blood. \\r\\n- photophobia. \\r\\n- mild fever. \\r\\nROS: none except above. occasional headaches.\\r\\nPMH: none. Meds: OCP. All: NKDA.\\r\\nPSH: none. \\r\\nFH: mother - migrain. father-  hyperlipidemia. \\r\\nSH: sexually active with boyfriend, using condoms and OCP. \\r\\nnot smoking, drinkes 2-3 a week. smoking marijuana 3-4 times a week.',\n  'pn_history_len': 562},\n {'id': 40593,\n  'pn_num': 93723,\n  'case_num': 9,\n  'pn_history': 'A 20 yo female presents to the clinic with c/o a headache since yesterday. Pt states headache is constant, progressive, and diffuse. Associaed with nausea, vomiting , and decreased appetite. Pain is 10/10 in intensity, non-radiating. Patient states she also felt warmer yesterday. Pt states muscle aches and runny nose.. Patient denies cough, chst pain, adbominal pain. Pt denies recent travel.\\r\\n\\r\\nAllergies: none\\r\\nmeds: OCPs\\r\\nPMHx:/PShx/hopsi: none\\r\\nFamily hx:amily hx (mother-migraine headache)\\r\\nsocial hx: \\r\\n',\n  'pn_history_len': 511}]\n# write the data to the s3 bucket\nfor record in tqdm(data_test):\n    dt =f\"dt={datetime.now().strftime('%Y%m%d')}\"\n    record_file_name = f\"medical_record_noteId_{record['id']}.json\"\n    s3.write_s3_json(data=record, filename=f\"raw/{dt}/{record_file_name}\")\n100%|██████████| 10/10 [00:02<00:00,  3.48it/s]\n\n🧹 Retrieval and Cleaning of Notes We will retrieve note 42141 from the S3 bucket, specifically from the folder “raw”.\nUsing these data, we will use the re module to replace the characters   and , which correspond to line breaks and tabs.\n Next, we will review the dictionary with the note and proceed to modify these characters in the retrieved text.\n\n# Read the data from the s3 bucket \nnote_id = 42141\nnote = s3.read_s3_json(f'raw/dt=20240804/medical_record_noteId_{note_id}.json')\npprint(note)\nnote_clean = re.sub(r'[\\n\\r\\t]', ' ',note['pn_history'])\n{'case_num': 9,\n 'id': 42141,\n 'pn_history': 'Ms. Madden is a 20 yo female presenting w/ the worst HA of her life, unlike anything that she has had before. It is a dull + constant pain, it has gotten progressively worse, started yesterday morn. It is a diffuse pain felt around her head and is nonpulsating. She has photophobia but no phonophobia, has nausea, and vomited 3x yesterday. No sick contacts. Felt warm earlier. No chills, fatigue, CP, SOB, abd pain, or rashes. No sx before the onset of this HA. Ibuprofen, tylenol, sleep have not helped. Walking + bending over makes the pain worse. She has had HA before once or twice a yr but they are usually very mild.   Meds: OCPs  FH: mother w/ migraines, dad w/ HPL  Social alcohol use, 3 or 4 marijuana joints per week, no tobacco use  PMH: none significant',\n 'pn_history_len': 765,\n 'pn_num': 95330}\n🟣 Real-Time Processing with AWS Comprehend Medical\n\nHere we are creating an object from the class called ComprehendMedical and you can see the complete code in the following link of this class\n\n# create an object of the class ComprehendMedical to use the comprehend medical service\naws_comprehendMedical = ComprehendMedical(\n                        aws_region_name=AWS_REGION_NAME,\n                        aws_access_key=AWS_ACCESS_KEY,\n                        aws_secret_access=AWS_SECRET_KEY)\n🩺 AWS Comprehend Medical: Entities\n# get the entities from the note \ntmp_entities = aws_comprehendMedical.get_entities_dataframe(text=note_clean)\n# With the function get_entities_dataframe we can get the mapped and unmapped entities\nmapped_df, unmapped_df = tmp_entities\nmapped_df.head(3)\n\nunmapped_df.head(3)\n\n# write the mapped and unmapped entities to the s3 bucket\ndt =f\"dt={datetime.now().strftime('%Y%m%d')}\"\ns3.write_s3_parquet(data=mapped_df, filename=f'stage/{dt}/entites/mapped_entities_noteId_{note_id}.parquet')\ns3.write_s3_parquet(data=unmapped_df, filename=f'stage/{dt}/entites/unmapped_entities_noteId_{note_id}.parquet')\n🩺 AWS Comprehend Medical: RxNorm\n# get the rxnorm entities from the note\nrxnorm_entities = aws_comprehendMedical.get_rxnorm_dataframe(text=note_clean)\nrxnorm_entities.head()\n\n# Some of the entities are mapped to RxNorm\nfor item in rxnorm_entities['RxNormConcepts'][0]:\n    pprint(item)\n{'Code': '5640', 'Description': 'ibuprofen', 'Score': 0.9967318773269653}\n{'Code': '10255', 'Description': 'suprofen', 'Score': 0.5894578695297241}\n{'Code': '4331', 'Description': 'fenoprofen', 'Score': 0.5856923460960388}\n{'Code': '1312748', 'Description': 'truprofen', 'Score': 0.574164867401123}\n{'Code': '17387', 'Description': 'alminoprofen', 'Score': 0.5531540513038635}\n# Write the RxNorm entities to the s3 bucket\ndt =f\"dt={datetime.now().strftime('%Y%m%d')}\"\ns3.write_s3_parquet(data=rxnorm_entities, filename=f'stage/{dt}/rxnorm/rxnorm_entities_noteId_{note_id}.parquet')\n🩺 AWS Comprehend Medical: SNOMED CT (Clinical Terms)\n# create a dataframe with the snomed entities\nsnomed_ct_entities = aws_comprehendMedical.get_snomed_dataframe(text=note_clean)\nsnomed_ct_entities.head()\n\n# Write the snomed-ct entities to the s3 bucket\ndt =f\"dt={datetime.now().strftime('%Y%m%d')}\"\ns3.write_s3_parquet(data=snomed_ct_entities, filename=f'stage/{dt}/snomed-ct/snomed_ct_noteId_{note_id}.parquet')\n🟣 Batch Processing with AWS Comprehend Medical\nTo perform batch processing, you’ll first need to store the notes as individual txt files in an S3 bucket.\nThese files will be processed, and the results will be saved in a new folder named output within the same bucket.\n\nTo view this section of the tutorial, you can check out my GitHub repository linked below.\n\n\nIf you find this useful, please leave a star ⭐️ and follow me to receive notifications of new articles.\nThis will help me grow in the tech community and create more content.\n\n{% github r0mymendez/aws-comprehend-medical %}\n📚 References\n[1] Amazon Comprehend Medical, Amazon Web Services, URL: https://aws.amazon.com/es/comprehend/medical/\n[2] Summary of the HIPAA Privacy Rule, U.S Deparment of health human services, URL: https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html\n[3] Boto3 1.34.153 documentation, Amazon Web Services, URL: https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n[4] AWS SDK for pandas (awswrangler),AWS Professional Service open source , URL: https://aws-sdk-pandas.readthedocs.io/en/stable/\n[5] NBME - Score Clinical Patient Notes,Kaggle, URL: https://www.kaggle.com/c/nbme-score-clinical-patient-notes/data?select=patient_notes.csv\n[6] Detect entities (Version 2), Amazon Web Services, URL: https://docs.aws.amazon.com/comprehend-medical/latest/dev/textanalysis-entitiesv2.html\n[7] RxNorm, National Library of Medicie, URL: https://www.nlm.nih.gov/research/umls/rxnorm/index.html\n[8] Overview of SNOMED CT, National Library of Medicie, URL: https://www.nlm.nih.gov/healthit/snomedct/snomed_overview.html\n[9] The Book Of Ohdsi, Ohdsi, URL: https://ohdsi.github.io/TheBookOfOhdsi/\n📚 Other references:\n- Image preview reference: [Image by jcomp on Freepik]\n\n\n\n",
    "preview": "posts_en/2024-08-07-employing-aws-comprehend-medical-for-medical-data-extraction-in-healthcare-analytics/preview.jpg",
    "last_modified": "2024-08-08T02:17:21+02:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2024-05-26-python-projects-with-sql-strategies-for-effective-query-management/",
    "title": "Python Projects with SQL: Strategies for Effective Query Management",
    "description": "In this article, you will discover how to use aiosql and I will review how to use it and explained in its documentation.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2024-05-26",
    "categories": [
      "Python",
      "Database"
    ],
    "contents": "\n\nContents\n⚙️ What is aiosql library?\n⚙️ How Does aiosql Work?\n⚙️ Key Features of Aiosql Library\n⚙️ Aiosql Tutorial\n🔧 Prerequisites\n🚀 Quick Start\n🛠️Create a postgres database\n🏥 Hospital Data\n👥 User stories\n🚀 Implementation\n👥 User story I: Static Values\n👥 User story I: Dynamic Values\n👥 User stories II\n👥 User story III\n\n\n📚 Project query catalog\n🔍 Final Conclusions\n\n📚 References\n\n\nMany times, when programming in a project involving interaction with a database, we face the ❓question of how to organize our queries and make them reusable.\nFor this reason, some 🧑‍💻 developers create functions where they concatenate strings to make the queries more dynamic and others prefer to create variables where they define these queries.\nAlthough some more sophisticated developers also use SQLAlchemy object declaration to define the queries, but this has a learning curve and can 📈complicate the development process, especially when dealing with more complex queries.\n\nOne day, I found myself searching for a way to perform this in an orderly, organized, and reusable manner without overly complicating my code, and I stumbled upon an interesting library called aiosql.\nIn the following article, I will review how to use it and explained in its documentation and also I will share some approaches I used to implement it in other contexts.\n⚙️ What is aiosql library?\nAiosql is a 🐍Python library that simplifies the writing of SQL queries in separate files from your main Python project code.\nThese queries, stored in SQL files, are then transformed into methods within a 🐍Python object.\nAnother notable feature of aiosql is its ability to generate dynamic methods that accept parameters, enabling flexible query execution and effective interaction with the underlying database.\nThis separation of SQL queries from the main Python code promotes cleaner and more modular code, enhancing project readability and maintainability.\n⚙️ How Does aiosql Work?\nIn the diagram, you can see that all the queries from an SQL file can be imported and used in Python code by invoking them with the name defined in the query header.\nSubsequently, you can execute the queries by passing the necessary parameters directly from your Python code, which makes the queries reusable and easier to maintain.\n\n⚙️ Key Features of Aiosql Library\nBelow, I will share a series of features that this library already has or can have based on its usage: * Provides CRUD functionality (Create: Insert, Read: Select, Update , Delete) for database operations.\n* Separates Python code from SQL code, making it easier to locate queries within projects with multiple databases.\n* Each query can be assigned a descriptive name and docstring, similar to Python functions, enabling documentation of the query.\n* Facilitates the creation of a query catalog within the project, aiding in identification based on entities, databases, or other grouping criteria.\n* Enables easy generation of dynamic queries with the ability to pass dynamic values and modify them as needed.\nImage description⚙️ Aiosql Tutorial\n🔧 Prerequisites\n🐳 Docker\n🐙 Docker Compose\n🐍 Install python libraries: pip install aiosql pandas\n🚀 Quick Start\n🛠️Create a postgres database\n1️⃣ - Clone this repository: aiosql-tutorial →\n    git clone https://github.com/r0mymendez/aiosql-tutorial.git\n2️⃣ - Change directory to the ‘postgres’ folder →\n    cd aiosql-tutorial/postgres\n3️⃣ - Create postgres database → Execute in the terminal→\n    docker-compose -f docker-compose.yml up --build\n4️⃣ - Check if your container is running → Execute in the terminal →\n      docker ps\n5️⃣ - Load the csv files → Execute the following command for load the csv file in the container →\n    cd src \n    python3 etl.py\n🏥 Hospital Data\nTo implement aiosql, we will use the datasets from Synthea, which simulates a hospital database.\nThese synthetic data are generated from a simulation considering various variables of a population in Massachusetts.\nFrom these datasets, we will use the tables: conditions, encounters, and patients.\n👥 User stories\nTo make this example more real we are going to make 3 use cases:\n1️⃣ - As a data analyst, I want to be able to retrieve a list of patients whose visit count is above the 90th percentile, so that I can identify the most active patients in the clinic.\nAdditionally, I want this percentile to be configurable for easy adjustment in the future.\n2️⃣ - As a researcher or data analyst, I want to access the data of patients who have been diagnosed with the 10 most frequent diagnoses in a period of time, in order to analyze trends and improve the quality of medical care.\n3️⃣ - As a marketing analyst, I want to create a table for patient satisfaction surveys, so that I can gather feedback on the quality of care and take measures to improve it.\n🚀 Implementation\nBased on the user stories that we are going to create, we will define two files in which we will load the queries and scripts that we need to execute: * patients.sql: where we have all the queries related to recovering patient data.\n* visits.sql: where we have all the queries related to visits, such as surveys.\nTherefore in our project we are going to have this structure of folders and files\n- 📁 db\n    - 📁 queries\n        - 📄 patients.sql\n        - 📄 visits.sql\n- 📄 main.ipynb\nIn this way we are isolating the python code from the sql code, in our case we are going to implement this 🐍python code in a notebook in such a way as to make its explanation easier.\n1️⃣ - Import python libraries\nimport aiosql\nimport psycopg2\nimport pandas as pd\n2️⃣ - Import the SQL queries and configure the database driver\n\nIn this project, the SQL queries are located in the ‘db/queries’ directory and ‘psycopg2’ is the PostgreSQL database adapter.\n\nsql = aiosql.from_path('src/db/queries', 'psycopg2')\n3️⃣ - Create the connection to the PostgreSQL database.\npostgres_secrets = {'host': 'localhost','port': 5432, 'user': 'postgres', 'password': 'postgres', 'dbname': 'postgres'}\nconn = psycopg2.connect(**postgres_secrets)\nconn.autocommit = True\n👥 User story I: Static Values\n\nAs a data analyst, I want to be able to retrieve a list of patients whose visit count is above the 90th percentile, so that I can identify the most active patients in the clinic.\nAdditionally, I want this percentile to be configurable for easy adjustment in the future.\n\nBased on this user story, we will first create one that allows generating a query to retrieve the list of patients with a visit frequency above the 90th percentile.\n1️⃣ - In the sql file we have the query for the first user story\nThe following are the three components that a SQL statement comprises in aiosq:\n📗 Name: This is the descriptive name used to invoke the query from Python code.\nIn the following example the name is \"fn_get_patients_adove_90th_percentile\"\n📗 Description: It’s a detailed description used to generate a docstring.\nIt provides a more comprehensive explanation of the purpose and context of the query.\nIn the following example the description is \"get all the patients that have more visits than the 90th percentile of visits...\"\n📗 Query: Here is the SQL query that will be executed in the database.\n📄sql:db/queries/patients.sql\n    -- name: fn_get_patients_adove_90th_percentile\n    -- get all the patients that have more visits than the 90th percentile of visits. All this data is stored in encounters table.\n    WITH patient_visits AS (\n        SELECT\n            patient,\n            COUNT(*) AS visit_count\n        FROM\n            hospital.encounters\n        GROUP BY\n            patient\n    ),\n    percentil_n AS (\n        SELECT\n            percentile_cont(0.9) WITHIN GROUP (ORDER BY visit_count) AS p_visits\n        FROM\n            patient_visits\n    )\n    SELECT \n        pv.patient, \n        pv.visit_count\n    FROM \n        patient_visits pv\n    CROSS JOIN \n        percentil_n  pn\n    WHERE \n        pv.visit_count >= pn.p_visits;\n2️⃣ - Execute the ‘fn_get_patients_above_90th_percentile’ SQL function using the database connection ‘conn’.\n\nThe function returns a list of tuples representing patients whose visit count is above the 90th percentile.\n\n🐍Python\nresponse = sql.fn_get_patients_above_90th_percentile(conn)\n3️⃣ - Now we can convert the response object into a pandas DataFrame for easier data manipulation\n\nThe column names (‘patient_id’ and ‘num_visit’) are added manually because aiosql only returns the query result as a list of tuples without column names.\n\ndata = pd.DataFrame([item for item in response], columns=['patient_id', 'num_visit'])\n# Display the DataFrame.\ndata\n\n\nif we want to see the query, we can use the following code\n\nprint(sql.fn_get_patients_adove_90th_percentile.sql)\n👥 User story I: Dynamic Values\n\nAs a data analyst, I want to be able to retrieve a list of patients whose visit count is above the 90th percentile, so that I can identify the most active patients in the clinic.\nAdditionally, I want this percentile to be configurable for easy adjustment in the future.\n\nNow, we are going to create another query that allows us to accept different percentile values so that the query can be dynamically modified based on the values passed.\nIn our case, we are going to provide an example of obtaining the list of patients that exceed the 75th percentile.\n\nNotice that we now have a dynamic variable called percentile_value\n\n📄sql\n-- name: fn_get_patients_above_n_percentile\nWITH patient_visits AS (\n    ...\n),\npercentil_n AS (\n    SELECT\n        percentile_cont(:percentil_value) WITHIN GROUP (ORDER BY visit_count) AS p_visits\n    FROM\n        patient_visits\n)\nSELECT ...;\n1️⃣ - This following code executes a dynamic SQL query that accepts different percentile values as input.\n🐍Python\n# In this case, we're getting patients above the 75th percentile.\nresponse = sql.fn_get_patients_above_n_percentile(conn, percentil_value=0.75)\ndata = pd.DataFrame([item for item in response], columns=['patient_id', 'num_visit'])\n👥 User stories II\n\nAs a researcher or data analyst, I want to access the data of patients who have been diagnosed with the 10 most frequent diagnoses in a period of time, in order to analyze trends and improve the quality of medical care.\n\nTo resolve this user story, we will create a query that retrieves patients with the most common conditions within a specified time period.\nThis query will be dynamic, allowing for future variations in the number of conditions of interest.\nIt will accept three parameters:\n- ‘num_condition’ will allow us to limit the number of conditions we’re interested in (e.g., the top 10 most common conditions).\n- ‘period_start_date’ and ‘period_start_end’ will define the time window for which we want to retrieve data.\n📄sql\n-- name: fn_get_patients_top_conditions\n-- Get patients with top conditions for a given period of time, the patients are sorted by the number of days they had the condition and the source of the data is the hospital schema.\nwith top_n_conditions as(\nSELECT  code, description, COUNT(*) \n     FROM hospital.CONDITIONS \n     GROUP BY  code,description \n     ORDER BY COUNT(*) DESC \n     LIMIT  :num_condition\n),\ntop_n_condition_patients as (\nSELECT \n    p.ID, \n    p.FIRST, \n    p.LAST, \n    p.CITY, \n    p.GENDER, \n    EXTRACT(YEAR FROM AGE(p.BIRTHDATE)) AS age,\n    c.start condition_start_date,\n    c.stop condition_stop_date,\n    EXTRACT(DAY FROM (c.stop - c.start )) AS condition_days, \n    c.encounter,\n    c.code,\n    c.description\n    from hospital.patients p \n    inner join hospital.conditions c  on c.patient = p.id\n    inner join top_n_conditions t on t.code=c.code\n)\nselect * \n    from top_n_condition_patients\n    where condition_start_date between :period_start_date and :period_start_end;\n🐍Python\nresponse = sql.fn_get_patients_top_conditions(conn, num_condition_days=10, \n                                        period_start_date='2022-01-01', \n                                        period_start_end='2022-12-31')\n\ncolumn_name=['id', 'first','last','city','gender',\n'age','condition_start_date','condition_stop_date','condition_days','encounter','code','description']\n\ndata = pd.DataFrame([item for item in response], columns=column_name)\ndata.head()\n👥 User story III\nAs a marketing analyst, I want to create a table for patient satisfaction surveys, so that I can gather feedback on the quality of care and take measures to improve it.\nNow we are going to create the table using aiosql, if you look at our code in SQL you will see that a # symbol is added, these symbols are used by aiosql to identify the different operations that must be performed.\n📄sql\n-- name: fn_create_survey_table#\nCREATE TABLE HOSPITAL.VISIT_SURVEY(\n    ID SERIAL PRIMARY KEY,\n    PATIENT_ID VARCHAR(50),\n    SURVEY_DATE TIMESTAMP,\n    RATING INT,\n    COMMENTS TEXT,\n    CREATED_AT TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n1️⃣ - Execute the ‘fn_create_survey_table’ SQL function to create a new table in the database.\n🐍Python\nsql.fn_create_survey_table(conn)\n'CREATE TABLE'\n2️⃣ - Once the table is created we are going to use the following insert statement to be able to insert a review of a patient\n📄sql\n-- name: fn_add_one_visit_survey<!\ninsert into HOSPITAL.VISIT_SURVEY(PATIENT_ID,SURVEY_DATE,RATING,COMMENTS) \n    values (:patient_id, :survey_date, :rating,:comments) returning ID;\n\n⚠️ Disclaimer: During the coding of this tutorial, I used the insert statement without a return and encountered an error due to its absence.\n(The version of aiosql I am using is 10.1) This ‘returning ID’ allows us to retrieve the value assigned to the ‘id’ column within the ‘hospital_visit_survey’ table when the insert operation is performed.\n\n🐍Python\n# Add a new visit survey record \n\nsql.fn_add_one_visit_survey(conn, \n                            patient_id='8b9a93f6-3df3-203d-932f-f456e00d2c01', \n                            survey_date='2022-01-01', \n                            rating=5,\n                            comments='This is a great hospital!' )\n3️⃣ - Now we will utilize a new insert statement to load multiple reviews, which are stored in a list of dictionaries (each dictionary in Python corresponds to a review). To accomplish this, we will employ a similar query but we need to modify its name\n📄sql\n    -- name: fn_add_many_visit_survey*!\n    insert into HOSPITAL.VISIT_SURVEY(PATIENT_ID,SURVEY_DATE,RATING,COMMENTS) \n        values (:patient_id, :survey_date, :rating ,:comments) returning ID;\n🐍Python\n# Add several visit survey records\nresponse_survey = [\n    {\n        'patient_id': '8b9a93f6-3df3-203d-932f-f456e00d2c01',\n        'survey_date': '2022-01-01',\n        'rating': 3,\n        'comments': 'The service was good. But the waiting time was a bit long.'\n    },\n    {\n        'patient_id': '7c8a93f6-4df3-203d-932f-f456e00d2c02',\n        'survey_date': '2022-02-01',\n        'rating': 4,\n        'comments': 'The staff was very helpful!'\n    },\n    {\n        'patient_id': '6b7a93f6-5ef3-203d-932f-f456e00d2c03',\n        'survey_date': '2022-03-01',\n        'rating': 3,\n        'comments': 'The waiting time was a bit long.'\n    }\n]\n\n\nsql.fn_add_many_visit_survey(conn, response_survey)\n📚 Project query catalog\nAt the beginning of the tutorial, I mentioned the possibility of creating a catalog of queries for your project.\nAlthough this library doesn’t provide this functionality directly, you can see how to do it and access the complete code and data for this tutorial in my GitHub repository.\nIf you find it useful, you can leave a star ⭐️ and follow me for recieve the notification of new articles, this will help me grow in the tech community and create more content.\n\n🔍 Final Conclusions\nVersatility and Utility: I believe aiosql is a useful library that allows you to implement queries from different projects efficiently.\nIt provides a structured way to manage and execute SQL queries separately from your main codebase, enhancing readability and maintainability.\nFlexible Query Handling: While aiosql enables direct execution of your queries using database connections, in the projects I work on, I primarily use the library to return the SQL code and execute it with classes that I have already set up in Python code.\nOther databases: The ability to store and manage queries can extend beyond SQL databases.\nFor example, this approach can also be applied to NoSQL databases such as Neo4j.\nBy organizing and handling queries in a structured manner, you can optimize interactions with various types of databases.\n📚 References\nIf you want to learn…\n1.aiosql official documentation\nOther references:\n- Image preview reference: [Imagen de Freepik]\n\n\n\n",
    "preview": "posts_en/2024-05-26-python-projects-with-sql-strategies-for-effective-query-management/preview.jpg",
    "last_modified": "2024-05-26T16:35:18+02:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2024-04-02-simplify-database-migrations-using-python-with-alembic/",
    "title": "Simplify Database Migrations using Python with Alembic",
    "description": "In this article, you will discover how to use Alembic for database migration in 🐍Python.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2024-04-02",
    "categories": [
      "Python",
      "Database"
    ],
    "contents": "\n\nContents\n🟣 ORM (Object Relational Mapping)\n🟣 SQLAlchemy\n🟣 Database migrations\n🟣 What are the benefits of using migrations?\n\n🟣 What is Alembic?\n🟣 What is Audit alembic?\n⚙️ Alembic Tutorial\n🔧 Prerequisites\n\nQuick Start\n🔧 Create a postgres database\n🔧 Alembic: Configuration\n🔧 Alembic: Create our first migration\n\n📚 References\n\n\nThe following article will discuss working with database migration using the popular Python library called Alembic.\nHowever, before diving into this tutorial, we’ll mention some definitions that will help us better understand how Alembic works and the best way to implement database migration.\n🟣 ORM (Object Relational Mapping)\nThe ORM (Object Relational Mapping) is a layer that allows connecting object-oriented programming with relational databases, abstracting the underlying SQL queries.\n\n🟣 SQLAlchemy\nSQLAlchemy is a 🐍 python library that implements ORM and allows you to perform different actions on a related database.\n\nThe following are the key components of SQLAlchemy to understand how it interacts with the database:\n🟣 Engine: It is the interface that allows interaction with the database.\nIt handles connections and executes queries.\n🟣 Pool: It is a collection of connections that allows reusing connections and improving query performance by reducing time.\n🟣 Dialect: It is the component that allows interaction with the database.\nEach dialect is designed to interact and translate queries for a database; By default, this library has dialects for MySQL, MariaDB, PostgreSQL, SQL Server, and Oracle.\nBut there are external dialects, in which you should import other libraries, which you can see in the following image.\n🟣 DBAPI: It is the interface that provides methods to enable communication between Python and the database.\n\nBelow is a simple example of how to execute a query in SQL and SQLAlchemy:\nSELECT \n    customer_id, \n    customer_unique_id,\n    customer_zip_code_prefix, \n    customer_city, \n    customer_state \nFROM ecommerce.customers \nLIMIT 10;\nfrom sqlalchemy import Column, Integer, String,create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\n# Create a base class which allows us to define classes that map to tables\nBase = declarative_base()\n\n# Define the class that maps to the table\nclass Customer(Base):\n    __tablename__ = 'customers'  # Cambia a minúsculas\n    __table_args__ = {'schema': 'ecommerce'}\n\n    customer_id = Column(Integer, primary_key=True)\n    customer_unique_id = Column(Integer)\n    customer_zip_code_prefix= Column(Integer)\n    customer_city= Column(String)\n    customer_state = Column(String)\n\n# Create an engine that connects to the PostgreSQL server\nconn ='postgresql://postgres:postgres@localhost/postgres'\nengine = create_engine(conn)\n\n# Create a session\nconn_session = sessionmaker(bind=engine)\nsession = conn_session()\n\n# Execute the query\ncustomers = session.query(Customer).limit(10)\n\n# Extract the data and create a list of tuples\ndata = [ (customer.customer_id,\n          customer.customer_unique_id,\n          customer.customer_zip_code_prefix,\n          customer.customer_city,\n          customer.customer_state) \n    for customer in customers]\n🟣 Database migrations\nA migration is the process that allows you to modify the structure of the database, these migrations are created to maintain consistency and integrity.\n🟣 What are the benefits of using migrations?\n\n🟣 Version Control: Avoids manual intervention in the database by maintaining control over schema versions.\n🟣 Environment Management: Facilitates the creation of new environments through the application of migrations, enabling easy reproduction of specific configurations and maintaining coherence between them.\n🟣 Upgrade & Downgrade: Another benefit is the ability not only to apply changes but also to revert them.\nThis provides flexibility and security in database management.\n🟣 Auditing: Alembic-audit is another library that can be implemented to maintain a chronological record of changes made to the database, facilitating traceability.\n🟣 CI/CD Integration: Easily integrates into CI/CD pipelines to apply database changes automatically, streamlining and ensuring consistency in application deployment.\n🟣 Standardization: This implementation enables cleaner, structured, and coherent development for defining and applying changes to the database schema.\nBy using templates, script reuse is promoted, ensuring efficient and consistent management of database changes.\n🟣 What is Alembic?\nAlembic is a 🐍Python library that enables controlled and automated database migrations.\nThis library utilizes SQLAlchemy and it allows for the management of changes in the database schema through scripts, which describe the modifications and can be applied automatically.\n\n🟣 What is Audit alembic?\n Audit Alembic is a 🐍Python library that complements Alembic by providing an audit table with a detailed record of applied changes.\nWhile Alembic typically maintains only a table in the database with the ID of the last applied migration and allows tracking files using the history command, Audit Alembic goes a step further by creating an additional table that facilitates change tracking and enables the addition of metadata to applied transactions.\ncolumn_name\ncolumn_description\nid\nunique identifier\nalembic_version\nversion of the migration\nprev_alembic_version\nprevious version of the migration\noperation\n“migration” or “rollback”\noperation_direction\ntype of operation (upgrade or downgrade)\nuser_verion\nuser version of the migration in our case we are using the timestamp\nchanged_at\ntimestamp of the migration\n⚙️ Alembic Tutorial\nYou can find the complete code with a step-by-step example in the 🐍 Python notebook in this link.\nHowever, I will provide a brief overview of the main commands in rest sections of this post.\nFor detailed commands and the implementation of Audit Alembic, please refer to the notebook.\n\n\nFeel free to check it out and give it a star if you find it helpful!\n⭐️\n\n🔧 Prerequisites\n🐳 Docker\n🐙 Docker Compose\n🐍 Install python libraries: !pip install alembic Audit-Alembic\nQuick Start\n🔧 Create a postgres database\n1️⃣ - Create docker-compose.yml file\n    version: \"3.7\"\n    services:\n      db:\n        image: postgres:13.3-alpine\n        volumes:\n          - ./db_data:/var/lib/postgresql/data\n        environment:\n          POSTGRES_USER: postgres\n          POSTGRES_PASSWORD: postgres\n          POSTGRES_DB: postgres\n        ports:\n          - \"5433:5432\"\n\n    volumes:\n      db_data:\n2️⃣ - Create postgres database Execute in the terminal: docker-compose -f docker-compose.yml up --build\n3️⃣ - Check if your container is running Execute in the terminal: docker ps\nCONTAINER ID   IMAGE     COMMAND                  CREATED             STATUS             PORTS                                       NAMES\nedb1f7077e66   sqla_db   \"docker-entrypoint.s…\"   About an hour ago   Up About an hour   0.0.0.0:5433->5432/tcp, :::5433->5432/tcp   sqla_db_1\n🔧 Alembic: Configuration\n1️⃣ - Create a new Alembic environment Now, we are going to create a new Alembic project.\nFor this reason, we need to execute the following command, which will create a group of directories and files.\nalembic init project\n2️⃣ - Files & New directory\nfile_name\nDescription\n📄alembic.ini\nThis file is the main configuration file for Alembic, containing the configuration settings for the Alembic environment.\n📁project\\verions\nThis directory is where the migration scripts will be stored.\n📄project\\env.py\nThis Python script contains the function for executing the migration scripts.\n📄project\\script.py.mako\nThis file is the template for generating new migration scripts.\n📄project\\README\nThis file contains a short description of the directory.\n3️⃣ - Add the database connection In the alembic.ini file, add the database connection string to the sqlalchemy.url variable.\nThe connection string should be in the format:\n sqlalchemy.url = driver://user:pass@localhost/dbname \nIn my case I need to configure the following connection * driver: postgresql * user: postgres * password: postgres * dbname: postgres\nsqlalchemy.url = postgresql://postgres:postgres@localhost:5433/postgres \n4️⃣ - File name template We can uncomment the following line in the alembic.ini file to change the name of the files created by Alembic, ensuring a chronological order of the files created.\nfile_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s \n🔧 Alembic: Create our first migration\n1️⃣ - Create a migration script\nalembic revision -m \"create schema ecommerce\"\nNow you can see the new file created in the project/version folder, the file has the following name:\n{current_timestamp}-{unique_identifier}_create_schema_ecommerce.py\ncurrent timestamp\nUnique identifier, in my case is 9ec3d7e4bde9\nThe message that I added in the command, only change the space for a underscore.\n2️⃣ - Modify the migration file In our case, we will create the schema for the ecommerce project.\nHowever, Alembic does not have a specific method for this task.\nTherefore, we will use the op.execute method to execute the SQL query that will create the schema.\n# alembic does not support creating schema directly and we need to use op.execute\ndef upgrade() -> None:\n    op.execute('CREATE SCHEMA IF NOT EXISTS ecommerce_olist;')\n\n\ndef downgrade() -> None:\n    op.execute('DROP SCHEMA IF EXISTS ecommerce_olist CASCADE;')\n3️⃣ - Execute the migration The following command will execute the migration and create the schema in the database.\nIf you see the message “Done,” the migration was successful.\nYou can also check the database to verify that the new schema was created and the Alembic version table was updated.\nalembic upgrade head\n4️⃣ - Check the migrations Now, we can verify the current migration that was executed.\nThis can be controlled using the Alembic command or by checking the table created earlier.\nalembic current\nINFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\nINFO  [alembic.runtime.migration] Will assume transactional DDL.\nb1bc43e2f536 (head)\n5️⃣ - History of migrations Additionally, we can check all the migrations that were executed in this project by using the following command.\nThis will display a detailed history of the migrations, including revision IDs, parent revisions, paths to migration files, and descriptions of the changes made in each migration.\nalembic history --verbose\nRev: b1bc43e2f536 (head)\nParent: 784a7abb86b7\nPath: /project/versions/2024_04_01_1719-b1bc43e2f536_create_table_customer.py\n\n    create table customer\n    \n    Revision ID: b1bc43e2f536\n    Revises: 784a7abb86b7\n    Create Date: 2024-04-01 17:19:09.844065\n\nRev: 784a7abb86b7\nParent: <base>\nPath: /project/versions/2024_04_01_1718-784a7abb86b7_create_schema_ecommerce.py\n\n    create schema ecommerce\n    \n    Revision ID: 784a7abb86b7\n    Revises: \n    Create Date: 2024-04-01 17:18:06.680872\n6️⃣ - Downgrade the migration The following code allows you to revert the changes made previously.\nalembic downgrade -1\nAs mentioned earlier, don’t forget to check out my repository containing the step-by-step guide and the implementation of Audit Alembic, allowing you to have a table with traceability of changes.\nIf you find it useful, you can leave a star ⭐️.\nRemember, it’s always a good development practice to have a tool that applies changes to your database, ensuring coherence, avoiding manual tasks, and enabling quick reproduction of new environments.\n📚 References\nIf you want to learn…\n1.Alembic official documentation\n2.sqlalchemy\n3.Audit-Alembic\nOther references:\n- Image preview reference: [Imagen de Freepik]\n\n\n\n",
    "preview": "posts_en/2024-04-02-simplify-database-migrations-using-python-with-alembic/preview.jpg",
    "last_modified": "2024-04-02T01:39:56+02:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2024-02-12-learning-aws-s3-on-localhost-best-practices-with-boto3-and-localstack/",
    "title": "Learning AWS S3 on Localhost: Best Practices with Boto3 and LocalStack",
    "description": "In this article, you will discover new features of **S3** and learn how to implement some of them using Boto3 in 🐍Python.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2024-02-12",
    "categories": [
      "Python",
      "Cloud Computing"
    ],
    "contents": "\n\nContents\n🔸 What is AWS s3?\n📙 Multiple Use Cases for S3\n📙 S3 storage type\n📙 Object tagging\n📙 S3 Inventory\n\n📙 What is boto3?\n📘 What is localstack?\n📙 Boto3 & 📘 LocalStudio\nPrerequisites\n\n🟣 Build and run the Docker Compose environment\n🚀 Using LocalStack with Boto3: A Step-by-Step Guide\n🛠️ Install Boto3\n🛠️ Create a session using the localstack endpoint\n🛠️ Create new buckets\n📋 List all buckets\n📤 Upload the JSON file to s3\n📋 List all objects\n📄 Upload multiple CSV files to s3\n📄 Read csv file from s3\n🏷️ Add tags to the bucket\n🔄 Versioning in the bucket\n🗑️ Create a static site using s3 bucket\n\n📚 References\n\n\nIn this article, you will discover new features of S3 and learn how to implement some of them using Boto3 in 🐍Python.\nAdditionally, you will deploy a Localstack container to explore these functionalities without the need to use a credit card.\nI🔸 What is AWS s3?\nAWS S3, or Simple Storage Service, is a core service of AWS that serves as a primary storage solution.\nWhile it is commonly known for storing objects, it offers a wide range of functionalities beyond basic storage.\nUnderstanding these features can significantly enhance the utilization of this service.\nSome of the main features of Amazon S3 include:\n🌐 Web accessibility via API or HTTPS, allowing for easy access and integration with web applications.\n🔄 Object versioning, which enables the creation of copies of objects, providing additional data protection and recovery options.\n🔒 Policy creation and application at the bucket level, enhancing security by controlling access to resources and defining permissions.\n📉 Low storage costs and serverless architecture, providing cost-effective and scalable storage solutions with virtually unlimited capacity.\n\n# ✨ Exploring 8 Key Features of Amazon S3\n📙 Multiple Use Cases for S3\nWhile S3 is commonly associated with file storage, such as CSV, JSON, or Parquet files, it offers a wide range of other use cases as well.\nThese include hosting static websites, sharing files, storing data for machine learning models, application configuration, and logging purposes.\n\n📙 S3 storage type\nIn the following image, you can see the types of storage that S3 allows, which depend on the frequency of accessing the object.\nIf you need to access objects frequently, it’s advisable to use the standard storage type.\nOn the other hand, if access to objects is less frequent, it’s recommended to use S3 🧊 Glacier services.\n\n📙 Object tagging\nAnother important feature of S3 is the ability to use tags, this tags are useful for classifying objects and managing costs.\nThey also serve as a strategy for data protection, allowing you to label objects with categories such as confidentiality or sensitive data, and apply policies accordingly.\nAdditionally, S3 supports using tags to trigger Lambda functions, enabling you to perform various actions based on these labels.\n\n📙 S3 Inventory\nWhen managing an S3 bucket, it’s common to accumulate a large number of files within the same bucket.\nTo efficiently organize and manage these files, it’s often necessary to generate an inventory.\nIn S3, an inventory is a file that can be scheduled for updates and contains information about the objects stored in the bucket, including their type, size, and other metadata.\nThis allows for better organization and management of objects within the bucket.\n\n## 📙 S3 Lifecycle configuration\nComprising a set of rules, the S3 lifecycle configuration dictates actions applied by AWS S3 to a group of objects.\nIn the following image, you can observe the typical actions that can be configured.\n\n## 📙 S3 Batch operators\nIt is a feature provided by S3 that allows users to perform operations on objects stored in buckets.\nWith S3 Batch Operations, users can automate tasks such as copying, tagging, deleting, and restoring objects.\nThis feature is particularly useful for organizations managing large amounts of data in S3 and need to perform these operations at scale efficiently.\n\n## 📙 S3 Query Select\nS3 Select is a feature provided by S3 that enables users to find some data from objects stored in S3 buckets using simple SQL queries.\nWith S3 Select, users can efficiently query large datasets stored in various formats such as CSV, JSON, and Parquet, without the need to download and process the entire object.\nThis feature is particularly useful for applications that require selective access to data stored in S3, as it minimizes data transfer and processing overhead.\n\n## 📙 S3 Storage Lens\nS3 Storage Lens is a feature that offers a centralized dashboard with customizable reports and visualizations, allowing users to monitor key metrics such as storage usage, access patterns, and data transfer costs.\nAlso it provides detailed metrics, analytics, and recommendations to help organizations optimize their S3 storage resources, improve data security, and reduce costs.\n\n📙 What is boto3?\nBoto3 is a 🐍 Python library that allows the integration with AWS services, facilitating various tasks such as creation, management, and configuration of these services.\nThere are two primary implementations within Boto3: * Resource implementation: provides a higher-level, object-oriented interface, abstracting away low-level details and offering simplified interactions with AWS services.\n* Client implementation: offers a lower-level, service-oriented interface, providing more granular control and flexibility for interacting with AWS services directly.\n📘 What is localstack?\nLocalstack is a platform that provides a local version of several cloud services, allowing you to simulate a development environment with AWS services.\nThis allows you to debug and refine your code before deploying it to a production environment.\nFor this reason, Localstack is a valuable tool for emulating essential AWS services such as object storage and message queues, among others.\nAlso, Localstack serves as an effective tool for learning to implement and deploy services using a Docker container without the need for an AWS account or the use of your credit card.\nIn this tutorial, we create a Localstack container to implement the main functionalities of S3 services.\n\n📙 Boto3 & 📘 LocalStudio\nAs mentioned earlier, LocalStudio provides a means to emulate a local environment for Amazon with some of the most popular services.\nThis article will guide you through the process of creating a container using the LocalStudio image.\nSubsequently, it will demonstrate the utilization of Boto3 to create an S3 bucket and implement key functionalities within these services.\nBy the end of this tutorial, you’ll have a clearer understanding of how to seamlessly integrate Boto3 with LocalStudio, allowing you to simulate AWS services locally for development and testing purposes.\nPrerequisites\nBefore you begin, ensure that you have the following installed:\n🐳 Docker\n🐙 Docker Compose\n🟣 Build and run the Docker Compose environment\nClone the repository > Feel free to check it out and give it a star if you find it helpful! ⭐️\n\n{% github r0mymendez/LocalStack-boto3 %}\ngit clone https://github.com/r0mymendez/LocalStack-boto3.git\ncd LocalStack-boto3\nBuild an run the docker compose\n\n docker-compose -f docker-compose.yaml up --build\nRecreating localstack ... done\nAttaching to localstack\nlocalstack    | LocalStack supervisor: starting\nlocalstack    | LocalStack supervisor: localstack process (PID 16) starting\nlocalstack    | \nlocalstack    | LocalStack version: 3.0.3.dev\nlocalstack    | LocalStack Docker container id: f313c21a96df\nlocalstack    | LocalStack build date: 2024-01-19\nlocalstack    | LocalStack build git hash: 553dd7e4\n🥳Now we have in Localtcak running in localhost!\n🚀 Using LocalStack with Boto3: A Step-by-Step Guide\n🛠️ Install Boto3\n!pip install boto3\n🛠️ Create a session using the localstack endpoint\nThe following code snippet initializes a client for accessing the S3 service using the LocalStack endpoint.\nimport boto3\nimport json \nimport requests\nimport pandas as pd\nfrom datetime import datetime\nimport io\nimport os\n\n\ns3 = boto3.client(\n    service_name='s3',\n    aws_access_key_id='test',\n    aws_secret_access_key='test',\n    endpoint_url='http://localhost:4566',\n)\n🛠️ Create new buckets\nBelow is the code snippet to create new buckets using the Boto3 library\n# create buckets\nbucket_name_news = 'news'\nbucket_name_config = 'news-config'\n\ns3.create_bucket(Bucket= bucket_name_new )\ns3.create_bucket(Bucket=bucket_name_config)\n📋 List all buckets\nAfter creating a bucket, you can use the following code to list all the buckets available at your endpoint.\n# List all buckets\nresponse = s3.list_buckets()\npd.json_normalize(response['Buckets'])\n\n📤 Upload the JSON file to s3\nOnce we extract data from the API to gather information about news topics, the following code generates a JSON file and uploads it to the S3 bucket previously created.\n# invoke the config news\nurl = 'https://ok.surf/api/v1/cors/news-section-names' \nresponse = requests.get(url)\nif response.status_code==200:\n    data = response.json()\n    # ad json file to s3\n    print('data', data)\n    # upload the data to s3\n    s3.put_object(Bucket=bucket_name_config, Key='news-section/data_config.json', Body=json.dumps(data))\ndata ['US', 'World', 'Business', 'Technology', 'Entertainment', 'Sports', 'Science', 'Health']\n📋 List all objects\nNow, let’s list all the objects stored in our bucket.\nSince we might have stored a JSON file in the previous step, we’ll include code to retrieve all objects from the bucket.\ndef list_objects(bucket_name):\n    response = s3.list_objects(Bucket=bucket_name)\n    return pd.json_normalize(response['Contents'])\n\n# list all objects in the bucket\nlist_objects(bucket_name=bucket_name_config)\n\n📄 Upload multiple CSV files to s3\nIn the following code snippet, we will request another method from the API to extract news for each topic.\nSubsequently, we will create different folders in the bucket to save CSV files containing the news for each topic.\nThis code enables you to save multiple files in the same bucket while organizing them into folders based on the topic and the date of the data request.\n# Request the news feed API Method\nurl = 'https://ok.surf/api/v1/news-feed' \nresponse = requests.get(url)\nif response.status_code==200:\n    data = response.json()\n\n# Add the json file to s3\nfolder_dt =  f'dt={datetime.now().strftime(\"%Y%m%d\")}'\n\nfor item in data.keys():\n    tmp = pd.json_normalize(data[item])\n    tmp['section'] = item   \n    tmp['download_date'] = datetime.now()\n    tmp['date'] = pd.to_datetime(tmp['download_date']).dt.date\n    path = f\"s3://{bucket_name_news}/{item}/{folder_dt}/data_{item}_news.csv\"\n\n    # upload multiple files to s3\n    bytes_io = io.BytesIO()\n    tmp.to_csv(bytes_io, index=False)\n    bytes_io.seek(0)\n    s3.put_object(Bucket=bucket_name_news, Key=path, Body=bytes_io)\n\n# list all objects in the bucket\nlist_objects(bucket_name=bucket_name_news)\n\n📄 Read csv file from s3\nIn this section, we aim to read a file containing news about technology topics from S3.\nTo accomplish this, we first retrieve the name of the file in the bucket.\nThen, we read this file and print the contents as a pandas dataframe.\n# Get the technology file\nfiles = list_objects(bucket_name=bucket_name_news)\ntechnology_file = files[files['Key'].str.find('Technology')>=0]['Key'].values[0]\nprint('file_name',technology_file)\nfile_name s3://news/Technology/dt=20240211/data_Technology_news.csv\n# get the file from s3 using boto3\nobj = s3.get_object(Bucket=bucket_name_news, Key=technology_file)\ndata_tech = pd.read_csv(obj['Body'])\n\ndata_tech\n\n🏷️ Add tags to the bucket\nWhen creating a resource in the cloud, it is considered a best practice to add tags for organizing resources, controlling costs, or applying security policies based on these labels.\nThe following code demonstrates how to add tags to a bucket using a method from the boto3 library.\ns3.put_bucket_tagging(\n    Bucket=bucket_name_news,\n    Tagging={\n        'TagSet': [\n            {\n                'Key': 'Environment',\n                'Value': 'Test'\n            },\n            {\n                'Key': 'Project',\n                'Value': 'Localstack+Boto3'\n            }\n        ]\n    }\n)\n# get the tagging\npd.json_normalize(s3.get_bucket_tagging(Bucket=bucket_name_news)['TagSet'])\n\n🔄 Versioning in the bucket\nAnother good practice to apply is enabling versioning for your bucket.\nVersioning provides a way to recover and keep different versions of the same object.\nIn the following code, we will create a file with the inventory of objects in the bucket and save the file twice.\n# allow versioning in the bucket\ns3.put_bucket_versioning(\n    Bucket=bucket_name_news,\n    VersioningConfiguration={\n        'Status': 'Enabled'\n    }\n)\n# Add new file to the bucket\n\n# file name\nfile_name = 'inventory.csv'\n\n# list all objects in the bucket\nfiles = list_objects(bucket_name=bucket_name_news)\nbytes_io = io.BytesIO()\nfiles.to_csv(bytes_io, index=False)\nbytes_io.seek(0)\n# upload the data to s3\ns3.put_object(Bucket=bucket_name_news, Key=file_name, Body=bytes_io)\n# add again the same file\ns3.put_object(Bucket=bucket_name_news, Key=file_name, Body=bytes_io)\n# List all the version of the object\nversions = s3.list_object_versions(Bucket=bucket_name, Prefix=file_name)\n\npd.json_normalize(versions['Versions'])\n\n🗑️ Create a static site using s3 bucket\nIn this section, we need to utilize a different command, which requires prior installation of the awscli-local tool specifically designed for use with LocalStack.\nThe awscli-local tool facilitates developers in seamlessly engaging with the LocalStack instance, because you can automatically redirecting commands to local endpoints instead of real AWS endpoints.\n# install awslocal to use the cli to interact with localstack\n!pip3.11 install awscli-local\n# the following command creates a static website in s3\n!awslocal s3api create-bucket --bucket docs-web\n# add the website configuration\n!awslocal s3 website s3://docs-web/ --index-document index.html --error-document error.html\n# syncronize the static site with the s3 bucket\n!awslocal s3 sync static-site s3://docs-web\n\nIf you are using localstack, you can access the website using the following\nUrl: http://docs-web.s3-website.localhost.localstack.cloud:4566/\n\n\n📚 References\nIf you want to learn…\nAWS Boto3\nLocalStack\nAPI:OkSurf News\nOther references:\n- Image preview reference: [Imagen de vectorjuice en Freepik]\n\n\n\n",
    "preview": "posts_en/2024-02-12-learning-aws-s3-on-localhost-best-practices-with-boto3-and-localstack/preview.jpg",
    "last_modified": "2024-04-02T01:23:29+02:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2024-01-14-transform-your-pandas-dataframes-in-r/",
    "title": "Transform your R Dataframes: Styles, 🎨 Colors, and 😎 Emojis ",
    "description": "In the following article, we will explore a method to add colors and styles to R DataFrames.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2024-01-21",
    "categories": [
      "R",
      "Data",
      "DataViz"
    ],
    "contents": "\n\nContents\nWhat libraries can I use to style my R dataframes?\n🟣 Pivot Tables\nExample\n\n\n🟣 Dataframe: Apple Store apps\nData Schema overview\n🟣 Create Dataframe\n🟣 Pivot Table\n🟣 Styles with R libraries\n🎨 Styling: Setting Background Color for Headers\n🎨 Style: Setting the background color for all the rows\n🎨 Style: Setting the background color for a particular cell\n🎨 Style: Setting the background color for max/min values in the dataframe\n🎨 Style: Color Background Gradients\n🎨 Style: Color Background by columns\n🎨 Style: Color Background by rows\n🎨 Style: Color Bar\n\n\n🎨 Style: Image in Columns\n🎨 Style: Icons and Charts derived from column comparisons\n🎨 Style: Emoji Representation Based on Percentile Values\n📚 References\n\n\nA few weeks ago I wrote an article about pandas dataframes and how to assign styles, but I received messages about how to do it in R (my first love ❤️ in languages with data) and so I decided to rewrite the article using R libraries.\nSo in the next section of this article, we will explore a method to add 🎨colors and 🖌️styles in R DataFrames.\nWe will focus on the application of colors and emojis, using approaches similar to the popular conditional formatting commonly used in pivot tables within spreadsheets.\nThrough this strategy, we aim to improve the presentation of our data, making the exploration and understanding of the information not only informative but also visually attractive.\nWhat libraries can I use to style my R dataframes?\nThe R libraries used to create this article are as follows:\n\n🔍 tidyverse: Among the best, it integrates various R libraries for data manipulation, graphics, and analysis, promoting clear and efficient code.\n📝 knitr: Automates the generation of dynamic reports.\n📝 kableExtra: An additional extension that enhances table presentation in R Markdown documents with extra formatting options.\n📄 reactablefrmtr: Incorporates functions to craft interactive and flexible tables in R, featuring filtering, sorting, and search functionalities.\n✏️ htmltools: Offers functions to build and manipulate HTML objects in R.\n📄 formattable: Equipped with functions for formatting and customizing tables in R.\n📄 flextable: Another library enabling the creation of flexible and customizable tables in R, with advanced formatting options for documents and presentations.\n📊 ggplot2: Among the most popular R visualization libraries, it produces appealing and comprehensible graphs.\n🎨 viridis: A R library for creating visually appealing color maps\nThese libraries empowered me to employ functions for generating HTML-style representations of DataFrames.\nThis capability enables customization of DataFrame visual appearance during viewing.\nThe functions employed in this article facilitate the highlighting, coloring, and formatting of cells based on specific conditions.\nThis makes it effortless to visually identify patterns and trends within datasets.\n\nNext we have the code with we are going to create a pivot table using a set of data and from this you will begin to give it different styles and conditional formats such as can be seen in the previous image.\n🟣 Pivot Tables\n\nThe pivot table is a tabular data structure that provides a summarized overview of information from another table, organizing the data based on one variable and displaying values associated with another variable.\nIn this specific scenario, the pivot table organizes the data according to the ‘smoker’ column and presents the total sum of tips, categorized by the days on which clients consume in the restaurant\n\n\nExample\nThe following example shows the pivot_table method with the ‘tips’ DataFrame\n\n\nlibrary(reshape2)\nlibrary(tidyverse)\n\ndata = tips\ndata_pivot <- data %>%\n  group_by(smoker, day) %>%\n  summarise(total_bill = sum(total_bill), .groups = 'drop') %>%\n  pivot_wider(names_from = day, values_from = total_bill)\n\ndata_pivot\n\n# A tibble: 2 × 5\n  smoker   Fri   Sat   Sun  Thur\n  <fct>  <dbl> <dbl> <dbl> <dbl>\n1 No      73.7  885. 1169.  770.\n2 Yes    252.   894.  458.  326.\n\n🟣 Dataframe: Apple Store apps\nIn this analysis, we will use the ‘🍎 Apple Store apps’ DataFrame to explore the creation of pivot tables and customization of table styles.\nThis dataset provides detailed insights into Apple App Store applications, covering aspects from app names to specifics like size, price, and ratings.\nOur objective is to efficiently break down the information while applying styles that enhance the presentation and comprehension of data effectively.\nThe dataset was downloaded from Kaggle and it contains more than 7000 Apple iOS mobile application details.\nIt is important to note that the data was collected in July 2017.\nData Schema overview\ncolumn_name\ncolumn description\ntrack_name\nthe column contains the name of the app.\nsize_bytes\nthe column contains the size of the app in bytes.\ncurrency\nthe column contains the currency type.\nprice\nthe column contains the price of the app.\nrating_count_tot\nthe column contains the total number of ratings.\nrating_count_ver\nthe column contains the number of ratings for the current version of the app.\nuser_rating\nthe column contains the average user rating for the app.\nuser_rating_ver\nthe column contains the average user rating for the current version of the app.\nver\nthe column contains the current version of the app.\ncont_rating\nthe column contains the content rating.\nprime_genre\nthe column contains the primary genre.\nsup_devices.num\nthe column contains the number of supported devices.\nipadSc_urls.num\nthe column contains the number of screenshots showed for display.\nlang.num\nthe column contains the number of supported languages.\nvpp_lic\nthe column contains the Vpp Device Based Licensing Enabled.\n🟣 Create Dataframe\nIn the following code chunk, we will create a DataFrame by reading the CSV file.\n\n\nprint(paste0(\"tidyverse version: \",packageVersion(\"tidyverse\")[1]))\n\n[1] \"tidyverse version: 1.3.1\"\n\n\n\n# Create a dataframe from a csv file\n# You can download the file from the following link https://github.com/r0mymendez/pandas-styles\npath = 'https://raw.githubusercontent.com/r0mymendez/pandas-styles/main/data/AppleStore.csv'\ndata = read_delim(path , delim = \";\")\n\n\n🟣 Pivot Table\nIn the next step, our goal is to generate a dynamic table from a Dataframe, in which the top 15 genres with the largest number of applications are filtered.\n\n\n# Pivot table\n\n# filter the data to keep only the top 15 genres\ntop_genre = data %>%\n  group_by(prime_genre) %>%\n  summarise(count = n(), .groups = 'drop') %>%\n  arrange(desc(count)) %>%\n  head(n = 15) %>%\n  pull(prime_genre)\n\n\ntmp = data %>%\n  filter(prime_genre %in% top_genre) %>%\n  select(prime_genre, user_rating, price)\n\n\n# create a new column with the rating rounded to the nearest integer\ntmp$user_rating = paste0(\"rating_\", as.character(trunc(tmp$user_rating)))\n\n\n# create a pivot table\ntmp_pivot <- tmp %>%\n  group_by(prime_genre, user_rating) %>%\n  summarise(price = mean(price, na.rm = TRUE), .groups = 'drop') %>%\n  pivot_wider(names_from = user_rating, values_from = price, values_fill = 0) %>%\n  mutate(across(where(is.numeric), ~round(., 2)))\n\n\n# print the pivot table\ntmp_pivot\n\n# A tibble: 15 × 7\n   prime_genre   rating_0 rating_1 rating_2 rating_3 rating_4 rating_5\n   <chr>            <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n 1 Book              0.49     0        5.32     1.66     3.04     1.92\n 2 Education         3.42     1.79     1.95     2.32     5.2      3.12\n 3 Entertainment     0.51     1.99     0.78     0.9      0.95     1.03\n 4 Finance           0.3      0        0        0.72     0.53     0.5 \n 5 Games             0.85     0.84     1.21     1.71     1.52     1.29\n 6 Health & Fit…     1.33     3.24     1.5      1.21     2.15     1.83\n 7 Lifestyle         0.29     1.27     0.81     0.9      1.08     1.37\n 8 Music             2.74     0        0        2.08     5.27    13.2 \n 9 Photo & Video     0.75     0.74     1.36     2.16     1.47     1.33\n10 Productivity      0.66     2.49     0.99     4.9      4.73     2.61\n11 Shopping          0        0        0        0        0.03     0   \n12 Social Netwo…     0.12     0.4      0.66     0.33     0.41     0.2 \n13 Sports            0.92     2        0.68     0.61     1.25     1.66\n14 Travel            1.1      0        1.28     0.28     1.53     0.2 \n15 Utilities         2.03     2.49     1.25     1.67     1.65     0.66\n\n🟣 Styles with R libraries\nNow we will explore the functions of the aforementioned libraries that will allow us to improve the visual presentation of DataFrames.\nThis functionality provides us with different options to modify the appearance of the data, allowing us to customize aspects such as:\nHighlighting: Emphasize specific rows, columns, or values.\nFormatting: Adjust the format of the displayed values, including precision and alignment.\nBar Charts: Represent data with horizontal or vertical bar charts within cells.\n🎨 Styling: Setting Background Color for Headers\nIn this section, we will apply styles to both the titles and the table.\nTherefore we use background colors to highlight the headers and the rest of the table.\n\n\nlibrary(knitr)\nlibrary(kableExtra)\n\nkable(tmp_pivot, \"html\") %>%\n  kable_styling(\"striped\", full_width = F) %>%\n  row_spec(0, background = \"#5E17EB\", color = \"white\")\n\n\nprime_genre\n\n\nrating_0\n\n\nrating_1\n\n\nrating_2\n\n\nrating_3\n\n\nrating_4\n\n\nrating_5\n\n\nBook\n\n\n0.49\n\n\n0.00\n\n\n5.32\n\n\n1.66\n\n\n3.04\n\n\n1.92\n\n\nEducation\n\n\n3.42\n\n\n1.79\n\n\n1.95\n\n\n2.32\n\n\n5.20\n\n\n3.12\n\n\nEntertainment\n\n\n0.51\n\n\n1.99\n\n\n0.78\n\n\n0.90\n\n\n0.95\n\n\n1.03\n\n\nFinance\n\n\n0.30\n\n\n0.00\n\n\n0.00\n\n\n0.72\n\n\n0.53\n\n\n0.50\n\n\nGames\n\n\n0.85\n\n\n0.84\n\n\n1.21\n\n\n1.71\n\n\n1.52\n\n\n1.29\n\n\nHealth & Fitness\n\n\n1.33\n\n\n3.24\n\n\n1.50\n\n\n1.21\n\n\n2.15\n\n\n1.83\n\n\nLifestyle\n\n\n0.29\n\n\n1.27\n\n\n0.81\n\n\n0.90\n\n\n1.08\n\n\n1.37\n\n\nMusic\n\n\n2.74\n\n\n0.00\n\n\n0.00\n\n\n2.08\n\n\n5.27\n\n\n13.16\n\n\nPhoto & Video\n\n\n0.75\n\n\n0.74\n\n\n1.36\n\n\n2.16\n\n\n1.47\n\n\n1.33\n\n\nProductivity\n\n\n0.66\n\n\n2.49\n\n\n0.99\n\n\n4.90\n\n\n4.73\n\n\n2.61\n\n\nShopping\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.03\n\n\n0.00\n\n\nSocial Networking\n\n\n0.12\n\n\n0.40\n\n\n0.66\n\n\n0.33\n\n\n0.41\n\n\n0.20\n\n\nSports\n\n\n0.92\n\n\n2.00\n\n\n0.68\n\n\n0.61\n\n\n1.25\n\n\n1.66\n\n\nTravel\n\n\n1.10\n\n\n0.00\n\n\n1.28\n\n\n0.28\n\n\n1.53\n\n\n0.20\n\n\nUtilities\n\n\n2.03\n\n\n2.49\n\n\n1.25\n\n\n1.67\n\n\n1.65\n\n\n0.66\n\n\n🎨 Style: Setting the background color for all the rows\nIn following code snippet illustrates how to set a custom background color for all the rows in our DataFrame.\n\n\nkable(tmp_pivot, \"html\") %>%\n  kable_styling(\"striped\", full_width = F)  %>%\n  row_spec(0, background = \"#5E17EB\", color = \"white\") %>%\n  column_spec(1, column=1:ncol(tmp_pivot) ,background = \"#ECE3FF\", color = \"black\")\n\n\nprime_genre\n\n\nrating_0\n\n\nrating_1\n\n\nrating_2\n\n\nrating_3\n\n\nrating_4\n\n\nrating_5\n\n\nBook\n\n\n0.49\n\n\n0.00\n\n\n5.32\n\n\n1.66\n\n\n3.04\n\n\n1.92\n\n\nEducation\n\n\n3.42\n\n\n1.79\n\n\n1.95\n\n\n2.32\n\n\n5.20\n\n\n3.12\n\n\nEntertainment\n\n\n0.51\n\n\n1.99\n\n\n0.78\n\n\n0.90\n\n\n0.95\n\n\n1.03\n\n\nFinance\n\n\n0.30\n\n\n0.00\n\n\n0.00\n\n\n0.72\n\n\n0.53\n\n\n0.50\n\n\nGames\n\n\n0.85\n\n\n0.84\n\n\n1.21\n\n\n1.71\n\n\n1.52\n\n\n1.29\n\n\nHealth & Fitness\n\n\n1.33\n\n\n3.24\n\n\n1.50\n\n\n1.21\n\n\n2.15\n\n\n1.83\n\n\nLifestyle\n\n\n0.29\n\n\n1.27\n\n\n0.81\n\n\n0.90\n\n\n1.08\n\n\n1.37\n\n\nMusic\n\n\n2.74\n\n\n0.00\n\n\n0.00\n\n\n2.08\n\n\n5.27\n\n\n13.16\n\n\nPhoto & Video\n\n\n0.75\n\n\n0.74\n\n\n1.36\n\n\n2.16\n\n\n1.47\n\n\n1.33\n\n\nProductivity\n\n\n0.66\n\n\n2.49\n\n\n0.99\n\n\n4.90\n\n\n4.73\n\n\n2.61\n\n\nShopping\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.03\n\n\n0.00\n\n\nSocial Networking\n\n\n0.12\n\n\n0.40\n\n\n0.66\n\n\n0.33\n\n\n0.41\n\n\n0.20\n\n\nSports\n\n\n0.92\n\n\n2.00\n\n\n0.68\n\n\n0.61\n\n\n1.25\n\n\n1.66\n\n\nTravel\n\n\n1.10\n\n\n0.00\n\n\n1.28\n\n\n0.28\n\n\n1.53\n\n\n0.20\n\n\nUtilities\n\n\n2.03\n\n\n2.49\n\n\n1.25\n\n\n1.67\n\n\n1.65\n\n\n0.66\n\n\n🎨 Style: Setting the background color for a particular cell\nIn following code snippet illustrates how to set a custom background color for a particular cell in our DataFrame using pandas styling.\n\n\nvalue = 4\n\ntmp_pivot %>%\n  mutate(\n    rating_4 = cell_spec(rating_4, \"html\", \n                    background = if_else(tmp_pivot$rating_4>value, \"#FD636B\", \"#ECE3FF\"),\n                    color = if_else(tmp_pivot$rating_4>value, \"white\", \"black\")\n    )\n  ) %>%\n  kable(format = \"html\", escape = F) %>%\n  kable_styling(\"striped\", full_width = FALSE) %>%\n  row_spec(0, background = \"#5E17EB\", color = \"white\") %>%\n  column_spec(1:ncol(tmp_pivot), background = \"#ECE3FF\", color = \"black\")\n\n\nprime_genre\n\n\nrating_0\n\n\nrating_1\n\n\nrating_2\n\n\nrating_3\n\n\nrating_4\n\n\nrating_5\n\n\nBook\n\n\n0.49\n\n\n0.00\n\n\n5.32\n\n\n1.66\n\n\n3.04\n\n\n1.92\n\n\nEducation\n\n\n3.42\n\n\n1.79\n\n\n1.95\n\n\n2.32\n\n\n5.2\n\n\n3.12\n\n\nEntertainment\n\n\n0.51\n\n\n1.99\n\n\n0.78\n\n\n0.90\n\n\n0.95\n\n\n1.03\n\n\nFinance\n\n\n0.30\n\n\n0.00\n\n\n0.00\n\n\n0.72\n\n\n0.53\n\n\n0.50\n\n\nGames\n\n\n0.85\n\n\n0.84\n\n\n1.21\n\n\n1.71\n\n\n1.52\n\n\n1.29\n\n\nHealth & Fitness\n\n\n1.33\n\n\n3.24\n\n\n1.50\n\n\n1.21\n\n\n2.15\n\n\n1.83\n\n\nLifestyle\n\n\n0.29\n\n\n1.27\n\n\n0.81\n\n\n0.90\n\n\n1.08\n\n\n1.37\n\n\nMusic\n\n\n2.74\n\n\n0.00\n\n\n0.00\n\n\n2.08\n\n\n5.27\n\n\n13.16\n\n\nPhoto & Video\n\n\n0.75\n\n\n0.74\n\n\n1.36\n\n\n2.16\n\n\n1.47\n\n\n1.33\n\n\nProductivity\n\n\n0.66\n\n\n2.49\n\n\n0.99\n\n\n4.90\n\n\n4.73\n\n\n2.61\n\n\nShopping\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.03\n\n\n0.00\n\n\nSocial Networking\n\n\n0.12\n\n\n0.40\n\n\n0.66\n\n\n0.33\n\n\n0.41\n\n\n0.20\n\n\nSports\n\n\n0.92\n\n\n2.00\n\n\n0.68\n\n\n0.61\n\n\n1.25\n\n\n1.66\n\n\nTravel\n\n\n1.10\n\n\n0.00\n\n\n1.28\n\n\n0.28\n\n\n1.53\n\n\n0.20\n\n\nUtilities\n\n\n2.03\n\n\n2.49\n\n\n1.25\n\n\n1.67\n\n\n1.65\n\n\n0.66\n\n\n🎨 Style: Setting the background color for max/min values in the dataframe\nNow, we will focus on highlighting the maximum and minimum values in our DataFrame.\nFor this reason, we will assign distinctive background colors to these extreme values, facilitating a quicker and more intuitive understanding of the dataset.\nThe code snippet below demonstrates how to implement this stylistic enhancement.\n\n\nrating_columns <- grep(\"^rating\", names(tmp_pivot), value = TRUE)\nmax_value <- max(unlist(tmp_pivot %>% select(rating_columns), use.names = FALSE))\nmin_value <- min(unlist(tmp_pivot %>% select(rating_columns), use.names = FALSE))\n\n# The next function to apply specific formatting and preserve the original\nformat_spec <- function(x) {\n  if_else(x == max_value, sprintf(\"%.2f\", x),\n          if_else(x == min_value, sprintf(\"%.2f\", x),\n                  sprintf(\"%.2f\", x)))\n}\n\ntmp_pivot %>%\n  mutate(\n    across(rating_columns, \n           ~ cell_spec(format_spec(.x),\n          \"html\", \n           background = if_else(. == max_value, \"#3BE8B0\",\n                                if_else(. == min_value, \"#FF66C4\", \"#ECE3FF\")),\n           bold = if_else(. == max_value, TRUE,if_else(. == min_value, TRUE, FALSE))\n                )\n         )\n  ) %>%\n  kable(format = \"html\", escape = F) %>%\n  kable_styling(\"striped\", full_width = FALSE) %>%\n  row_spec(0, background = \"#5E17EB\", color = \"white\") %>%\n  column_spec(1:ncol(tmp_pivot), background = \"#ECE3FF\", color = \"black\")\n\n\nprime_genre\n\n\nrating_0\n\n\nrating_1\n\n\nrating_2\n\n\nrating_3\n\n\nrating_4\n\n\nrating_5\n\n\nBook\n\n\n0.49\n\n\n0.00\n\n\n5.32\n\n\n1.66\n\n\n3.04\n\n\n1.92\n\n\nEducation\n\n\n3.42\n\n\n1.79\n\n\n1.95\n\n\n2.32\n\n\n5.20\n\n\n3.12\n\n\nEntertainment\n\n\n0.51\n\n\n1.99\n\n\n0.78\n\n\n0.90\n\n\n0.95\n\n\n1.03\n\n\nFinance\n\n\n0.30\n\n\n0.00\n\n\n0.00\n\n\n0.72\n\n\n0.53\n\n\n0.50\n\n\nGames\n\n\n0.85\n\n\n0.84\n\n\n1.21\n\n\n1.71\n\n\n1.52\n\n\n1.29\n\n\nHealth & Fitness\n\n\n1.33\n\n\n3.24\n\n\n1.50\n\n\n1.21\n\n\n2.15\n\n\n1.83\n\n\nLifestyle\n\n\n0.29\n\n\n1.27\n\n\n0.81\n\n\n0.90\n\n\n1.08\n\n\n1.37\n\n\nMusic\n\n\n2.74\n\n\n0.00\n\n\n0.00\n\n\n2.08\n\n\n5.27\n\n\n13.16\n\n\nPhoto & Video\n\n\n0.75\n\n\n0.74\n\n\n1.36\n\n\n2.16\n\n\n1.47\n\n\n1.33\n\n\nProductivity\n\n\n0.66\n\n\n2.49\n\n\n0.99\n\n\n4.90\n\n\n4.73\n\n\n2.61\n\n\nShopping\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.03\n\n\n0.00\n\n\nSocial Networking\n\n\n0.12\n\n\n0.40\n\n\n0.66\n\n\n0.33\n\n\n0.41\n\n\n0.20\n\n\nSports\n\n\n0.92\n\n\n2.00\n\n\n0.68\n\n\n0.61\n\n\n1.25\n\n\n1.66\n\n\nTravel\n\n\n1.10\n\n\n0.00\n\n\n1.28\n\n\n0.28\n\n\n1.53\n\n\n0.20\n\n\nUtilities\n\n\n2.03\n\n\n2.49\n\n\n1.25\n\n\n1.67\n\n\n1.65\n\n\n0.66\n\n\n🎨 Style: Color Background Gradients\nIn the upcoming section, we will delve into the concept of color maps, representing a spectrum of colors arranged in a gradient.\nA colormap, essentially a palette of colors, consists of distinctive denominations, with the most popular ones being [‘viridis,’ ‘magma,’ ‘inferno,’ ‘plasma’, ‘cividis’].\nThe primary objective behind creating these color spectrums is to enhance the visual representation of data.\nEach color in the gradient carries specific nuances, contributing to a more nuanced data visualization experience.\n\n\nlibrary(viridisLite)  \nlibrary(viridis)\nlibrary(unikn)  # load package\nseecol(pal = pal_unikn)\n\n\n# Reference of the following code: https://bookdown.org/hneth/ds4psy/D-4-apx-colors-pkgs.html\nn <- 10  # number of colors\n\n# define 5 different color scales (n colors each):\nv1 <- viridis(n)\nv2 <- magma(n)\nv3 <- inferno(n)\nv4 <- plasma(n)\nv5 <- cividis(n)\n\n# See and compare color scales:\nseecol(list(v1, v2, v3, v4, v5), \n       col_brd = \"white\", lwd_brd = 4, \n       title = \"Various viridis color palettes (n = 10)\",\n       pal_names = c(\"v1: viridis\", \"v2: magma\", \"v3: inferno\", \"v4: plasma\",  \"v5: cividis\"))\n\n\n\nViridis palette\n\nNow, we will apply a color gradient to our pivot table, allowing you to observe how it is colored using the Viridis palette.\nIn this context, lighter colors signify larger values within the distribution, while darker shades correspond to smaller values in the distribution.\nThis approach provides a visual representation that intuitively conveys the magnitude of the data, making it easier to discern patterns and variations across the dataset.\n\n\n\nlibrary(ggplot2)\n# Número de tonos (lut)\nlut <- 10\n\n# Crear un data frame con una variable continua\ndata <- data.frame(x = seq(1, lut))\n\noptions(repr.plot.width = 5, repr.plot.height =2) \n\n# Crear un gráfico de barras con colores de la paleta \"viridis\"\nggplot(data, aes(x = x, y = 0.2, fill = as.factor(x))) +\n  geom_tile() +\n  scale_fill_manual(values = viridis(lut, option = \"D\")) +\n  labs(x = \"Índice\", y = \"\") +\n  theme_void() +\n  theme(legend.position = \"none\") \n\n\n\n\n\n# Calculate maximum and minimum values\nmax_value <- max(unlist(tmp_pivot %>% select(rating_columns), use.names = FALSE))\nmin_value <- min(unlist(tmp_pivot %>% select(rating_columns), use.names = FALSE))\n\n# Define the number of cuts for the \"viridis\" palette\nnum_cuts <-nrow(tmp_pivot)\n\nxc <- seq(min_value, max_value, length.out = num_cuts)\npal <- viridis(num_cuts)\n\n# Apply color gradients to each cell with viridis \nstyled_table <- map(tmp_pivot, function(col) {\n  if (is.numeric(col)) {\n    cell_spec( format_spec(col),\n               \"html\", \n               background = pal[cut(col, breaks = xc, include.lowest = TRUE)])\n  } else {\n    cell_spec(col, \"html\")\n  }\n}) %>%\n  as.data.frame() %>%\n  kable(format = \"html\", escape = F)  %>%\n  kable_styling(\"striped\", full_width = FALSE)  %>%\n  row_spec(0, background = \"#440154FF\", color = \"white\") %>%\n  column_spec(2:ncol(tmp_pivot), color = \"white\") %>%\n  column_spec(1:1, background = \"#ECE3FF\") \n\nstyled_table\n\n\nprime_genre\n\n\nrating_0\n\n\nrating_1\n\n\nrating_2\n\n\nrating_3\n\n\nrating_4\n\n\nrating_5\n\n\nBook\n\n\n0.49\n\n\n0.00\n\n\n5.32\n\n\n1.66\n\n\n3.04\n\n\n1.92\n\n\nEducation\n\n\n3.42\n\n\n1.79\n\n\n1.95\n\n\n2.32\n\n\n5.20\n\n\n3.12\n\n\nEntertainment\n\n\n0.51\n\n\n1.99\n\n\n0.78\n\n\n0.90\n\n\n0.95\n\n\n1.03\n\n\nFinance\n\n\n0.30\n\n\n0.00\n\n\n0.00\n\n\n0.72\n\n\n0.53\n\n\n0.50\n\n\nGames\n\n\n0.85\n\n\n0.84\n\n\n1.21\n\n\n1.71\n\n\n1.52\n\n\n1.29\n\n\nHealth & Fitness\n\n\n1.33\n\n\n3.24\n\n\n1.50\n\n\n1.21\n\n\n2.15\n\n\n1.83\n\n\nLifestyle\n\n\n0.29\n\n\n1.27\n\n\n0.81\n\n\n0.90\n\n\n1.08\n\n\n1.37\n\n\nMusic\n\n\n2.74\n\n\n0.00\n\n\n0.00\n\n\n2.08\n\n\n5.27\n\n\n13.16\n\n\nPhoto & Video\n\n\n0.75\n\n\n0.74\n\n\n1.36\n\n\n2.16\n\n\n1.47\n\n\n1.33\n\n\nProductivity\n\n\n0.66\n\n\n2.49\n\n\n0.99\n\n\n4.90\n\n\n4.73\n\n\n2.61\n\n\nShopping\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.03\n\n\n0.00\n\n\nSocial Networking\n\n\n0.12\n\n\n0.40\n\n\n0.66\n\n\n0.33\n\n\n0.41\n\n\n0.20\n\n\nSports\n\n\n0.92\n\n\n2.00\n\n\n0.68\n\n\n0.61\n\n\n1.25\n\n\n1.66\n\n\nTravel\n\n\n1.10\n\n\n0.00\n\n\n1.28\n\n\n0.28\n\n\n1.53\n\n\n0.20\n\n\nUtilities\n\n\n2.03\n\n\n2.49\n\n\n1.25\n\n\n1.67\n\n\n1.65\n\n\n0.66\n\n\n🎨 Style: Color Background by columns\nIn the next code chunk, we will enhance the visual representation of our pivot table by introducing distinct color backgrounds to specific columns.\nThis technique aids in better highlighting and categorizing data, making it easier to draw insights from the table.\n\n\ntmp_pivot%>%\n  kable(format = \"html\", escape = F)  %>%\n  kable_styling(\"striped\", full_width = FALSE)  %>%\n  column_spec(2:3, background = \"#FFCFC9\", color = \"black\") %>%\n  column_spec(4:5, background = \"#FFF1B0\", color = \"black\") %>%\n  column_spec(6:7, background = \"#BEEAE5\", color = \"black\")%>%\n  row_spec(0, background = \"#440154FF\", color = \"white\") %>%\n  column_spec(1:1, background = \"#ECE3FF\") \n\n\nprime_genre\n\n\nrating_0\n\n\nrating_1\n\n\nrating_2\n\n\nrating_3\n\n\nrating_4\n\n\nrating_5\n\n\nBook\n\n\n0.49\n\n\n0.00\n\n\n5.32\n\n\n1.66\n\n\n3.04\n\n\n1.92\n\n\nEducation\n\n\n3.42\n\n\n1.79\n\n\n1.95\n\n\n2.32\n\n\n5.20\n\n\n3.12\n\n\nEntertainment\n\n\n0.51\n\n\n1.99\n\n\n0.78\n\n\n0.90\n\n\n0.95\n\n\n1.03\n\n\nFinance\n\n\n0.30\n\n\n0.00\n\n\n0.00\n\n\n0.72\n\n\n0.53\n\n\n0.50\n\n\nGames\n\n\n0.85\n\n\n0.84\n\n\n1.21\n\n\n1.71\n\n\n1.52\n\n\n1.29\n\n\nHealth & Fitness\n\n\n1.33\n\n\n3.24\n\n\n1.50\n\n\n1.21\n\n\n2.15\n\n\n1.83\n\n\nLifestyle\n\n\n0.29\n\n\n1.27\n\n\n0.81\n\n\n0.90\n\n\n1.08\n\n\n1.37\n\n\nMusic\n\n\n2.74\n\n\n0.00\n\n\n0.00\n\n\n2.08\n\n\n5.27\n\n\n13.16\n\n\nPhoto & Video\n\n\n0.75\n\n\n0.74\n\n\n1.36\n\n\n2.16\n\n\n1.47\n\n\n1.33\n\n\nProductivity\n\n\n0.66\n\n\n2.49\n\n\n0.99\n\n\n4.90\n\n\n4.73\n\n\n2.61\n\n\nShopping\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.03\n\n\n0.00\n\n\nSocial Networking\n\n\n0.12\n\n\n0.40\n\n\n0.66\n\n\n0.33\n\n\n0.41\n\n\n0.20\n\n\nSports\n\n\n0.92\n\n\n2.00\n\n\n0.68\n\n\n0.61\n\n\n1.25\n\n\n1.66\n\n\nTravel\n\n\n1.10\n\n\n0.00\n\n\n1.28\n\n\n0.28\n\n\n1.53\n\n\n0.20\n\n\nUtilities\n\n\n2.03\n\n\n2.49\n\n\n1.25\n\n\n1.67\n\n\n1.65\n\n\n0.66\n\n\n🎨 Style: Color Background by rows\nIn the next code chunk, we will enhance the visual representation of our pivot table by introducing distinct color backgrounds by rows.\n\n\nn <- nrow(tmp_pivot)\nrow_0 <-c()\nrow_1 <-c()\n\n\nfor (item in seq(1, n)) {\n  if (item %% 2 == 0) {\n    row_0 <- c(row_0,item)\n  } else {\n    row_1 <- c(row_1,item)\n  }\n}\n\n tmp_pivot %>%\n  kable(format = \"html\", escape = F) %>%\n  kable_styling(\"striped\", full_width = FALSE) %>%\n  row_spec(row_0, background = \"#ECE3FF\", color = \"black\") %>%\n  row_spec(row_1, background = \"#ffdefa\", color = \"black\")%>%\n  row_spec(0, background = \"#440154FF\", color = \"white\") \n\n\nprime_genre\n\n\nrating_0\n\n\nrating_1\n\n\nrating_2\n\n\nrating_3\n\n\nrating_4\n\n\nrating_5\n\n\nBook\n\n\n0.49\n\n\n0.00\n\n\n5.32\n\n\n1.66\n\n\n3.04\n\n\n1.92\n\n\nEducation\n\n\n3.42\n\n\n1.79\n\n\n1.95\n\n\n2.32\n\n\n5.20\n\n\n3.12\n\n\nEntertainment\n\n\n0.51\n\n\n1.99\n\n\n0.78\n\n\n0.90\n\n\n0.95\n\n\n1.03\n\n\nFinance\n\n\n0.30\n\n\n0.00\n\n\n0.00\n\n\n0.72\n\n\n0.53\n\n\n0.50\n\n\nGames\n\n\n0.85\n\n\n0.84\n\n\n1.21\n\n\n1.71\n\n\n1.52\n\n\n1.29\n\n\nHealth & Fitness\n\n\n1.33\n\n\n3.24\n\n\n1.50\n\n\n1.21\n\n\n2.15\n\n\n1.83\n\n\nLifestyle\n\n\n0.29\n\n\n1.27\n\n\n0.81\n\n\n0.90\n\n\n1.08\n\n\n1.37\n\n\nMusic\n\n\n2.74\n\n\n0.00\n\n\n0.00\n\n\n2.08\n\n\n5.27\n\n\n13.16\n\n\nPhoto & Video\n\n\n0.75\n\n\n0.74\n\n\n1.36\n\n\n2.16\n\n\n1.47\n\n\n1.33\n\n\nProductivity\n\n\n0.66\n\n\n2.49\n\n\n0.99\n\n\n4.90\n\n\n4.73\n\n\n2.61\n\n\nShopping\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.03\n\n\n0.00\n\n\nSocial Networking\n\n\n0.12\n\n\n0.40\n\n\n0.66\n\n\n0.33\n\n\n0.41\n\n\n0.20\n\n\nSports\n\n\n0.92\n\n\n2.00\n\n\n0.68\n\n\n0.61\n\n\n1.25\n\n\n1.66\n\n\nTravel\n\n\n1.10\n\n\n0.00\n\n\n1.28\n\n\n0.28\n\n\n1.53\n\n\n0.20\n\n\nUtilities\n\n\n2.03\n\n\n2.49\n\n\n1.25\n\n\n1.67\n\n\n1.65\n\n\n0.66\n\n\n🎨 Style: Color Bar\nIn this section, we will implement the style.bar function to introduce a dynamic color bar into our DataFrame.\nThe color bar provides a visual representation of data values, assigning varying colors to different data ranges.\n\n\nlibrary(htmlwidgets)\nlibrary(htmltools)\nlibrary(formattable)\n\nformattable(tmp_pivot, list(\n  rating_0 = color_bar(\"#FFCFC9\"),\n  rating_1 = color_bar(\"#FFCFC9\"),\n  rating_2 = color_bar(\"#FFF1B0\"),\n  rating_3 = color_bar(\"#FFF1B0\"),\n  rating_4 = color_bar(\"#BEEAE5\"),\n  rating_5 = color_bar(\"#BEEAE5\")\n)) %>%\n  as.htmlwidget() %>%\n  prependContent(tags$style(\"th { padding: 0px !important; background: #5E17EB; color: white }\")) %>%\n  prependContent(tags$style(\"table tr td:first-child { background-color: #ECE3FF }\")) \n\n\n\n🎨 Style: Image in Columns\nIn this section, we explore the enhancement of data representation by adding an image to an additional column.\nThis approach provides an alternative method to elevate the visual impact of the data being presented.\nThese images can serve as icons, represent brands, or convey additional visual elements to captivate and engage the audience.\n\n\nlibrary(flextable)\n\nflextable(tmp_pivot%>%\n              head(5)%>%\n              mutate(id=0:4,\n                     id=paste0('img/img_',id,'.png'))%>%\n              select(c('id',names(tmp_pivot)))\n         ) %>%\n  colformat_image(j = \"id\", width = .5, height = 0.5) %>%\n  bg(part = \"header\", bg = \"#5E17EB\", j =  c('id',names(tmp_pivot)) ) %>% \n  color(color = \"white\", part = \"header\") %>% \n  bg(part = \"body\", bg = \"#ECE3FF\")\n\nidprime_genrerating_0rating_1rating_2rating_3rating_4rating_5Book0.490.005.321.663.041.92Education3.421.791.952.325.203.12Entertainment0.511.990.780.900.951.03Finance0.300.000.000.720.530.50Games0.850.841.211.711.521.29\n\n🎨 Style: Icons and Charts derived from column comparisons\nIn this section, we’ll explore a creative approach using icons and charts to visually represent comparisons between two or more columns of data.\n\n\n# Define the column names\ncolumn_names <- paste0(\"rating_\", 0:5)\n\n# Apply the formatter function to each column\nformatters_list <- lapply(column_names, function(col) {\n  prev_col <- paste0(\"rating_\", as.numeric(str_extract(col, \"\\\\d+\")) - 1)\n  if (col == \"rating_0\") {\n    formatter(\n      \"span\",\n      style = ~ formattable::style(\"font.weight\" = \"bold\"),\n      ~icontext(\"arrow-right\", tmp_pivot[[col]])\n    )\n  } else {\n    formatter(\n      \"span\",\n      style = ~ formattable::style(color = ifelse(tmp_pivot[[col]] > tmp_pivot[[prev_col]], \n                                                  \"green\", \"red\"),\n                      \"font.weight\" = \"bold\"),\n      ~icontext(ifelse(tmp_pivot[[col]] > tmp_pivot[[prev_col]], \"arrow-up\", \"arrow-down\"), tmp_pivot[[col]])\n    )\n  }\n})\n\n# Create a named list for formattable\nformatters_list <- setNames(formatters_list, column_names)\n\n# Apply formattable and convert to htmlwidget\nformattable(tmp_pivot, formatters_list) %>%\n  as.htmlwidget() %>%\n  prependContent(\n    tags$style(\"th { padding: 0px !important; background: #5E17EB; color: white }\")) %>%\n  prependContent(\n    tags$style(\"table tr td:first-child { background-color: #ECE3FF }\"))\n\n\n\n\n\n# Get the names of columns that start with 'rating'\ncolumn_str = names(tmp_pivot[startsWith(names(tmp_pivot), \"rating\")])\n\n# Get the names of columns that we will expect to have\nexpected_columns = c(names(tmp_pivot), 'plot_line', 'plot_bar')\n\n# Create the new data frame with two new columns with the charts representation\ntmp_nest <- tmp_pivot %>%\n  nest(values = c(column_str), values_1=c(column_str)) %>%\n  mutate( values = map(values, ~ as.numeric(.x) ) ) %>%\n  unnest(values_1)%>%\n  mutate(\n    plot_line = map_chr(values,\n                        ~ as.character(htmltools::as.tags(sparkline::sparkline(.x, \n                                                               type = \"line\",\n                                                               lineColor='#5E17EB',\n                                                               fillColor='#5E17EB'))\n                                       )\n                        ),\n     plot_bar = map_chr(values, \n                       ~ as.character(htmltools::as.tags(sparkline::sparkline(.x,\n                        type = \"bar\",\n                        barColor = '#5E17EB'))\n                        )\n                       )\n  )%>%\n  select(expected_columns)\n\n# Print the table with html format\nout = as.htmlwidget(formattable(tmp_nest))\nout$dependencies = c(out$dependencies, htmlwidgets:::widget_dependencies(\"sparkline\", \"sparkline\"))\nout %>%\n  prependContent(tags$style(\"th { padding: 0px !important; background: #5E17EB; color: white }\")) %>%\n  prependContent(tags$style(\"table tr td:first-child { background-color: #ECE3FF }\"))\n\n\n\n\n\nlibrary(reactablefmtr)\n\nreactable(tmp_pivot%>%head(10),\n          defaultColDef = colDef(\n            cell = data_bars(\n              tmp_pivot, \n              box_shadow = TRUE, \n              round_edges = TRUE,\n              text_position = \"outside-base\",\n              fill_color = c(\"#5E17EB\", \"#ECE3FF\"),\n              background = \"#e5e5e5\",\n              fill_gradient = TRUE\n            ),\n            style = list(\n              #backgroundColor = \"#ECE3FF\",\n              color = \"black\",\n              fontSize = \"14px\"\n            ),\n            headerStyle = list(\n              backgroundColor = \"#5E17EB\",\n              color = \"white\",\n              fontFamily = \"Comic Sans MS\",\n              fontSize = \"14px\"\n            )\n          )\n)\n\n\n\n\n\nreactable(\n  tmp_pivot%>%head(5)%>%select(-c('rating_0')),\n  defaultColDef = colDef(\n    align = 'center',\n    cell = bubble_grid(\n      data = tmp_pivot%>%head(5),\n      shape = \"circles\",\n      number_fmt = scales::comma,\n      colors = c(\"#D6B1FB\",\"#C687FC\",\"#A060FF\"),\n      brighten_text=TRUE,\n      text_size=14,\n      box_shadow = TRUE,\n      opacity = 0.6\n    ),\n    headerStyle = list(\n      backgroundColor = \"#5E17EB\",\n      color = \"white\",\n      fontFamily = \"Comic Sans MS\",\n      fontSize = \"14px\"\n    )\n  ),\n  columns = list(\n    prime_genre = colDef(\n      style = list(\n        fontSize = \"14px\"\n      )\n    )\n  )\n)%>% \n  add_title(\n    title = reactablefmtr::html(\"🍎 Apple Store apps\"),\n    margin = reactablefmtr::margin(t=0,r=0,b=5,l=0)\n    ) \n\n🍎 Apple Store apps\n\n\n\n\ntmp_pivot_5 = tmp_pivot%>%head(5)\n\nreactable(\n  tmp_pivot_5%>%select(-c('rating_0')),\n  defaultColDef = colDef(\n    align = 'center',\n    cell = bubble_grid(\n      data = tmp_pivot_5,\n      shape = \"squares\",\n      number_fmt = scales::comma,\n      colors = c(\"#D6B1FB\",\"#C687FC\",\"#A060FF\"),\n      brighten_text=TRUE,\n      text_size=14,\n      box_shadow = TRUE,\n      opacity = 0.6,\n    ),\n    headerStyle = list(\n      backgroundColor = \"#5E17EB\",\n      color = \"white\",\n      fontFamily = \"Comic Sans MS\",\n      fontSize = \"14px\"\n    )\n  ),\ncolumns = list(\n    prime_genre = colDef(\n      minWidth = 175,\n      cell = pill_buttons(data = tmp_pivot_5$prime_genre, box_shadow = TRUE,colors=\"#A060FF\",text_color='white'),\n      style = list(\n        fontSize = \"14px\"\n      )\n    )\n  )\n) %>% \n  add_title(\n    title = reactablefmtr::html(\"🍎 Apple Store apps\"),\n    margin = reactablefmtr::margin(t=0,r=0,b=5,l=0)\n  ) \n\n🍎 Apple Store apps\n\n\n\n\nreactable(\n  tmp_pivot%>%head(5)%>%select(-c('rating_0')),,\n  defaultColDef = colDef(\n    align = 'left',\n    cell = gauge_chart(\n      data = tmp_pivot_5,\n      number_fmt = scales::comma,\n      fill_color=\"#5E17EB\",\n      background=\"#e5e5e5\",\n    ),\n    headerStyle = list(\n      backgroundColor = \"#5E17EB\",\n      color = \"white\",\n      fontFamily = \"Comic Sans MS\",\n      fontSize = \"14px\"\n    )\n  ),\n  columns = list(\n    prime_genre = colDef(\n      minWidth = 175,\n      header = htmltools::tags$div(\n        \"Prime Genre\", \n        style = list(\n          backgroundColor = \"#5E17EB\",\n          color = \"white\",\n          fontFamily = \"Comic Sans MS\",\n          fontSize = \"14px\"\n        )\n      ),\n      cell = pill_buttons(data = tmp_pivot_5$prime_genre, box_shadow = TRUE,colors=\"#A060FF\",text_color='white'),\n      style = list(\n        fontSize = \"14px\",\n        fontFamily = \"Comic Sans MS\"\n      )\n    )\n  )\n) %>% \n  add_title(\n  title = htmltools::HTML(\n    sprintf(\"<div style='font-family:Comic Sans MS;'>%s<\/div>\", \"🍎 Apple Store apps\")\n  ),\n  margin = reactablefmtr::margin(t=0,r=0,b=5,l=0)\n)\n\n🍎 Apple Store apps\n\n\n🎨 Style: Emoji Representation Based on Percentile Values\nIn this section, we delve into the creative use of emojis based on percentile values, offering a distinctive approach to elevate data representation.\nBy incorporating diverse emojis, we enhance the visual impact of the data.\nSpecifically, we employ circles and squads as emojis to bring nuanced expressions to our data points.\nIf you’d like to view the code for creating this style, it’s available in my GitHub repository.\nFeel free to check it out and give it a star if you find it helpful!\n⭐️\n\n\ncreate_series <- function(row_data, emoji) {\n  if (emoji == 'max') {\n    return(ifelse(row_data == max(row_data), '🟩', '⬜'))\n  } else if (emoji == 'min') {\n    return(ifelse(row_data == min(row_data), '🟥', '⬜'))\n  } else if (emoji == 'min_max') {\n    return(ifelse(row_data == min(row_data), '🟥',\n                  ifelse(row_data == max(row_data), '🟩', '⬜')))\n }\n}\n\n\n\n\nget_percentiles <- function(row_data, bins=3, emoji='circle') {\n  emoji_labels <- list(\n    'circle' = list('3' = c('🔴', '🟡', '🟢'), \n                    '4' = c('🔴', '🟠', '🟡', '🟢')),\n    'squad' =  list('3' = c('🟥', '🟨', '🟩'), \n                   '4' = c('🟥', '🟨', '🟧', '🟩'))\n  )\n  \n  if (emoji %in% c('max', 'min', 'min_max')) {\n    return(create_series(row_data, emoji))\n  } else if (emoji %in% names(emoji_labels) & bins %in% names(emoji_labels[[emoji]])) {\n    labels <- emoji_labels[[emoji]][[as.character(bins)]]\n    return(cut(row_data, breaks=length(labels), labels=labels, ordered_result=FALSE))\n  } else {\n    return(row_data)\n  }\n}\n\n\n\n\nget_conditional_table_column <- function(data, bins=3, emoji='circle') {\n  tmp <- data\n  for (column in colnames(data)) {\n    if (is.numeric(data[[column]])) {\n      row_data_emoji <- as.character(get_percentiles(data[[column]], bins, emoji))\n      tmp[[column]] <- paste(format(round(data[[column]], 2), nsmall = 2), row_data_emoji)\n    }\n  }\n  return(tmp)\n}\n\n\n\n\nget_conditional_table_row <- function(data, bins=3, emoji='circle') {\n  response_values <- list()\n  column_str <- names(data)[sapply(data, is.character)]\n  columns_num <- names(data)[sapply(data, is.numeric)]\n  \n  for (row in 1:nrow(data)) {\n    row_data <- data[row, columns_num]\n    percentil <- get_percentiles(row_data, bins, emoji)\n    row_data <- sapply(round(row_data, 2),format_spec)\n    percentil_values <- paste(row_data, percentil)\n    response_values <- append(response_values, list(percentil_values))\n  }\n  \n  result_df <- data.frame(matrix(unlist(response_values), \n                                 nrow=length(response_values), byrow=TRUE))\n  \n  names(result_df) <- columns_num\n  result_df <- cbind(data[column_str], result_df)\n  return(result_df)\n}\n\n\n\n\n# get conditional table by column with 3 bins \nget_conditional_table_row(data=tmp_pivot%>%head(5),emoji='min_max')\n\n    prime_genre rating_0 rating_1 rating_2 rating_3 rating_4 rating_5\n1          Book  0.49 ⬜  0.00 🟥  5.32 🟩  1.66 ⬜  3.04 ⬜  1.92 ⬜\n2     Education  3.42 ⬜  1.79 🟥  1.95 ⬜  2.32 ⬜  5.20 🟩  3.12 ⬜\n3 Entertainment  0.51 🟥  1.99 🟩  0.78 ⬜  0.90 ⬜  0.95 ⬜  1.03 ⬜\n4       Finance  0.30 ⬜  0.00 🟥  0.00 🟥  0.72 🟩  0.53 ⬜  0.50 ⬜\n5         Games  0.85 ⬜  0.84 🟥  1.21 ⬜  1.71 🟩  1.52 ⬜  1.29 ⬜\n\n\n\n# get conditional table by column using the max value\nget_conditional_table_column(data=tmp_pivot_5,emoji='max')\n\n# A tibble: 5 × 7\n  prime_genre   rating_0 rating_1 rating_2 rating_3 rating_4 rating_5\n  <chr>         <chr>    <chr>    <chr>    <chr>    <chr>    <chr>   \n1 Book          0.49 ⬜  0.00 ⬜  5.32 🟩  1.66 ⬜  3.04 ⬜  1.92 ⬜ \n2 Education     3.42 🟩  1.79 ⬜  1.95 ⬜  2.32 🟩  5.20 🟩  3.12 🟩 \n3 Entertainment 0.51 ⬜  1.99 🟩  0.78 ⬜  0.90 ⬜  0.95 ⬜  1.03 ⬜ \n4 Finance       0.30 ⬜  0.00 ⬜  0.00 ⬜  0.72 ⬜  0.53 ⬜  0.50 ⬜ \n5 Games         0.85 ⬜  0.84 ⬜  1.21 ⬜  1.71 ⬜  1.52 ⬜  1.29 ⬜ \n\n\n\n# get conditional table by column using the circle emoji with 4 bins\nget_conditional_table_column(data=tmp_pivot_5,emoji='circle',bins=4)\n\n# A tibble: 5 × 7\n  prime_genre   rating_0 rating_1 rating_2 rating_3 rating_4 rating_5\n  <chr>         <chr>    <chr>    <chr>    <chr>    <chr>    <chr>   \n1 Book          0.49 🔴  0.00 🔴  5.32 🟢  1.66 🟡  3.04 🟡  1.92 🟡 \n2 Education     3.42 🟢  1.79 🟢  1.95 🟠  2.32 🟢  5.20 🟢  3.12 🟢 \n3 Entertainment 0.51 🔴  1.99 🟢  0.78 🔴  0.90 🔴  0.95 🔴  1.03 🔴 \n4 Finance       0.30 🔴  0.00 🔴  0.00 🔴  0.72 🔴  0.53 🔴  0.50 🔴 \n5 Games         0.85 🔴  0.84 🟠  1.21 🔴  1.71 🟡  1.52 🔴  1.29 🟠 \n\n📚 References\nIf you want to learn…\nData Science for Psychologists, D.4 Using color packages\nUsing the flextable R package\nCreate Awesome HTML Table with knitr::kable and kableExtra\nViridisLite: Documentation\nIntroduction to the viridis color maps\nFormattable: Documentation\nReactablefmtr: Documentation\nOther references:\nImage preview reference: Imagen de vectorjuice en storyset\n\n\n\n",
    "preview": "posts_en/2024-01-14-transform-your-pandas-dataframes-in-r/preview.jpg",
    "last_modified": "2024-02-04T19:57:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2024-01-13-sql-assistant-text-to-sql-application-in-streamlit/",
    "title": "SQL Assistant: Text-to-SQL Application in Streamlit 🤖",
    "description": "In this article, we will explore the application of Vanna.ai, a Python library specifically designed for training a model capable of processing natural language questions and generating SQL queries as responses.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2024-01-13",
    "categories": [
      "Python",
      "Data",
      "DataViz",
      "AI"
    ],
    "contents": "\n\nContents\n🤖 Text-to-SQL & Vanna.ai\nVanna.AI: Output Possibilities\nVanna.AI: Features\n\n🛠️ Simple Implementation\nInstall vanna-ai\nImplementation Steps\n\n🧪 Model Training\n💬 Streamlit Chatbot\nDescription\nQuick Start\n🤖 Chatbot Preview\n\n📚 References\n\n\nIn this article, we will explore the application of Vanna.ai, a 🐍 Python library specifically designed for training a model capable of processing natural language questions and generating SQL queries as responses.\nThis implementation will be integrated into a ✨Streamlit application, creating a 🤖chatbot that facilitates posing questions and provides explanations for the returned queries.\n\n\n\n\n\n\n\n🤖 Text-to-SQL & Vanna.ai\nText-to-SQL is a tool that utilizes models to translate natural language queries into SQL queries, aiming to make it easy for users to generate SQL queries and interact with databases seamlessly.\nThe implementation of Text-to-SQL can be achieved through the use of Vanna.AI, an open-source 🐍Python library that allows the training of an RAG model with queries, DDL, and documentation from a database.\n\nVanna.AI: Output Possibilities\nThe following are the possible outputs that can be generated with Vanna methods, facilitating diverse ways of interacting with the database using natural language.\nOutput\nDescription\n📄 SQL\nVanna.AI can generate SQL queries from natural language questions. These SQL queries can be used to interact with a database.\n📁 DataFrame\nAfter generating an SQL query, Vanna.AI can execute it in the database and return the results as a pandas DataFrame.\n📊 Charts\nVanna.AI can generate plots using Plotly from the results of the SQL query.\n❓ Follow-up questions\nVanna.AI can generate follow-up questions based on the generated SQL query. These follow-up questions can help users refine their queries or explore the data in more detail.\n🔍 Explanations queries\nVanna.AI can provide explanations for the generated SQL queries. These explanations can help users understand how their natural language question is interpreted into an SQL query.\nVanna.AI: Features\nThe following table contains the key features available with Vanna.AI, enriching data interaction capabilities:\nFeature\nDescription\n🚀 Model Training\nTrain the RAG model on data sources, including Data Definition Language (DDL) statements, documentation, and SQL queries during the training process.\n🤖 User Query Handling\nUsers can pose questions in natural language, and Vanna.AI responds by generating SQL queries.\n📚 Documentation\nExtensive documentation, a dedicated website, and a support community on Discord are available to offer comprehensive assistance.\n🔌 Database Connections\nVanna allows connecting to multiple databases, enabling users not only to retrieve SQL queries but also to execute them by establishing connections to their respective databases.\n🤔 AI-Generated Recommendation Questions\nThis framework includes a feature of generating AI-driven questions, offering suggestions for additional queries that the user could explore.\n🛠️ Simple Implementation\nFor this initial implementation, we will leverage an example provided by vanna.ai, using the Chinook database.\nThis database includes tables and data relevant to a music store, encompassing information about artists, albums, tracks, customers, orders, and various aspects associated with the management of an online music store.\n\nInstall vanna-ai\n!pip install vanna\nImplementation Steps\nFollow these steps to implement a straightforward example of text-to-SQL:\nAPI Key Retrieval: To initiate this example, acquire an API key by registering at https://vanna.ai/.\nUtilize your registered email to obtain the key.\nSetting API Key and Model: Configure the API key obtained and specify the model to be used, in this instance, the pre-existing “chinook” model\nConnecting to the Database: Establish a connection with the database, utilizing an SQLite file available at https://vanna.ai/Chinook.sqlite.\nAsking a Question: Finally, pose a natural language question to extract specific information from the database.\nIn this step, the result includes the SQL query, the DataFrame from the query execution, and a representative chart.\nThe system also generates follow-up questions based on the dataset.\nimport vanna as vn\n\n# STEP 01: This is a simple example of how to use the Vanna API\napi_key = vn.get_api_key('your_email')\n\n# Set the API key and the model\nvn.set_api_key(api_key)\n\n# STEP 02:  Set the model\nvn.set_model('chinook')\n\n# STEP 03:  Connect with the database\nvn.connect_to_sqlite('https://vanna.ai/Chinook.sqlite')\n\n# STEP 04:  Ask a question\nvn.ask('What are the top 10 artists by sales?')\n📃sql query\nSELECT a.name,\n       sum(il.quantity) as totalsales\nFROM   artist a\n    INNER JOIN album al\n        ON a.artistid = al.artistid\n    INNER JOIN track t\n        ON al.albumid = t.albumid\n    INNER JOIN invoiceline il\n        ON t.trackid = il.trackid\nGROUP BY a.name\nORDER BY totalsales desc limit 10;\n📊plotly chart\n\n❓New Questions\nAI-generated follow-up questions:\n\n* Who is the artist with the highest sales?\n* What are the total sales for each artist?\n* Which genre has the highest sales?\n* Can you provide a breakdown of sales by album?\n* Which artist has the lowest sales?\n* What is the average sales per artist?\n* Can you provide a list of the top-selling albums?\n* Which genre has the least sales?\n* Can you provide a breakdown of sales by country?\n* What is the total sales for each genre?\n🧪 Model Training\nTo train your own model follow the following steps: 1.\nLog in to your account https://vanna.ai/ and create a new model.\n2.\nNext we will define how to train the model.\nIn our next example we will use ddl (data definition language), documentation and queries.\n\n# Check the models available in the account\nvn.get_models()\n['ecommerce-test', 'demo-tpc-h', 'tpc', 'chinook', 'thelook']\n# Set the model\nvn.set_model(\"ecommerce-test\")\n# Get the ddl for training the model\n# Train the model with the ddl\nddl = \"\"\"\nCREATE TABLE if not exists stage.customers(\n    customer_id           INT NOT NULL PRIMARY KEY,\n    email_address         VARCHAR(50) NOT NULL,\n    name                  VARCHAR(50) NOT NULL,\n    business_type_id      INT NOT NULL,\n    site_code             VARCHAR(10) NOT NULL,\n    archived              BOOLEAN NOT NULL,\n    is_key_account        BOOLEAN NOT NULL,\n    date_updated          TIMESTAMP NOT NULL,\n    date_created          TIMESTAMP NOT NULL,\n    job_created_date  TIMESTAMP WITH TIME ZONE DEFAULT \n        CURRENT_TIMESTAMP,\n    job_created_user  varchar(50) default null,\n    job_updated_date  TIMESTAMP default null,\n    job_updated_user  varchar(50) default null,\n    CONSTRAINT fk_business_type_id FOREIGN KEY(business_type_id) REFERENCES stage.business_types (business_type_id)\n);\n\"\"\"\nvn.train(ddl=ddl)\n\nIn my repository, you can find all the scripts, documentation, and queries to train the model and answer questions such as the following\n\n# Ask a question for generating the SQL\nquestion  =  \"\"\"What is the total count of new clients who registered between October 1, 2020, and \nJanuary 1, 2022, and have made more than 10 purchases, each exceeding $20? Additionally,\n could you provide their email addresses, the number of purchases made, and the date of their\n  most recent purchase?\"\"\"\n\nprint(vn.generate_sql(question=question))\nSELECT COUNT(*) AS total_count,\n       c.email_address,\n       COUNT(o.order_id) AS num_purchases,\n       MAX(o.order_date) AS most_recent_purchase_date\nFROM Customers c\nJOIN Orders o ON c.customer_id = o.customer_id\nWHERE c.registration_date >= '2020-10-01' AND c.registration_date <= '2022-01-01'\n  AND o.order_value > 20\nGROUP BY c.email_address\nHAVING COUNT(o.order_id) > 10;\n💬 Streamlit Chatbot\nstreamlit - vanna-aiDescription\nIn this section, we will implement a 🤖chatbot application using text-to-SQL capabilities with ✨Streamlit.\nThis app will be developed through the integration of Vanna.AI and ✨Streamlit, providing a user-friendly interface for entering your username, selecting an avatar, and initiating a 💬chat.\nQuick Start\nClone the Repository {% github r0mymendez/text-to-sql %}\nAdd your ddl scripts, documentation and sql queries in src\\db\\\nAdd your credentials in src\\.streamlit\\secrets.toml\nExecute the Application\n\nDetailed instructions on how to run the application and add credentials can be found in the repository’s README.md.\n\n🤖 Chatbot Preview\nThe application, crafted with Vanna.AI and ✨Streamlit, and you can see below a video of how it works, remember that all the explanations are in the readme.md file of the repository.\nFeel free to check it out and give it a star if you find it helpful!\n⭐️\n\n\n\n\n\n\n\n\nchatbot-3📚 References\nIf you want to learn…\nvanna-ai\n✨streamlit\nOther references:\nImage preview reference: [Imagen de vectorjuice en Freepik]\n\n⚠️ Disclaimer: Before proceeding with the use of your own data in the Vanna.AI application, it is recommended to review and validate the data protection policies established by Vanna.AI. Ensure a thorough understanding of how your personal data is handled and protected within the application framework. It is crucial to comply with privacy and security regulations before providing any sensitive information.\n\n\n\n\n",
    "preview": "posts_en/2024-01-13-sql-assistant-text-to-sql-application-in-streamlit/preview.jpg",
    "last_modified": "2024-01-14T07:39:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2024-01-02-transform-your-pandas-dataframes/",
    "title": "Transform your Pandas Dataframes: Styles, 🎨 Colors, and 😎 Emojis",
    "description": "In the following article, we will explore a method to add colors and styles to Pandas DataFrames.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2024-01-02",
    "categories": [
      "Python",
      "Data",
      "DataViz"
    ],
    "contents": "\n\nContents\nWhat is Pandas Style?\n🟣 Pivot Tables\nExample\n\n🟣 Dataframe: Apple Store apps\nData Schema overview\n\n🟣 Create Dataframe\n🟣 Pivot Table\n🟣 Styling with Pandas\n🎨 Styling: Setting Background Color for Headers\n🎨 Style: Setting the background color for a particular cell\n🎨 Style: Setting the background color for max/min values in the dataframe\n🎨 Style: Color Background Gradients\n🎨 Style: Color Background in columns\n🎨 Style: Color Bar\n🎨 Style: Image in Columns\n🎨 Style: Emoji Representation Based on Percentile Values\n\n📚 References\n\n\n\nIn the following section of this article, we will explore a method to add colors and styles to Pandas DataFrames.\nOur focus will be on the application of colors and emojis, utilizing approaches similar to the popular conditional formatting commonly employed in pivot tables within spreadsheets.\nThrough this strategy, we aim to enhance the presentation of our data, making the exploration and understanding of information not only informative but also visually appealing\nimage generated using partyrockWhat is Pandas Style?\nPandas Styler is a module within the Pandas library that provides methods for creating HTML-styled representations of DataFrames.\nThis feature allows for the customization of the visual appearance of DataFrames during their visualization.\nThe core functionality of Pandas Styler lies in the ability to highlight, color, and format cells based on specific conditions, facilitating the visual identification of patterns and trends in datasets.\nAlso, Pandas Styler stands out for its capability to assist in the design of DataFrames or series by generating visual representations using HTML and CSS.\nThis functionality simplifies the creation of attractive and customized data presentations, enhancing the visualization experience, and enabling a more intuitive interpretation of the information contained in the datasets.\n\nNext we have the code with we are going to create a pivot table using a set of data and from this you will begin to give it different styles and conditional formats such as can be seen in the previous image.\n🟣 Pivot Tables\n\nThe pivot table is a tabular data structure that provides a summarized overview of information from another table, organizing the data based on one variable and displaying values associated with another variable.\nIn this specific scenario, the pivot table organizes the data according to the ‘smoker’ column and presents the total sum of tips, categorized by the days on which clients consume in the restaurant\n\nExample\nThe following example shows the pivot_table method with the ‘tips’ DataFrame\n\npython code\nimport pandas as pd\nimport seaborn as sns\n\n# create the tips dataframe \ndata = sns.load_dataset('tips')\ndata_pivot = pd.pivot_table(data,\n                    index='smoker',\n                    columns='day',\n                    values='total_bill',\n                    aggfunc='sum').reset_index()\ndata_pivot\nouput\nday\nsmoker\nThur\nFri\nSat\nSun\n0\nYes\n326.24\n252.20\n893.62\n458.28\n1\nNo\n770.09\n73.68\n884.78\n1168.88\n🟣 Dataframe: Apple Store apps\nIn this analysis, we will use the ‘🍎 Apple Store apps’ DataFrame to explore the creation of pivot tables and customization of table styles.\nThis dataset provides detailed insights into Apple App Store applications, covering aspects from app names to specifics like size, price, and ratings.\nOur objective is to efficiently break down the information while applying styles that enhance the presentation and comprehension of data effectively.\nThe dataset was downloaded from Kaggle and it contains more than 7000 Apple iOS mobile application details.\nIt is important to note that the data was collected in July 2017.\nData Schema overview\ncolumn_name\n column description\ntrack_name\nthe column contains the name of the app.\nsize_bytes\nthe column contains the size of the app in bytes.\ncurrency\nthe column contains the currency type.\nprice\nthe column contains the price of the app.\nrating_count_tot\nthe column contains the total number of ratings.\nrating_count_ver\nthe column contains the number of ratings for the current version of the app.\nuser_rating\nthe column contains the average user rating for the app.\nuser_rating_ver\nthe column contains the average user rating for the current version of the app.\nver\nthe column contains the current version of the app.\ncont_rating\nthe column contains the content rating.\nprime_genre\nthe column contains the primary genre.\nsup_devices.num\nthe column contains the number of supported devices.\nipadSc_urls.num\nthe column contains the number of screenshots showed for display.\nlang.num\nthe column contains the number of supported languages.\nvpp_lic\nthe column contains the Vpp Device Based Licensing Enabled.\n🟣 Create Dataframe\nIn the following code chunk, we will create a DataFrame by reading the CSV file.\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport warnings\n\n# Deactivate pandas warning\nwarnings.filterwarnings('ignore')\n\n\nprint(\"Python Libraries version:\")\nprint('--'*20)\nprint(\"Pandas version: \", pd.__version__)\nprint(\"Numpy version: \", np.__version__)\nprint(\"Matplotlib version: \", plt.matplotlib.__version__)\nPython Libraries version:\n----------------------------------------\nPandas version:  2.1.3\nNumpy version:  1.26.1\nMatplotlib version:  3.8.1\n# Create a dataframe from a csv file\n# You can download the file from the following link https://github.com/r0mymendez/pandas-styles\npath='data/AppleStore.csv'\ndata =pd.read_csv(path,sep=';')\n🟣 Pivot Table\nIn the next step, we are going to create a pivot table from a DataFrame.\n# Pivot table\n\n# filter the data to keep only the top 15 genres\ntop_genre = data.value_counts('prime_genre')[:15].index.tolist()\ntmp = data.loc[data['prime_genre'].isin(top_genre),['prime_genre','user_rating','price']]\n\n# create a new column with the rating rounded to the nearest integer\ntmp['user_rating'] = [f'rating_{str(math.trunc(item))}' for item in  tmp['user_rating']]\n\n# create a pivot table\ntmp_pivot = (\n        pd.pivot_table(\n            data = tmp,\n            columns='user_rating',\n            index='prime_genre',\n            values='price',\n            aggfunc='mean',\n            fill_value=0\n            ).reset_index().round(2)\n)\n# rename the columns\ntmp_pivot.columns.name=''\n# print the pivot table\ntmp_pivot\n\n🟣 Styling with Pandas\nNow we will explore the style module in Pandas, that enables us to enhance the visual presentation of DataFrames.\nThe style module provides a differents of options to modify the appearance of the data, allowing us to customize aspects such as:\nColoring Cells: Apply different colors based on cell values or conditions.\nHighlighting: Emphasize specific rows, columns, or values.\nFormatting: Adjust the format of the displayed values, including precision and alignment.\nBar Charts: Represent data with horizontal or vertical bar charts within cells.\n🎨 Styling: Setting Background Color for Headers\nIn this section, we will apply styles to both the titles and the table.\nTherefore we use background colors to highlight the headers and the rest of the table.\n# Styling: Changing Background Color for Column Headers\nheaders = {\n    'selector': 'th.col_heading',\n    'props': 'background-color: #5E17EB; color: white;'\n}\n\nindex_style = {\n    'selector': 'th.index_name',\n    'props': 'background-color: #5E17EB; color: white;'\n}\n\ntmp_pivot_style = (\n    tmp_pivot\n        .style\n            .set_table_styles([headers,index_style])\n            .set_properties(**{'background-color': '#ECE3FF','color': 'black'})\n)\n\ntmp_pivot_style\n\n🎨 Style: Setting the background color for a particular cell\nIn following code snippet illustrates how to set a custom background color for a particular cell in our DataFrame using pandas styling.\n(\n    tmp_pivot\n        .style\n            .set_table_styles([headers, index_style])\n            .set_properties(**{'background-color': '#ECE3FF', 'color': 'black'})\n            .set_properties(**{'background-color': '#FD636B', 'color': 'white'},subset=pd.IndexSlice[4, 'rating_5'])\n)\n\n🎨 Style: Setting the background color for max/min values in the dataframe\nNow, we will focus on highlighting the maximum and minimum values in our DataFrame.\nFor this reason, we will assign distinctive background colors to these extreme values, facilitating a quicker and more intuitive understanding of the dataset.\nThe code snippet below demonstrates how to implement this stylistic enhancement.\n# select the columns that start with 'rating_'\ncolumns = tmp_pivot.columns[tmp_pivot.columns.str.startswith('rating_')]\n\n# get the max and min values\nmax_value = tmp_pivot[columns].max().max()\nmin_value = tmp_pivot[columns].min().min()\n\n# Establecer el estilo para la celda con el valor máximo\nmax_style = f'border: 4px solid #3BE8B0 !important;'\n\n# Establecer el estilo para la celda con el valor mínimo\nmin_style = f'background-color: #FF66C4; '\n\n(\n    tmp_pivot\n        .style\n            .set_table_styles([headers, index_style])\n            .set_properties(**{'background-color': '#ECE3FF', 'color': 'black'})\n            .set_properties(**{'background-color': '#FD636B', 'color': 'white'}, subset=pd.IndexSlice[4, 'rating_5'])\n            .applymap(lambda x: max_style if x == max_value else '')\n            .applymap(lambda x: min_style if x == min_value else '', subset=columns)\n)\n\n🎨 Style: Color Background Gradients\nIn the upcoming section, we will delve into the concept of color maps, representing a spectrum of colors arranged in a gradient.\nA colormap, essentially a palette of colors, consists of distinctive denominations, with the most popular ones being [‘viridis,’ ‘magma,’ ‘Greens,’ ‘Reds’].\nThe primary objective behind creating these color spectrums is to enhance the visual representation of data.\nEach color in the gradient carries specific nuances, contributing to a more nuanced data visualization experience.\nFor an extensive array of color options, you can explore the matplotlib colormaps link.\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the colormap\nfor cmap_item in ['viridis', 'magma','Greens','Reds']:\n    cmap = plt.get_cmap(cmap_item)\n    # Create a color gradient\n    gradient = np.linspace(0, 1, 256).reshape(1, -1)\n\n    # Display the color palette\n    plt.figure(figsize=(10, 0.2))\n    plt.imshow(gradient, aspect='auto', cmap=cmap)\n    plt.axis('off')\n    plt.title(f'{cmap_item.capitalize()} Color Palette', loc='left', fontsize=9)\n    plt.show()\n\n\nViridis palette\nNow, we will apply a color gradient to our pivot table, allowing you to observe how it is colored using the Viridis palette.\nIn this context, lighter colors signify larger values within the distribution, while darker shades correspond to smaller values in the distribution.\nThis approach provides a visual representation that intuitively conveys the magnitude of the data, making it easier to discern patterns and variations across the dataset.\n\nplt.get_cmap('viridis',lut=20)\n\n(\n    tmp_pivot\n        .style\n            .set_table_styles([headers, index_style])\n            .background_gradient(cmap='viridis',subset=columns)\n)\n\n🎨 Style: Color Background in columns\nIn the next code chunk, we will enhance the visual representation of our pivot table by introducing distinct color backgrounds to specific columns.\nThis technique aids in better highlighting and categorizing data, making it easier to draw insights from the table.\n(\n    tmp_pivot\n        .style\n            .set_table_styles([headers, index_style])\n            .set_properties(**{'background-color': '#FFCFC9','color':'black'},subset=['rating_0','rating_1'])\n            .set_properties(**{'background-color': '#FFF1B0','color':'black'},subset=['rating_2','rating_3'])\n            .set_properties(**{'background-color': '#BEEAE5','color':'black'},subset=['rating_4','rating_5'])\n)\nI🎨 Style: Color Bar\nIn this section, we will implement the style.bar function to introduce a dynamic color bar into our DataFrame.\nThe color bar provides a visual representation of data values, assigning varying colors to different data ranges.\n(\n    tmp_pivot\n        .style\n            .set_table_styles([headers, index_style])\n            .set_properties(**{'background-color': '#ECE3FF', 'color': 'black'})\n            .set_properties(**{'background-color': 'white','color':'black'},subset=columns)\n            .bar(color='#FFCFC9',subset=['rating_0','rating_1'])\n            .bar(color='#FFF1B0',subset=['rating_2','rating_3'])\n            .bar(color='#BEEAE5',subset=['rating_4','rating_5'])\n )\n\n🎨 Style: Image in Columns\nIn this section, we explore the enhancement of data representation by adding an image to an additional column.\nThis approach provides an alternative method to elevate the visual impact of the data being presented.\nThese images can serve as icons, represent brands, or convey additional visual elements to captivate and engage the audience.\n# create a function to add an image to the dataframe depending on the genre\ndef add_image(image_name):\n    img_url = f\"img/icons/img_{image_name}.png\"\n    width   = \"width: 50px\"\n    height  = \"height: 50px\"\n    text_align =\"center\"\n    return f'{width};{height}; content: url({img_url}); text-align:{text_align}'\n\n# apply the function to the dataframe\nstyled_df = (\n    tmp_pivot\n        .head(5)\n        .reset_index()\n        .rename({'index': 'genre'}, axis=1)\n        .style.applymap(add_image, subset=pd.IndexSlice[:, ['genre']])\n        .set_table_styles([headers, index_style])\n        .set_properties(**{'background-color': '#ECE3FF', 'color': 'black'})\n)\n\n# display the dataframe with the images\ndisplay(styled_df)\n\n\nDisclaimer: Issues with Notebook Cache\nDuring the creation of this content, I encountered difficulties related to the notebook cache.\nDespite making changes to the images, the visualization did not update correctly.\nEven after attempting to restart the kernel and clear the cell output, the problem persisted.\nThe only effective solution I found was to change the file names of the images, thus avoiding unexpected cache behavior.\nIt’s important to note that these issues may be specific to the Jupyter Notebooks environment and may not reflect inherent limitations in the code or libraries used.\nWhile I tried to address this problem, I did not find a complete solution and opted for an alternative fix by changing the file names.\nIf you have suggestions or additional solutions, I would be delighted to learn and improve this process.\n\n🎨 Style: Emoji Representation Based on Percentile Values\nIn this section, we delve into the creative use of emojis based on percentile values, offering a distinctive approach to elevate data representation.\nBy incorporating diverse emojis, we enhance the visual impact of the data.\nSpecifically, we employ circles and squads as emojis to bring nuanced expressions to our data points.\nIf you’d like to view the code for creating this style, it’s available in my GitHub repository.\nFeel free to check it out and give it a star if you find it helpful!\n⭐️\n\n\n\n📚 References\nIf you want to learn… * 🐼 Pandas Style Documentation\nOther references: * Image preview reference: [Imagen de vectorjuice en Freepik]\n\n\n\n",
    "preview": "posts_en/2024-01-02-transform-your-pandas-dataframes/preview.jpg",
    "last_modified": "2024-01-02T23:04:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-12-22-decoding-a-data-model-using-schemaspy-in-snowflake/",
    "title": "Decoding a Data Model - Using SchemaSpy in Snowflake",
    "description": "In following article, we will do on the implementation journey of SchemaPy, leveraging Docker for deployment, and employing a Snowflake database.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2023-12-22",
    "categories": [
      "Python",
      "SQL",
      "Database",
      "Data"
    ],
    "contents": "\n\nContents\nData Model\nConceptual Data Model (CDM)\nLogical Data Model (LDM)\nPhysical data model\n\n❄️Snowflake SchemaSpy 🐳 Docker Compose\n🚀 Benefits of SchemaSpy\nPrerequisites\nUsage\n📁 Clone this repository\n❄️ Snowflake Configuration\n❄️ Snowflake Account\n\n🐙 Build and run the Docker Compose environment\n\n🔍 Schemapy\nFunctionality Tabs\nSchemaSpy UI\n\n📚 References\n\n\nIn this article, we will delve into a comprehensive exploration of the intricacies of data modeling, spanning from its conceptual inception to its logical definition and eventual physical implementation.\nUnderstanding the life cycle of a data model is crucial for efficiently designing and managing databases.\nFurthermore, in real-world scenarios, existing databases often necessitate reverse engineering to unveil and comprehend their underlying structures.\nIn following sections, we will do on the implementation journey of SchemaPy, leveraging Docker for deployment, and employing a Snowflake database.\n\n\n\n\n\n\n\nData Model\nData modeling is a fundamental task that provides us with a clear understanding of data and the most efficient way to store it.\nThis approach involves representing entities and relationships based on business processes, facilitating documentation and the efficient exploration of data.\nThe ability to generate these documents and understand how data is stored is essential knowledge for any data team.\nIn this context, we will delve into the significance and process of data modeling, emphasizing how this practice becomes a valuable tool for the effective management and exploitation of information.\n\nConceptual Data Model (CDM)\nA conceptual data model (CDM) operates at a high level and offers an organization’s data needs.\nIt defines a broad and simplified view of the data and the purpose of conceptual data modeling is to establish a shared understanding of the business by capturing the essential concepts of a business process.\nThe focus is on abstracting and representing key entities, relationships, and their interdependencies, fostering a common understanding among stakeholders about the fundamental aspects of the business and its data requirements.\nBelow is an example of a CDM\n\nLogical Data Model (LDM)\nA logical data model (LDM) extends the conceptual data model by providing a complete definition of relationships, incorporating details and the structure of essential entities.\nIn summary the LDM encompasses specific attributes for each entity and the relationships between entities.\nBelow is an example of a LDM\n\nPhysical data model\nA Physical Data Model (PDM) outlines how the model will be translated into an actual database.\nThe PDM incorporates all necessary physical specifications essential for constructing a database, providing a comprehensive guide for database construction, including tables, columns, data types, indexes, constraints, and other implementation details tailored to the chosen database management system.\n\n❄️Snowflake SchemaSpy 🐳 Docker Compose\nIn this following example, we will implement of SchemaSpy through a Docker image in a Snowflake database.\nIt’s important to note that the implementation can be extended to other databases such as: mysql, PostgreSQL, Oracle, SQL Server, DB2, H2, HSQLDB, SQLite, Firebird, Redshift and Informix.\nSchemaSpy is a tool that generates an HTML representation of a database schema’s relationships, providing insights into the database structure.\nSnowflake is a cloud-based data storage and processing service that provides a highly scalable and fully managed data storage environment.\nIts architecture is built on the separation of storage and computing, allowing elastic scalability and optimal performance.\n🚀 Benefits of SchemaSpy\nVisual representation of the database schema.\nRelationship insights between tables.\nHTML report for easy sharing and documentation.\nPrerequisites\nBefore you begin, ensure that you have the following installed:\n🐳 Docker\n🐙 Docker Compose\n❄️ Snowflake account (You can create an account with a 30-day free trial which includes $400 worth of free usage.)\nUsage\n📁 Clone this repository\nsource:r0mymendez/schemaspy-snowflakegit clone https://github.com/r0mymendez/schemaspy-snowflake.git\ncd schemaspy-snowflake\n❄️ Snowflake Configuration\nAt this stage, you need to configure the configuration file located at the following path: config/snowflake.properties.\nThis configuration will be used to establish the connection to Snowflake, so it is necessary to modify the account, role, warehouse, schema, and database settings.\nschemaspy.t=snowflake\nschemaspy.u=<user>\nschemaspy.p=<password>\nschemaspy.account=<account>\nschemaspy.role=<role\nschemaspy.warehouse=<warehouse>\nschemaspy.db=<database>\nschemaspy.s=<schema>\n❄️ Snowflake Account\nBelow is an example of an account URL.\nHowever, you only need to use a portion of it in the configuration:\nFull account URL: https://%60nl1111.eu-west-3.aws`.snowflakecomputing.com\nAccount to use in the configuration: nl1111.eu-west-3.aws\nImage description🐙 Build and run the Docker Compose environment\nThis command will build the Docker image and start the container.\ndocker-compose -f docker-compose-snowflake.yaml up\n🔍 Schemapy\nAfter executing Docker Compose, you’ll find a web site in the output folder.\nThis page features multiple tabs, each offering distinct functionalities, and below, we will explain them.\nFunctionality Tabs\nTables: Provides an overview of all tables in the database schema.\nColumns: Displays detailed information about columns within each table.\nConstraints: Offers insights into constraints applied to the database.\nRelationships: Visualizes the relationships between different tables.\nOrphan Tables: Identifies tables without established relationships.\nAnomalies: Highlights any irregularities or anomalies in the schema.\nRoutines: Presents information about stored routines or procedures.\nSchemaSpy UI\nIn the provided example, we showcase a demo utilizing a synthetic database called Synthea.\nSynthea is a state-of-the-art synthetic data generation tool designed for creating realistic, yet entirely fictitious, patient records and healthcare data.\nIt enables the simulation of diverse medical scenarios, making it a valuable resource for testing and development purposes in the healthcare domain.\nVisit the following 👉link to access a demo👈.\n\n\n📚 References\nIf you want to learn…\nSnowflake\nSnowflake Free courses\nSchemaSpy\nSchemaSpy: Docker Image\nSynthea Project\nOther references:\n- Image preview reference: [Imagen de vectorjuice en Freepik]\n\n\n\n",
    "preview": "posts_en/2023-12-22-decoding-a-data-model-using-schemaspy-in-snowflake/preview.jpg",
    "last_modified": "2023-12-24T18:31:23+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-12-17-sql-to-python-pandas-a-sql-users-quick-guide/",
    "title": "SQL to Python Pandas: A SQL User's Quick Guide",
    "description": "Unlock the essentials of translating your code from SQL to Python with this quick guide tailored for SQL users. Dive into key insights and streamline your data manipulation process.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2023-12-17",
    "categories": [
      "Python",
      "SQL",
      "Database",
      "Data"
    ],
    "contents": "\n\nContents\nNew York Flights ✈️ 🧳 🗽\nEntity-relationship diagram [DER]\nInstallation: Setting Up nycflights13\n\n🟢 Pandas, NumPy, and nycflights13 for Data Analysis in Python\n🟢 SELECT and FROM Statements\n📗 SELECT: All Columns\n📗 SELECT: Specific Columns\n\n🟢 Filtering Operators (WHERE)\n📗 Utilizing ‘WHERE’ for Equality ( = )\n📗 Utilizing ‘WHERE’ for Equality ( = )\n📗 Utilizing ‘WHERE’ with Inequality ( != )\n📗 Utilizing ‘WHERE’ for Comparisons (>=, <=, <, >)\n📗 Utilizing ‘WHERE’ with between operator\n📗 Utilizing ‘WHERE’ with “LIKE” Clause\n📗 Utilizing ‘WHERE’ with Null or Not Null Values\n\n🟢 Order by Statement\n🟢 Distinct Values: Removing Duplicates from Results\n🟢 Adding Calculated Columns\n🟢 Group by Statement\n🟢 Group by and Having Statement\n🟢 Group by with multiple calculations\n🟢 Union Statement\n🟢 CASE WHEN Statement\n🟢 JOIN Statement\n📗 Join Types\n📗 Join Key\n\n🟢 Rename\n📚 References\n\n\nIn this post, we will compare the implementation of Pandas and SQL for data queries.\nWe’ll explore how to use Pandas in a manner similar to SQL by translating SQL queries into Pandas operations.\nIt’s important to note that there are various ways to achieve similar results, and the translation of SQL queries to Pandas will be done by employing some of its core methods.\nNew York Flights ✈️ 🧳 🗽\n\nsource image Image by upklyak on Freepik\nWe aim to explore the diverse Python Pandas methods, focusing on their application through the nycflights13 datasets.\nThis datasets offer comprehensive information about airlines, airports, weather conditions, and aircraft for all flights passing through New York airports in 2013.\nThrough this exercise, we’ll not only explore Pandas functionality but also learn to apply fundamental SQL concepts in a Python data manipulation environment.\nEntity-relationship diagram [DER]\n\nThe nycflights13 library contains tables with flight data from New York airports in 2023.\nBelow, you can find a high-level representation of an entity-relationship diagram with its five tables.\n\n\nInstallation: Setting Up nycflights13\nTo install the nycflights13 library, you can use the following command:\n!pip install nycflights13\nThis library provides datasets containing comprehensive information about flights from New York airports in 2023.\nOnce installed, you can easily access and analyze this flight data using various tools and functionalities provided by the nycflights13 package.\n🟢 Pandas, NumPy, and nycflights13 for Data Analysis in Python\nIn the next code snippet, we are importing essential Python libraries for data analysis.\n* 📗 Pandas is a library for data manipulation and analysis, * 📗 Numpy provides support for numerical operations * 📗 Nycflights13 is a specialized library containing datasets related to flights from New York airports in 2023.\nimport pandas as pd\nimport numpy as np\nimport nycflights13 as nyc\nIn the following lines of code, we are assigning two specific datasets from the nycflights13 library to variables.\nflights = nyc.flights\nairlines = nyc.airlines\n🟢 SELECT and FROM Statements\n📗 SELECT: All Columns\nThe following SQL query retrieves all columns and rows from the “🛩️ flights” table.\nIn Pandas, the equivalent is simply writing the DataFrame name, in this case, “flights.” For example:\n🔍sql\n  SELECT * FROM flights;\n🐍python\nflights\n📗 SELECT: Specific Columns\nTo select specific columns from a Pandas DataFrame, you can use the following syntax:\n🔍sql\n  select \n    year, \n    month, \n    day, \n    dep_time, \n    flight, \n    tailnum, \n    origin, \n    dest \n  from flights;\n🐍python\n(\n    flights\n        .filter(['year', 'month', 'day', 'dep_time', 'flight', \n                'tailnum', 'origin', 'dest'])\n)\n🟢 Filtering Operators (WHERE)\n📗 Utilizing ‘WHERE’ for Equality ( = )\nTo filter all ✈️ flights where the origin is ‘JFK’ in Pandas, you can use the following code:\n🔍sql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights \n  where origin = 'JFK'\nlimit 10;\n🐍python\n(   flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest'])\n      .query(\"origin=='JFK'\")\n      .head(10)\n)\n📗 Utilizing ‘WHERE’ for Equality ( = )\nTo achieve the same filtering in Pandas for specific criteria: * ✈️ Flights departing from JFK, LGA, or EWR.\n🔍sql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights \n  where origin in ( 'JFK', 'LGA', 'EWR' ) \nlimit 10;\n🐍python\n (  flights\n        .filter(['year', 'month', 'day', 'dep_time', 'flight', \n        'tailnum', 'origin', 'dest'])\n      .query(\"origin in ['JFK', 'EWR', 'LGA']\")\n      .head(10)\n)\n📗 Utilizing ‘WHERE’ with Inequality ( != )\nTo achieve the same filtering in Pandas for specific criteria:\n✈️ Flights departing from JFK, LGA, or EWR.\n✈️ Flights not destined for Miami (MIA).\n🔍sql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' ) and dest<>'MIA'\nlimit 10;\n🐍python\n(   flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest'])\n      .query(\"(origin in ['JFK', 'EWR', 'LGA']) and (dest != 'MIA')\")\n   .head(10)\n)\n📗 Utilizing ‘WHERE’ for Comparisons (>=, <=, <, >)\nTo achieve the same filtering in Pandas for specific criteria:\n✈️ Flights departing from JFK, LGA, or EWR.\n✈️ Flights not destined for Miami (MIA).\n✈️ Flights with a distance less than or equal to 1000 km.\n🔍sql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and distance < = 1000\nlimit 10;\n🐍python\n( flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight',\n            'tailnum', 'origin', 'dest', 'time_hour', 'distance'])\n      .query(\"(origin in ['JFK', 'EWR', 'LGA']) and (dest != 'MIA') and (distance <= 1000)\")\n      .head(10)\n)\n📗 Utilizing ‘WHERE’ with between operator\nTo achieve the same filtering in Pandas for specific criteria:\n✈️ Flights departing from JFK, LGA, or EWR.\n✈️ Flights not destined for Miami (MIA).\n✈️ Flights with a distance less than or equal to 1000 km.\n✈️ Flights within the period from September 1, 2013, to September 30, 2013.\n\n🔍sql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and distance < = 1000\n   and time_hour between '2013-09-01' and '2012-09-30'\nlimit 10;\n🐍python\n(   flights.filter([['year', 'month', 'day', 'dep_time', 'flight', \n          'tailnum', 'origin', 'dest', 'time_hour', 'distance'])\n      .query(\n            \"(origin in ['JFK', 'EWR', 'LGA'])\" \n             \" and (dest != 'MIA')\"\n             \" and (distance <= 1000)\"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n         )\n      .head(10)\n)\n📗 Utilizing ‘WHERE’ with “LIKE” Clause\nTo achieve the same filtering in Pandas for specific criteria:\n✈️ Flights departing from JFK, LGA, or EWR.\n✈️ Flights not destined for Miami (MIA).\n✈️ Flights with a distance less than or equal to 1000 km.\n✈️ Flights within the period from September 1, 2013, to September 30, 2013.\n✈️ Flights where the tailnum contains ‘N5’ in the text.\nYou can use the following code:\n🔍sql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and distance < = 1000\n   and time_hour between '2013-09-01' and '2012-09-30'\n   and tailnum like '%N5%'\nlimit 10;\n🐍python\n(\n    flights     \n      .filter(['year', 'month', 'day', 'dep_time', 'flight', 'tailnum',\n        'origin', 'dest', 'time_hour'])\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and (dest != 'MIA') \"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n             \" and (tailnum.str.find('N5')>=0)\"\n       )\n      .head(10)\n)\n📗 Utilizing ‘WHERE’ with Null or Not Null Values\nTo achieve the same filtering in Pandas for specific criteria:\n✈️ Flights departing from JFK, LGA, or EWR.\n✈️ Flights not destined for Miami (MIA).\n✈️ Flights with a distance less than or equal to 1000 km.\n✈️ Flights within the period from September 1, 2013, to September 30, 2013.\n✈️ Flights where the tailnum contains ‘N5’ in the text.\n✈️ Flights where dep_time is null\nYou can use the following code:\n🔍sql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and distance < = 1000\n   and time_hour between '2013-09-01' and '2012-09-30'\n   and tailnum like '%N5%'\n   and dep_time is null\nlimit 10;\n🐍python\n(\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest', 'time_hour'])\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and (dest != 'MIA') \"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n             \" and (tailnum.str.find('N5')>=0)\"\n             \" and dep_time.isnull()\"\n       )\n      .head(10)\n)\n🟢 Order by Statement\nThe .sort_values() methods in Pandas are equivalent to the ORDER BY clause in SQL.\n1️⃣.**.sort_values(['origin','dest'], ascending=False)**: This method sorts the DataFrame based on the ‘origin’ and ‘dest’ columns in descending order (from highest to lowest).\nIn SQL, this would be similar to the ORDER BY origin DESC, dest DESC clause.\n2️⃣.**.sort_values(['day'], ascending=True)**: This method sorts the DataFrame based on the ‘day’ column in ascending order (lowest to highest).\nIn SQL, this would be similar to the ORDER BY day ASC clause.\nBoth methods allow you to sort your DataFrame according to one or more columns, specifying the sorting direction with the ascending parameter.\nTrue means ascending order, and False means descending order.\n🔍sql\nselect year, month , day, dep_time, flight, tailnum, origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and distance < = 1000\n   and time_hour between '2013-09-01' and '2012-09-30'\n   and tailnum like '%N5%'\n   and dep_time is null\norder by  origin, dest desc\nlimit 10;\n🐍python\n(\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest', 'time_hour'])\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and (dest != 'MIA') \"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n             \" and (tailnum.str.find('N5')>=0)\"\n             \" and year.notnull()\"\n       )\n      .sort_values(['origin','dest'],ascending=False)\n      .head(10)\n      \n)\n🟢 Distinct Values: Removing Duplicates from Results\nTo perform a distinct select in pandas, you need to first execute the entire query, and then apply the drop_duplicates() method to eliminate all duplicate rows.\n🔍sql\nselect distinct origin, dest\nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and time_hour between '2013-09-01' and '2012-09-30'\norder by  origin, dest desc;\n🐍python\n(\n    flights\n      .filter(['origin','dest','time_hour'])\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and (dest != 'MIA') \"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n       )\n      .filter(['origin','dest'])\n      .drop_duplicates()\n      \n)\n🟢 Adding Calculated Columns\nNow, let’s introduce a new calculated column called “delay_total,” where we sum the values from the “dep_delay” and “arr_delay” columns.\n🔍sql\nselect  \n  origin,\n  dest,\n  time_hour,\n  dep_delay,\n  arr_delay,\n  flights.dep_delay + flights.arr_delay as delay_total \nfrom flights  \n  where origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and time_hour between '2013-09-01' and '2012-09-30';\n🐍python\n(\n    flights\n      .filter(['origin', 'dest', 'time_hour', 'dep_delay', 'arr_delay'])\n      .assign(delay_total = flights.dep_delay + flights.arr_delay )\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and (dest != 'MIA') \"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n       )\n)\n🟢 Group by Statement\nTo perform a GROUP BY operation in pandas, we’ll use the groupby method, which operates similarly to its SQL counterpart.\nSimilarly, we can employ common aggregate functions such as sum, max, min, mean (equivalent to avg in SQL), and count.\nBelow is a simple example to illustrate this process:\n🔍sql\nselect \n  year,\n  month,\n  max(dep_delay) as dep_delay,\nfrom flights\ngroup by \n  year,\n  month;\n🐍python\n(\n    flights\n      .groupby(['year','month'],as_index=False)\n      ['dep_delay'].max()\n)\n🟢 Group by and Having Statement\nIn the following example, we’ll explore how to implement a HAVING clause in pandas, leveraging the query method, as we’ve done previously for filtering.\n🔍sql\nselect \n  year,\n  month,\n  max(dep_delay) as dep_delay,\nfrom flights\ngroup by \n  year,\n  month\nhaving max(dep_delay)>1000\n🐍python\n(\n    flights\n      .groupby(['year','month'],as_index=False)['dep_delay']\n      .max()\n      .query('(dep_delay>1000)') # having\n)\n🟢 Group by with multiple calculations\nWhen working with pandas and needing to perform multiple calculations on the same column or across different columns, the agg function becomes a valuable tool.\nIt allows you to specify a list of calculations to be applied, providing flexibility and efficiency in data analysis.\nConsider the following SQL query:\n🔍sql\nselect \n  year,\n  month,\n  max(dep_delay)  as dep_delay_max,\n  min(dep_delay)  as dep_delay_min,\n  mean(dep_delay) as dep_delay_mean,\n  count(*)        as dep_delay_count,\n  max(arr_delay)  as arr_delay_max,\n  min(arr_delay)  as arr_delay_min,\n  sum(arr_delay)  as arr_delay_sum\nfrom flights\ngroup by \n  year,\n  month\n\nThis query retrieves aggregated information from the “flights” dataset, calculating various statistics like maximum, minimum, mean, count, and sum for both “dep_delay” and “arr_delay” columns.\nTo achieve a similar result in pandas, we use the agg function, which allows us to specify these calculations concisely and efficiently.\nThe resulting DataFrame provides a clear summary of the specified metrics for each combination of “year” and “month.”\n\n🐍python\nresult = (\n    flights\n      .groupby(['year','month'],as_index=False)\n      .agg({'dep_delay':['max','min','mean','count'], 'arr_delay':['max','min','sum']})     \n)\n\n# Concatenate function names with column names\nresult.columns = result.columns.map('_'.join)\n\n# Print the results\nresult\n🟢 Union Statement\nTo execute a UNION ALL operation in Pandas, it is necessary to create two DataFrames and concatenate them using the concat method.\nUnlike SQL, a DataFrame in Pandas can be combined to generate additional columns or additional rows.\nTherefore, it is essential to define how the concatenation should be performed:\naxis=1 => Union that appends another dataset to the right, generating more columns.\naxis=0 => Union that appends more rows.\n\nIn our example, we will perform the equivalent of a UNION ALL in SQL, so we will use axis=0.\n🔍sql\nselect  \n  origin,\n  dest,\n  time_hour,\n  dep_delay,\n  arr_delay,\n  flights.dep_delay + flights.arr_delay as delay_total ,\n  'NYC' group\nFROM flights  \n  WHERE origin in ( 'JFK', 'LGA', 'EWR' )\n   and dest<>'MIA' \n   and time_hour between '2013-09-01' and '2012-09-30'\nORDER BY flights.dep_delay + flights.arr_delay DESC\nLIMIT 3\nUNION ALL\nselect  \n  origin,\n  dest,\n  time_hour,\n  dep_delay,\n  arr_delay,\n  flights.dep_delay + flights.arr_delay as delay_total ,\n  'MIA' group\nFROM flights  \n  WHERE origin in ( 'JFK', 'LGA', 'EWR' ) \n   and time_hour between '2013-07-01' and '2012-09-30'\n  ORDER BY flights.dep_delay + flights.arr_delay DESC\n  LIMIT 2;\n🐍python\nFlights_NYC = (\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight',\n              'tailnum', 'origin', 'dest', 'time_hour',\n              'dep_delay', 'arr_delay'])\n      .assign(delay_total = flights.dep_delay + flights.arr_delay )\n      .query(\n             \" (origin in ['JFK', 'EWR', 'LGA'])\"\n             \" and ('2013-09-01' <= time_hour <= '2013-09-30')\"\n       )\n     .assign(group ='NYC')      \n     .sort_values('delay_total',ascending=False)     \n     .head(3)\n)\n\nFlights_MIAMI = (\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest', 'time_hour', \n              'dep_delay', 'arr_delay'])\n      .assign(delay_total = flights.dep_delay + flights.arr_delay )\n      .query(\n             \" (dest in ['MIA', 'OPF', 'FLL'])\"\n             \" and ('2013-07-01' <= time_hour <= '2013-09-30')\"\n       )\n     .assign(group ='MIA') \n     .sort_values('delay_total',ascending=False)     \n     .head(2)\n)\n\n# union all \npd.concat([ Flights_NYC,Flights_MIAMI],axis=0)\n🟢 CASE WHEN Statement\nTo replicate the CASE WHEN statement, we can use two different methods from NumPy:\n1️⃣.\nIf there are only two conditions, for example, checking if the total delay exceeds 0, then we label it as “Delayed”; otherwise, we label it as “On Time”.\nFor this, the np.where method from NumPy is utilized.\n🔍sql\nselect  \n  origin,\n  dest,\n  time_hour,\n  dep_delay,\n  arr_delay,\n  (case \n    when flights.dep_delay + flights.arr_delay >0 then 'Delayed'\n    else 'On Time' end) as status ,\nFROM flights  \nLIMIT 5;\n\n🐍python\n(\n    flights\n      .filter(['year', 'month', 'day', 'dep_time',\n              'flight', 'tailnum', 'origin', 'dest', \n              'time_hour', 'dep_delay', 'arr_delay'])\n      .assign(status=np.where((flights['dep_delay'] + flights['arr_delay']) > 0,                                'Delayed',\n                               'On Time'))\n      .head(5)\n)\n \n2️⃣.\nIn case there are more conditions, such as identifying Miami airports and labeling them as “MIA”, labeling “ATL” airports that they are in Altanta, and for any other cases, using the label “OTHER”.\nFor this, the np.select method from NumPy is employed.\nCity\nName\nAcronym\nMiami\nMiami International\n(MIA)\nMiami\nOpa-locka Executive\n(OPF)\nMiami\nFort Lauderdale-Hollywood\n(FLL)\nAtlanta\nHartsfield-Jackson Atlanta\n(ATL)\nAtlanta\nDeKalb-Peachtree\n(PDK)\nAtlanta\nFulton County\n(FTY)\n🔍sql\nselect  \n  origin,\n  dest,\n  time_hour,\n  dep_delay,\n  arr_delay,\n  (case \n    when dest in ('ATL','PDK','FTY') then 'ATL'\n    when dest in ('MIA','OPF','FLL') then 'MIA'\n    else 'Other'\n  end) as city ,\nFROM flights  \nLIMIT 10;\n🐍python\n(\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n              'tailnum', 'origin', 'dest', 'time_hour',\n              'dep_delay', 'arr_delay'])\n      .assign( city=np.select([flights['dest'].isin(['ATL','PDK','FTY']), \n                             flights['dest'].isin(['MIA', 'OPF', 'FLL'])],\n                             ['ATL','MIA'],\n                             default='Other')\n              )\n    .head(10)\n)\n🟢 JOIN Statement\nEntity relationship diagram [DER]\n\nWhen performing a join in Pandas, the merge method should be used.\n📗 Join Types\nHow: Specifies the type of join to be performed.\nAvailable options: {'left', 'right', 'outer', 'inner', 'cross'}\njoins📗 Join Key\nOn: The key on which the tables will be joined.\nIf more than one column is involved, a list should be provided.\nExamples:\nSingle variable: on='year'\nfligths.merge(planes, how='inner', on='tailnum')\nTwo variables: on=[‘year’,‘month’,‘day’]\nfligths.merge(weather, how='inner', on=['year','month','day'])\nleft_on/right_on: When the columns have different names, these parameters should be used. For example:\nfligths.merge(airports, how='inner', left_on = 'origin', rigth_on='faa')\nHere’s an example using the airlines and flights tables:\n🔍sql\nselect  \n  f.year,\n  f.month,\n  f.day,\n  f.dep_time,\n  f.flight,\n  f.tailnum,\n  f.origin as airport_origen,\n  f.dest,\n  f.time_hour,\n  f.dep_delay,\n  f.arr_delay,\n  f.carrier,\n  a.name as airline_name\nFROM flights  f\n  left join airlines a on f.carrier = a.carrier\nLIMIT 5;\n🟢 Rename\n\nThe rename method is used to rename columns, similar to the “as” clause in SQL.\n\n🐍python\n(\n    flights\n      .filter(['year', 'month', 'day', 'dep_time', 'flight', \n        'tailnum', 'origin', 'dest', 'time_hour', 'dep_delay', \n        'arr_delay', 'carrier'])\n      .merge(airlines, how = 'left', on ='carrier')\n      .rename(columns= {'name':'airline_name', 'origin':'airport_origen'})\n      .head(5)\n)\n \n\nYou can find all the code in a 🐍 python notebook in the following [link]\n\n📚 References\nIf you want to learn more about `Pandas` and `NumPy`…\n- [Pandas]\n- [NumPy]\nOther references:\n- Image preview reference: [Imagen de vectorjuice en Freepik]\n\n\n\n",
    "preview": "posts_en/2023-12-17-sql-to-python-pandas-a-sql-users-quick-guide/preview.jpg",
    "last_modified": "2023-12-24T18:28:45+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-12-10-code-quality-sonarqube/",
    "title": "Code Quality - SonarQube",
    "description": "Code Quality, crucial for robust software, is upheld by tools like SonarQube.  This article explores its significance, implementation, and management.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2023-12-10",
    "categories": [
      "Python"
    ],
    "contents": "\n\nContents\nWhat is a code quality?\nWhat is Clean Code?\nWhat is a SonarQube?\nWhat is a SonarLindt?\nSonarQube vs SonarLindt\nSonarQube Features\n📜 Rules\n📜 Quality Profiles\n📜 Quality Gates\n\n🟣 Using SonarQube with Docker: A Step-by-Step Guide\n🐳 Install Docker:\n🐳 Pull the SonarQube Image\n🐳 Run SonarQube Container:\n🐳 Check if the container is running\n🚀 Open the sonarqube web application\n📄 Create the script in python\n📄 Create a configuration file\n📄 Code folder\n📁 Create sonarqube project\n🔍 Create a quality gate\n🔍 Start the scanning\n🔍 Scan Results\n📊 Sonarqube Metrics\n🚧 SonarQube Issues\n\n🟣 Sonarqube API\nKey Features of the SonarQube API:\nCommon Use Cases:\n\n🟣 SonarQube API: A Step-by-Step Guide\nInstall SonarQubeClient\nConfig the sonarqube client\nGet sonarqube client projects\nGet the project events\n\n📚 References\n\n\nWhat is a code quality?\nCode quality measures the accuracy and reliability of code—but being bug-free and portable is not the only measure of code quality.\nIt also includes how developer-friendly the code is.\nCode quality also describes how easy it is to understand, modify, and reuse the code if necessary.\nTestable: A piece of code should be easy to develop tests for and run tests on.\nPortable: You might want it to be easy to take the code from one environment the make it work again in another environment. If so, you can measure portability.\nReusable: High-quality code is modular and designed for reuse.\nsource: Amazon: What is code quality?\n\nImproving code quality involves addressing these factors to create code that is not only technically robust but also user-friendly and conducive to collaborative development.\n\nWhat is Clean Code?\n\n“Clean Code” is code that’s easy to read, maintain, understand and change through structure and consistency yet remains robust and secure to withstand performance demands.\"\nsource: clean code](https://www.sonarsource.com/solutions/clean-code/))\n\n🧹 Clean Code refers to the practice of writing code in a clear, readable and efficient manner, placing a strong emphasis on the understandability and maintainability of the code.\nThe main premise is that the code must not only work correctly, but it must also be easy to understand for any developer who reads it so that it can be reused quickly.\nTo achieve this, it is important to follow good programming practices and adopt conventions that promote clarity.\nAspects to consider:\n📖 Readability: Code should be written in a way that is easily understandable.\nDescriptive names should be used for variables and functions, and confusing abbreviations should be avoided.\n🚀 Simplicity: Simplicity is sought in the design and structure of the code.\nAvoiding unnecessary complexity and keeping functions and methods concise helps facilitate understanding.\n🛠️ Maintainability: The code must be easy to maintain over time.\nThis involves minimizing code duplication, following sound design principles, and documenting effectively.\n🔄 Consistency: Consistent coding conventions should be followed throughout the project to improve consistency and make the code easier to read.\nWhat is a SonarQube?\nSonarQube is a self-managed, automatic code review tool that systematically helps you deliver Clean Code.\nAs a core element of our Sonar solution, SonarQube integrates into your existing workflow and detects issues in your code to help you perform continuous code inspections of your projects.\nsource: SonarQube\nWhat is a SonarLindt?\nSonarLint is a free IDE extension that can be used in Visual Code, Visual studio or eclipse.\nThis plugin allows you to identify coding problems in real time, in order to avoid errors, vulnerabilities and code smells while you write your code.\nSonarLint can perform code analysis in JS/TS, Python, PHP, Java, C, C++, Go and IaC.\nSonarQube vs SonarLindt\nThe following is a comparative table in which we compare the functionality and the context in which each of these tools is applied\nSonarQube - SonarLindtimage source:sonarsource docs\nFeature\nSonarQube\nSonarLint\nScope\nServer-based code analysis for entire projects/repositories\nIDE-based code analysis for individual developers\nDeployment\nRequires a centralized server installation\nIntegrated directly into the developer’s IDE\nReal-time Feedback\nProvides feedback during continuous integration\nOffers real-time feedback within the developer’s IDE\nCode Analysis Depth\nOffers in-depth static code analysis and metrics\nProvides on-the-fly code analysis with immediate feedback\nIntegration with CI/CD\nIntegrates with CI/CD pipelines for automated analysis\nSupports local analysis as well as CI/CD integration\nRule Configurability\nHighly configurable rules for code quality and security\nLimited rule configuration options within the IDE\nCollaboration\nFacilitates collaboration among development teams\nFocuses on individual developer experience and collaboration\nUse Case\nSuitable for larger projects with centralized management\nIdeal for individual developers or smaller development teams\nSonarQube Features\n📜 Rules\nIn SonarQube, “rules” are definitions that describe code patterns that indicate potential problems, security vulnerabilities, or areas for improvement in code quality.\nThe SonarQube analysis engine uses these rules to scan the source code and highlight potential problems.\nEach rule has a definition that allows a specific pattern to be identified and covers aspects such as: good practices, errors, vulnerability, security, among others.\n\n📜 Quality Profiles\nQuality profiles are a set of specific and organized rules that apply to specific projects.\nThese profiles allow you to customize the rules you want to use to evaluate code quality based on your specific needs and standards.\nTherefore, profiles allow you to customize which rules apply to a project and provide predefined profiles for different programming languages.\n\n\n📜 Quality Gates\nQuality Gates are sets of conditions that are applied to a project after running a static analysis of the code and applying the rules defined in the quality profiles.\nThese conditions allow you to quantify and evaluate whether a project meets specific quality criteria, helping to determine if the code is acceptable for implementation.\nMetric\nDescription\nReliability Rating\nThis indicator evaluates the reliability of the code, which means how prone the code is to contain errors or defects.\nSecurity Rating\nThis indicator evaluates the security level of the code, which means how prone the code is to contain security vulnerabilities.\nSecurity Hotspots Reviewed\nThis indicator evaluates whether all security points identified in the code have been reviewed.\nMaintainability Rating\nThis indicator evaluates the ease with which the code can be maintained and improved in the future\nCoverage\nThis indicator examines the % of code that has been executed.\nDuplicated Lines (%)\nThis flag checks for duplicate lines in the code\n\n🟣 Using SonarQube with Docker: A Step-by-Step Guide\n🐳 Install Docker:\nEnsure that Docker is installed on your machine.\nYou can download and install Docker from the official website: Docker\n🐳 Pull the SonarQube Image\nOpen a terminal and run the following command to pull the official SonarQube Docker image from Docker Hub:\ndocker pull sonarqube\n🐳 Run SonarQube Container:\nExecute the following command in a terminal to run the sonarqube container.\ndocker run \n   -d --name sonarqube \n   -e SONAR_ES_BOOTSTRAP_CHECKS_DISABLE=true \n   -p 9000:9000 \n   sonarqube:latest\nCommand\nDescription\ndocker run\nThis is the command used to run a Docker container.\n-d\nThis is a Docker run option that stands for “detached.” It runs the container in the background, which means you get your terminal prompt back immediately after starting the container.\n--name sonarqube\nThis option allows you to specify a name for the container. In this case, the name “sonarqube” is given to the container, which makes it easier to reference the container later.\n-e SONAR_ES_BOOTSTRAP_CHECKS_DISABLE=true\nThis option is used to set an environment variable within the container. It disables Elasticsearch bootstrap checks when starting SonarQube.\n-p 9000:9000\nThis option is used to map ports between the host and the container. It specifies that port 9000 on the host should be mapped to port 9000 inside the container, allowing access to SonarQube.\nsonarqube:latest\nThis is the Docker image to run. It specifies the image named “sonarqube” and the “latest” tag, pulling the latest version from Docker Hub and creating a container from that image.\n🐳 Check if the container is running\n$ docker ps\nCONTAINER ID   IMAGE              COMMAND                  CREATED          STATUS         PORTS                                       NAMES\nd8e576b6039e   sonarqube:latest   \"/opt/sonarqube/dock…\"   13 seconds ago   Up 8 seconds   0.0.0.0:9000->9000/tcp, :::9000->9000/tcp   sonarqube\n🚀 Open the sonarqube web application\n‘🔗 Link: localhost:9000’\n\n🔐 Initial user and password login: admin password: admin\n\n\n📄 Create the script in python\nThis Python🐍 code was created using chat-gpt intentionally includes some practices that may violate default SonarQube configurations.\nIn addition, we will duplicate this file in the same folder to be able to generate an alert for duplicate code by generating the same file with the name main_bk\nmain.py\n# Code Smell: Unused variable\nunused_variable = 42\n\n# Code Smell: Unused function\ndef unused_function():\n    pass\n\n# Code Smell: Redundant parentheses\nresult = (5 * 3)\n\n# Code Smell: Unused import\nimport unused_module\n\n# Code Smell: Print statement (considered a bad practice)\nprint(\"Hello, World!\")\n\n# Code Smell: Hardcoded values\nmagic_number = 42\n\n# Code Smell: Unused loop variable\nfor _ in range(5):\n    pass\n\n# Code Smell: Assignment in a condition\nif (result == 0):  # Fix the equality check\n    pass\n\n# Code Smell: Using a single underscore as a variable name\n_ = \"Unused variable\"\n\n# Code Smell: Using a mutable default argument in a function\ndef append_item(item, my_list=None):\n    if my_list is None:\n        my_list = []\n    my_list.append(item)\n    return my_list\n\n# Code Smell: Unused variable in an exception block\ntry:\n    value = int(\"text\")\nexcept ValueError as e:\n    unused_exception_variable = e\n\n# Code Smell: Complex lambda function\nsquare = lambda x: x**2 + 2*x + 1\n📄 Create a configuration file\nThe following file contains the properties for execute the code quality processes in sonarqube.\nIt is necesary to change the projectkey and the project name, in my case the both name is “test”, this parameters is config when you create the projects in SonarQube web application.\nFilename: “sonar-project.properties”\n# must be unique in a given SonarQube instance\nsonar.projectKey=test\n\n# --- optional properties ---\n\n# defaults to project key\nsonar.projectName=test\n\n# defaults to 'not provided'\nsonar.projectVersion=1.0\n \n# Path is relative to the sonar-project.properties file. Defaults to .\nsonar.sources=.\nsonar.language=python\n\n#----- Default SonarQube server\nsonar.host.url=http://localhost:9000 \n\n# Encoding of the source code. Default is default system encoding\nsonar.sourceEncoding=UTF-8\n📄 Code folder\nIn summary, the folder that should contain the following files\n📄 main.py\n📄 main_bk.py\n📄 sonar-project.properties\n📁 Create sonarqube project\nThe images below illustrate all the steps necessary to create the project and obtain the token for the scanner later.\n\n\n\n\n\n🔍 Create a quality gate\nClick on \"Create\" and define the name of the quality gate.\n\n\nClick on \"Unlock Editing\" to update the condition metrics.\n\nSet the current quality gate as the default\n\n🔍 Start the scanning\nTo start the scanning process using the SonarQube CLI, execute the following command after replacing placeholders with your specific information.\nEnsure that this command is run in the terminal where the source path, containing the ‘sonar-project.properties’ file, is located.\nIt’s crucial to set your token as an environment variable using the following syntax: -e SONAR_LOGIN=“your_token_here”.\nFor example, if your token is “sqp_08ad32fcb385e8192b1a4e0aabdc54be3b1ad946” the corresponding command to be executed would be:\ndocker run --network=host \n -e SONAR_HOST_URL=http://host.docker.internal:9000 \n -e SONAR_LOGIN=\"sqp_08ad32fcb385e8192b1a4e0aabdc54be3b1ad946\" \n -e SONAR_PROJECT_KEY=data-quality \n -it -v \"$(pwd):/usr/src\" \n sonarsource/sonar-scanner-cli\n\nIn the terminal you will see the following log\n\nDigest: sha256:494ecc3b5b1ee1625bd377b3905c4284e4f0cc155cff397805a244dee1c7d575\nStatus: Downloaded newer image for sonarsource/sonar-scanner-cli:latest\nINFO: Scanner configuration file: /opt/sonar-scanner/conf/sonar-scanner.properties\nINFO: Project root configuration file: /usr/src/sonar-project.properties\nINFO: SonarScanner 5.0.1.3006\nINFO: Java 17.0.8 Alpine (64-bit)\nINFO: Linux 5.10.25-linuxkit amd64\nINFO: User cache: /opt/sonar-scanner/.sonar/cache\nINFO: Analyzing on SonarQube server 10.3.0.82913\nINFO: Default locale: \"en_US\", source code encoding: \"UTF-8\"\nINFO: Load global settings\nINFO: Load global settings (done) | time=624ms\nINFO: Server id: 147B411E-AYxB7bsnEO8aoeQvN3oK\nINFO: User cache: /opt/sonar-scanner/.sonar/cache\nINFO: Load/download plugins\nINFO: Load plugins index\nINFO: Load plugins index (done) | time=453ms\nINFO: Load/download plugins (done) | time=5843ms\nINFO: Process project properties\nINFO: Process project properties (done) | time=42ms\nINFO: Execute project builders\nINFO: Execute project builders (done) | time=10ms\nINFO: Project key: data-quality\nINFO: Base dir: /usr/src\nINFO: Working dir: /usr/src/.scannerwork\nINFO: Load project settings for component key: 'data-quality'\nWARN: SCM provider autodetection failed. Please use \"sonar.scm.provider\" to define SCM of your project, or disable the SCM Sensor in the project settings.\nINFO: Load quality profiles\nINFO: Load quality profiles (done) | time=3356ms\nINFO: Load active rules\nwhen the process finish you can see the following log\nINFO: ------------- Run sensors on project\nINFO: Sensor Analysis Warnings import [csharp]\nINFO: Sensor Analysis Warnings import [csharp] (done) | time=3ms\nINFO: Sensor Zero Coverage Sensor\nINFO: Sensor Zero Coverage Sensor (done) | time=14ms\nINFO: SCM Publisher No SCM system was detected. You can use the 'sonar.scm.provider' property to explicitly specify it.\nINFO: CPD Executor Calculating CPD for 0 files\nINFO: CPD Executor CPD calculation finished (done) | time=0ms\nINFO: Analysis report generated in 273ms, dir size=137.8 kB\nINFO: Analysis report compressed in 306ms, zip size=17.4 kB\nINFO: Analysis report uploaded in 412ms\nINFO: ANALYSIS SUCCESSFUL, you can find the results at: http://localhost:9000/dashboard?id=test\nINFO: Note that you will be able to access the updated dashboard once the server has processed the submitted analysis report\nINFO: More about the report processing at http://localhost:9000/api/ce/task?id=AYxTnq5AFtsPP8-M5c1w\nINFO: Analysis total time: 18.832 s\nINFO: ------------------------------------------------------------------------\nINFO: EXECUTION SUCCESS\nINFO: ------------------------------------------------------------------------\nINFO: Total time: 27.819s\nINFO: Final Memory: 21M/80M\nINFO: --------------------------------------------------------\n🔍 Scan Results\nUpon completion of the code scan, you can view the results of the code analysis on the web application at localhost:9000\n📊 Sonarqube Metrics\nThe following table presents the metrics defined by SonarQube, which are objective indicators designed to evaluate the quality of the source code.\nThese metrics allow a quantitative evaluation of various critical aspects of the code.\nMetric\nDescription\nReliability\nThe “Reliability” metric refers to the reliability of the code. It measures the number of issues related to software reliability, such as errors and failures.\nMaintainability\nThe “Maintainability” metric in SonarQube assesses how easy it is to maintain and enhance the code over time. It measures code quality in terms of structure, readability, and ease of maintenance.\nNew Code Smells\nThis metric indicates the number of new “code smells” introduced in the code recently. “Code smells” are design patterns or coding practices that may indicate issues with code quality.\nSecurity\nThe “Security” metric in SonarQube evaluates code security for potential vulnerabilities. It measures the code’s ability to resist attacks and protect data and systems.\nNew Vulnerabilities\nIt indicates the number of new security vulnerabilities introduced in the code recently. Vulnerabilities are weaknesses that can be exploited by attackers.\nSecurity Review\nThis metric in SonarQube assesses the quality of security reviews conducted on the code. It measures the effectiveness of reviews in detecting and correcting security issues.\nNew Security Hotspots\nIt signifies the number of new “security hotspots” introduced in the code recently. Security hotspots are areas of the code that require special attention due to potential security issues.\nCoverage\nThe “Coverage” metric in SonarQube refers to code coverage. It measures the proportion of code that has been tested through unit tests or automated tests.\nDuplications\nThis metric in SonarQube identifies sections of code that are duplicated in multiple places. Identifying and reducing duplications can improve code quality and maintainability.\nThe result of the scan is the following\n\n🚧 SonarQube Issues\n“SonarQube Issues” refer to issues identified by the SonarQube static code analysis process.\nEach issue provides:\n📍 Location of the Issue: Enables identification of where the problem exists in the code.\n🤔 Reason for the Issue: Offers a detailed explanation of why it is considered a problem.\n💬 Activity: Facilitates collaboration by allowing the addition of comments and discussions about potential solutions.\n👤 Assignment: Permits the assignment of the issue to a registered user in SonarQube for tracking and resolution.\n📊 Status: Initiates with the “Open” status upon creation but may transition to other statuses such as:\n“Resolved as Fixed”: Indicates that the problem identified in the issue has been fixed in the source code.\n“Resolved as False Positive”: Initiates that the problem initially identified as an “issue” is not actually a problem or does not require correction.\n“Resolved as Won’t Fix”: Indicates that a decision has been made not to address or correct the problem noted in the issue.\n\n🏷️ Tags: Allows the addition of tags to enhance the identification and categorization of issues.\n\n\n🟣 Sonarqube API\nThe SonarQube API empowers users to interact with and extract information programmatically from a SonarQube instance.\nThese APIs serve as a tool for developers, administrators, and integrators, enabling the automation of tasks, retrieval of project metrics, and seamless integration of SonarQube data into various workflows.\nKey Features of the SonarQube API:\n🤖 Automation: The SonarQube API allows for the automation of various tasks related to project analysis, configuration, and management.\nThis includes triggering analyses, retrieving analysis results, and managing quality profiles.\n📊 Data Retrieval: Users can extract a wide range of data from SonarQube, including project metrics, issues, code smells, duplications, and more.\nThis information can be used for reporting, analytics, and custom dashboards.\n🔗 Integration: The API facilitates integration with other development tools, continuous integration (CI) systems, and external applications.\nThis enables seamless incorporation of SonarQube’s code quality and security analysis into existing development pipelines.\n⚙️ Configuration Management: The API allows users to manage SonarQube configurations programmatically.\nThis includes creating and updating quality profiles, setting project configurations, and managing global settings.\nCommon Use Cases:\n🤖 Automated Analysis: Integrate SonarQube analysis into your CI/CD pipeline by triggering analyses automatically after code commits or builds.\n📊 Custom Reporting: Extract specific metrics and data from SonarQube to generate custom reports tailored to your team or organization’s needs.\n🐛 Issue Tracking: Retrieve information about code issues, security vulnerabilities, and code smells to integrate SonarQube data into your issue tracking or project management system.\n🚦 Quality Gate Status: Monitor and retrieve the status of Quality Gates for projects to ensure that code meets predefined quality criteria.\n🛠️ Configuration as Code: Manage SonarQube configurations using scripts or code, making it easier to replicate configurations across different instances.\n🟣 SonarQube API: A Step-by-Step Guide\nInstall SonarQubeClient\npip install python-sonarqube-api\nConfig the sonarqube client\nfrom sonarqube import SonarQubeClient\nimport pandas as pd\n\nsonar = SonarQubeClient(sonarqube_url=\"http://localhost:9000\", username='admin', password='admin123')\nGet sonarqube client projects\nprojects = sonar.projects.search_projects()\nprojects\n{'paging': {'pageIndex': 1, 'pageSize': 100, 'total': 1},\n 'components': [\n  {'key': 'test',\n   'name': 'test',\n   'qualifier': 'TRK',\n   'visibility': 'public',\n   'lastAnalysisDate': '2023-12-10T13:47:32+0000',\n   'managed': False}]}\nGet the project events\nproject_analyses_and_events = sonar.project_analyses.search_project_analyses_and_events(project=\"test\")\nproject_analyses_and_events = pd.json_normalize(project_analyses_and_events['analyses'])\n\n[item for item in project_analyses_and_events['events'] if item!=[]]```\n\n```bash\n[[{'key': 'AYxT_AnVJaFOvYeUMXMa', 'category': 'VERSION', 'name': '1.0'}],\n [{'key': 'AYxT-xvhJaFOvYeUMXIe',\n   'category': 'QUALITY_GATE',\n   'name': 'Failed',\n   'description': 'Coverage on New Code < 80, New Code Smells > 0'}],\n [{'key': 'AYxT-cgDJaFOvYeUMXFb',\n   'category': 'QUALITY_GATE',\n   'name': 'Passed',\n   'description': ''}],\n [{'key': 'AYxT7MvDJaFOvYeUMW5W',\n   'category': 'QUALITY_GATE',\n   'name': 'Failed',\n   'description': 'Coverage on New Code < 80'}]]\n📚 References\nIf you want to learn…\nSonarqube: Documentation\nSonarLint: IDE\nSonarLint: Documentation\nSonarqube: API\nSonarqube: Python\nOther references:\nImage preview reference: [Imagen de storyset en Freepik]\n\n\n\n",
    "preview": "posts_en/2023-12-10-code-quality-sonarqube/preview.jpg",
    "last_modified": "2023-12-24T18:32:46+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-12-02-rabbitmq-pika/",
    "title": "RabbitMQ-Pika",
    "description": "RabbitMQ allows you to manage message queues between senders and recipients. In the next post we are going to use **Pika** in python for its implementation.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2023-12-02",
    "categories": [
      "Python"
    ],
    "contents": "\n\nContents\nIntroduction: What is RabbitMQ?\nImplementation with Pika in Python 🐍\n1️⃣ . Install pika\n2️⃣ . Create send.py 📄 file\n3️⃣. Create send.py 📄 file\n4️⃣. MongoDB + Pika\n\n\n\n\nRabbitMQ enables the management of message queues between senders and receivers.\nIn the following post, we will employ Python’s Pika library for its implementation.\n\nIntroduction: What is RabbitMQ?\nRabbitMQ is an intermediary system designed to facilitate the transfer of messages between producers and consumers through the implementation of queues.\nThis component, essential in distributed systems architecture, is grounded in key concepts:\n1️⃣.\nProducer: The entity responsible for originating and dispatching messages.\n2️⃣.\nQueue: A reservoir where messages are temporarily stored.\n3️⃣.\nConsumer: The receiving instance that processes messages according to the system’s needs.\nThis introduction aims to provide a clear and concise overview of the fundamental elements of RabbitMQ, paving the way for a deeper understanding of its functioning in messaging environments.\n\nImplementation with Pika in Python 🐍\nImplementing in Python with the Pika library involves creating two essential programs: the producer and the consumer.\nPika provides an effective interface for communication with RabbitMQ, leveraging a set of carefully designed objects for this purpose.\nIn our practical example, envision the producer as an application designed to manage food delivery orders 🛵.\nThis application, geared towards optimizing the delivery process, is responsible for sending multiple messages📝 related to user 📱 food orders.\nTo achieve this implementation, we will undertake the following steps:\nSteps\nDescriptions\nProducer:\nDevelop a program that, like an efficient order-taker, generates and sends messages📝 to the RabbitMQ queue. These messages will contain valuable information about food orders.\nConsumer:\nCreate a program that acts as the receiver of these messages in the queue. The consumer will be responsible for processing these messages according to the system’s needs, performing relevant actions, such as managing the delivery of orders.\nThis structured and efficient approach ensures a clear and functional implementation, providing a robust foundation for systems managing information flows in dynamic environments.\n1️⃣ . Install pika\n!pip install pika\n2️⃣ . Create send.py 📄 file\nimport pika\nfrom datetime import datetime\n\nconnection = pika.BlockingConnection(\n    pika.ConnectionParameters(host='localhost'))\nchannel = connection.channel()\n\nchannel.queue_declare(queue='delivery')\n\npedidos=['🍕🍕🍕','🍔🍔🍔','🍰🍰🍰','🍺🍺🍺']\n\nfor i in pedidos:\n    channel.basic_publish(exchange='', routing_key='delivery', body=i)\n    print(\" [x] Se envia pedido!'\"+ i)\n\nconnection.close()\n3️⃣. Create send.py 📄 file\nimport pika, sys, os\nfrom datetime import datetime\n\n\ndef main(queue='delivery'):\n    connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))\n    channel = connection.channel()\n    channel.queue_declare(queue=queue)\n\n    def callback(ch, method, properties, body):\n        print(\" [x] Received %r\" % body.decode())\n\n    channel.basic_consume(queue='delivery', on_message_callback=callback, auto_ack=True)\n\n    print(' [*] Waiting for messages. To exit press CTRL+C')\n    channel.start_consuming()\n\nif __name__ == '__main__':\n    try:\n        main(queue=queue)\n    except KeyboardInterrupt:\n        print('Interrupted')\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)\nImage description4️⃣. MongoDB + Pika\nIn the following, we will modify the script to enable it to connect to a MongoDB Atlas and perform the insertion of received messages.\nimport pymongo\nimport pika, sys, os\nfrom datetime import datetime\n\n# Crear una conexion con MongoClient\nclient = pymongo.MongoClient(\"mongodb+srv://NombreUser:PasswordUser@clusterName.moczg.mongodb.net/rabbit?retryWrites=true&w=majority\")\n\n# Database\ndb = client[\"rabbit\"]\n\n# Collection\ncollection= db[\"mensajes\"]\n\ndef main(queue='delivery'):\n    connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))\n    channel = connection.channel()\n    channel.queue_declare(queue=queue)\n\n    def callback(ch, method, properties, body):\n        print(\" [x] Received %r\" % body.decode())\n        body_indsert={'fecha':datetime.now(),'queue':queue,'message':body.decode()}\n        db[\"mensajes\"].insert_one(body_indsert)\n\n    channel.basic_consume(queue='hello', on_message_callback=callback, auto_ack=True)\n\n    print(' [*] Waiting for messages. To exit press CTRL+C')\n    channel.start_consuming()\n\nif __name__ == '__main__':\n    try:\n        main(queue=queue)\n    except KeyboardInterrupt:\n        print('Interrupted')\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)\nTo download the code for these two files, you can do so from the following link.\nTo learn more about RabbitMQ, you can visit the following sites:\n📄 Oficial Documentation\n📄 Rabbit Tutorial\n🐍 Pika\nImage preview reference: Imagen de rawpixel.com en Freepik\n\n\n\n",
    "preview": "posts_en/2023-12-02-rabbitmq-pika/preview.jpg",
    "last_modified": "2023-12-24T18:37:01+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-11-18-aws-copilot/",
    "title": "AWS Copilot",
    "description": "In the following article, I explain what AWS Copilot is, how to use this project, and the ease of implementing it.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://r0mymendez.github.io/romymendezblog/"
      }
    ],
    "date": "2023-11-14",
    "categories": [
      "Cloud Computing"
    ],
    "contents": "\n\nContents\nIntroduction\n💡AWS ECS (Elastic Container Services)\n⚒️ ECS with EC2 instances\n⚒️ ECS with AWS Fargate (Serverless)\n\n💡AWS Copilot\n🔎 Comparison of Task Responsibilities\n🧩 Components\n🚀 Deployment with AWS Copilot: A 5-Step Guide\n🔎 Logs\n📉 Traffic to production\n🧪Testing\n💰 Service Cost\n\n🎯 Key Takeaways\n📚 References\n\n\nIntroduction\nIn this article, I explain the use of the AWS Copilot service.\nHowever, to carry it out, it’s necessary to start by analyzing what the AWS Elastic Container Service (ECS) is and how it works, along with its deployment methods.\nThis is necessary because AWS Copilot performs the implementation and deployment of an application using ECS\n💡AWS ECS (Elastic Container Services)\nElastic Container Service ( ECS ) is a scalable container orchestration platform proprietary to AWS.\nIt is designed to run, stop, and manage containers in a cluster.\nTherefore, ECS is AWS’s Docker container service that handles the orchestration and provisioning of Docker containers.\nThis service includes the following concepts:\n☁️ Task Definition: Describes how to start a Docker container.\n☁️ Task: This is a running container with the configuration defined in the task definition.\n☁️ Service: Defines long-running tasks from the same task definition.\n☁️ Cluster: A logical group of EC2 instances.\n☁️ Container Instance: This is just an EC2 instance that is part of an ECS cluster and has Docker installed.\n⚒️ ECS with EC2 instances\nIn this model, containers are deployed on EC2 instances (VMs) created for the cluster.\nECS manages them along with the tasks that are part of the task definition\n✅ Advantages\n❌ Disadvantages\n- Complete control over the type of EC2 instance used is provided.\n- When working with EC2, it’s necessary for the administrator of this architecture to handle all security updates and scaling of instances.\n- It allows the use of instances that can be optimized depending on what you want to execute.\n- The cost is based on the type of EC2 instance running within the cluster and the VPC networks.\n⚒️ ECS with AWS Fargate (Serverless)\nIn this serverless configuration, the reliance on EC2 instances is eliminated, simplifying the deployment process.\nInstead, you only need to specify the required CPU and memory combination.\nAWS Fargate allows for a fully managed and serverless container deployment experience.\n✅ Advantages\n❌ Disadvantages\n- There are no servers to manage.\n- ECS + Fargate supports only one network mode, and this limits control over the network layer.\n- AWS is in charge of the availability and scalability of the containers.\n- Cost is based on the CPU and memory you select. The number of CPU cores and GB determines the cost of running the cluster.\n- Fargate Spot is a new capability that can run ECS tasks that are interruption-tolerant at up to a 70% discount compared to the Fargate price.\n\n💡AWS Copilot\nAWS Copilot is a tool used through the AWS command line that simplifies the creation, deployment, monitoring, and operation of containers in ECS using a local development environment\nThis tool manages the components required for the deployment and operation of an application, such as VPC, load balancers, deployment pipelines, and storage.\nTherefore, it’s only necessary to provide an application container and minimal configurations, resulting in a faster deployment and focusing on application development.\n🔎 Comparison of Task Responsibilities\nThe services will communicate with each other, so it is necessary to consider the following scenarios:\nActivities\nWithout AWS-copilot\nWith AWS-copilot\nApplication developmen\n📗 Development team\n📗 Development team\nContainer generation\n📗 Development team\n📗 Development team\nVirtual Private Cloud (VPC) Subnets\n📗 Development team\n📙 AWS-Copilot\nLoad balancers\n📗Development team\n📙 AWS-Copilot\nDeployment flows (ci/cd)\n📗 Development team\n📙 AWS-Copilot\nPersistent storage of your application\n📗 Development team\n📙 AWS-Copilot\nSynchronize deployment across environments\n📗 Development team\n📙 AWS-Copilot\n🧩 Components\nThe following table contains the components that are configured when using the AWS Copilot service.\nComponent\nDescription\nApplication\nAn application is a grouping mechanism for the pieces of your system.\nEnviroment\nAn environment is a stage in the deployment of an application.\nService\nA service is a single process of long-running code within a container.\n🚀 Deployment with AWS Copilot: A 5-Step Guide\nIn just 5 steps we can deploy an application using aws-copilot, as shown in the following image.\nThis allows the development team to only focus on development and not so much on the deployment of the infrastructure.\nThe first application that is deployed in copilot will make a default configuration and the same will be with a serverless container in fargate.\nAs seen in the following image, with only 5 steps we can deploy an application.\nImage descriptionThe steps in the flow are as follows:\nInstall AWS Copilot, which will require AWS client credentials.\nCreate the Dockerfile for our application.\nExecute copilot init in a terminal to initialize.\nWhen running init, some questions will appear to answer, such as the application name, service type, service name, and Dockerfile location.\nIn this final step, a URL will be provided to access the application\n🔎 Logs\nTo obtain the logs of the deployed containers, it is necessary to execute the following command:\n$ copilot svc logs- follow\n📉 Traffic to production\nTo deploy in production it is necessary to be able to generate different environments, so to generate them it is necessary to execute the following command.\n$ copilot env init\nSubsequently, it is important to be able to modify the manifest file that contains all the application configurations and is located in  nombredeaplicacion/manifest.yml\nOnce the environment configuration is complete, it is necessary to deploy it to production (or another environment, but the following example is in production).\n$ copilot svc deploy —env production\n🧪Testing\nIn order to test the deployed application, you can use ApacheBench which allows you to generate traffic to the web application.\nFor this it is necessary to be able to execute the following command in which you want to generate a number of 5000 transactions to my service with a concurrency of 25 requests at a time.\nab -n 5000 -c 25<http://app12345.us-east-1.elb.amazonaws.com>\nIf I do not have the expected response, I can modify my manifest file and horizontally scale the application based on the different environments.\n💰 Service Cost\nAWS Copilot is distributed by Amazon under an Apache 2.0 license, making it an open-source application.\nAs an open-source tool, AWS Copilot incurs no additional costs.\nThe pricing is solely determined by the usage of the configured services.\nThis cost-efficient model allows users to leverage the full capabilities of AWS Copilot without incurring any licensing fees.\n🎯 Key Takeaways\nIn conclusion, AWS Copilot stands out for the following features:\nAWS Copilot emerges as a robust, open-source AWS tool that streamlines the deployment of production-ready containers in a mere 5 steps, allowing development teams to concentrate on coding rather than grappling with infrastructure intricacies.\nConfiguration is effortless, demanding only the execution of a few commands and adjustments to the manifest file based on the application’s resource requirements.\nAddressing horizontal scaling needs is a breeze – a simple modification to the manifest file followed by a deployment is all it takes.\nAWS Copilot facilitates the establishment of a CI/CD pipeline for seamless, automatic deployments across various environments.\nEffortlessly generate KPIs, set up alarms, and collect metrics with just a few commands through the user-friendly AWS Copilot service.\n📚 References\n📚Title: Presentamos AWS Copilot, Site: Blog de Amazon Web Services (AWS), Author: Nathan Peck,Gabriel Gasca Torres y José Lorenzo Cuéncar, url: <https://aws.amazon.com/es/blogs/aws-spanish/presentamos-aws-copilot/>,\n📚Title: Introducción a Amazon ECS mediante AWS Copilot, Site: Documentación oficical de AWS, Author: AWS, url: <https://docs.aws.amazon.com/es_es/AmazonECS/latest/userguide/getting-started-aws-copilot-cli.html>\n📚Title: AWS Copilot, Site: AWS, Author: AWS, url: <https://aws.amazon.com/es/containers/copilot/>\n📚Title: Gentle Introduction to How AWS ECS Works with Example Tutorial, Site: Medium, Author: Tung Nguyen , Url: <https://medium.com/boltops/gentle-introduction-to-how-aws-ecs-works-with-example-tutorial-cea3d27ce63d>\nImage preview reference: Image by vectorjuice on Freepik\n\n\n\n",
    "preview": "posts_en/2023-11-18-aws-copilot/preview.jpg",
    "last_modified": "2023-12-24T18:38:07+01:00",
    "input_file": {}
  },
  {
    "path": "posts_en/2023-11-17-data-quality/",
    "title": "Data Quality",
    "description": "In the following article you will find the definition of data quality, what the domains are and how to quickly implement a solution.",
    "author": [
      {
        "name": "Romina Mendez",
        "url": "https://r0mymendez.github.io/romymendezblog/"
      }
    ],
    "date": "2023-11-12",
    "categories": [
      "Data",
      "Python"
    ],
    "contents": "\n\nContents\nWhat is Data quality?\nData quality dimensions - Use case\nPython Frameworks\nDifferences between Licencia Apache 2.0 y MIT\nDataset\n🟢 Pandera\nInstall pandera\nImplementation Example\n\n🟠 Great Expectations\nInstall great expectation\nImplementation example\n\nIf you want to learn…\n\n\n\nIn the current digital environment, the amount of available data is overwhelming.\nHowever, the true cornerstone for making informed decisions lies in the quality of this data.\nIn this article, we will explore the crucial importance of data quality, analyzing the inherent challenges that organizations face in managing information.\nAlthough often overlooked, data quality plays a fundamental role in the reliability and usefulness of the information that underpins our strategic decisions.\nWhat is Data quality?\nData quality measures how well a dataset complies with the criteria of accuracy, completeness, validity, consistency, uniqueness, timeliness, and fitness for purpose, and is fundamental for all data governance initiatives within an organization.\nData quality standards ensure that companies make decisions based on data to achieve their business objectives.\nsource: IBM\n\nsource: DataCamp cheat sheet\nThe following table highlights the various domains of data quality, from accuracy to fitness, providing an essential guide for assessing and enhancing the robustness of datasets:\nDimensions\nDescription\n🎯 Accuracy\nData accuracy, or how close data is to reality or truth. Accurate data is that which faithfully reflects the information it seeks to represent.\n🧩 Completeness\nMeasures the entirety of the data. A complete dataset is one that has no missing values or significant gaps. Data integrity is crucial for gaining a comprehensive and accurate understanding.\n✅ Validity\nIndicates whether the data conforms to defined rules and standards. Valid data complies with the established constraints and criteria for a specific dataset..\n🔄 Consistency\nRefers to the uniformity of data over time and across different datasets. Consistent data does not exhibit contradictions or discrepancies when compared with each other\n📇 Uniqueness\nEvaluates whether there are no duplicates in the data. Unique data ensures that each entity or element is represented only once in a dataset\n⌛Timeliness\nRefers to the timeliness of data. Timely information is that which is available when needed, without unnecessary delays.\n🏋️ Fitness\nThis aspect evaluates the relevance and usefulness of data for the intended purpose. Data should be suitable and applicable to the specific objectives of the organization or analysis being conducted.\nData quality dimensions - Use case\nNext, we provide an example where some issues with an e-commerce-based use case can be observed.\nID Transacción\nID Cliente\nProducto\nCantidad\nPrecio Unitario\nTotal\n⚪ 1\n10234\nLaptop HP\n1\n$800\n$800\n🟣 2\n\nWireless Headphones\n2\n$50\n$100\n🔵 3\n10235\nSmartphone\n-1\n$1000\n-$1000\n🟢 4\n10236\nWireless Mouse\n3\n$30\n$90\n🟢 4\n10237\nWireless Keyboard\n2\n$40\n$80\n🟣 Row 2 (Completeness): Row 2 does not comply with data integrity (Completeness) as the customer ID is missing.\nCustomer information is incomplete, making it challenging to trace the transaction back to a specific customer.\n🔵 Row 3 (Accuracy and Consistency): Row 3 exhibits accuracy (Accuracy) and consistency (Consistency) issues.\nThe quantity of products is negative, which is inaccurate and goes against the expected consistency in a transaction dataset.\n🟢 Row 4 (Uniqueness): The introduction of a second row with the same transaction ID (Transaction ID = 4) violates the uniqueness principle.\nEach transaction should have a unique identifier, and having two rows with the same Transaction ID creates duplicates, affecting the uniqueness of transactions.\n\nPython Frameworks\nThe following are some of the Python implementations carried out to perform data quality validations:\nFramework\nDescripción\nGreat Expectations\nGreat Expectations is an open-source library for data validation. It enables the definition, documentation, and validation of expectations about data, ensuring quality and consistency in data science and analysis projects\nPandera\nPandera is a data validation library for data structures in Python, specifically designed to work with pandas DataFrames. It allows you to define schemas and validation rules to ensure data conformity\nDora\nDora is a Python library designed to automate data exploration and perform exploratory data analysis.\nLet’s analyze some of the metrics that can be observed in their GitHub repositories, taking into account that the metrics were obtained on 2023-11-12.\nMetricas\nGreat Expectations\nPandera\nDora\n👥 Members\n399\n109\n106\n⚠️ Issues: Open\n112\n273\n1\n🟢 Issues: Close\n1642\n419\n7\n⭐ Stars\n9000\n2700\n623\n📺 Watching\n78\n17\n42\n🔎 Forks\n1400\n226\n63\n📬 Open PR\n43\n19\n0\n🐍 Version Python\n>=3.8\n>=3.7\nNo especificada\n📄 Version Number\n233\n76\n3\n📄 Last Version\n0.18.2\n0.17.2\n0.0.3\n📆 Last Date Version\n9 Nov 2023\n30 sep 2023\n30 jun 2020\n📄 Licence type\nApache-2.0 license\nMIT\nMIT\n📄 Languages\nPython 95.1%\nJupyter Notebook 4.3%\nJinja 0.4%\nJavaScript 0.1%\nCSS 0.1%\nHTML 0.0%\n\nPython 99.9%\nMakefile 0.1%\n\nPython100%\n\nDifferences between Licencia Apache 2.0 y MIT\nNotification of Changes:\nApache 2.0: Requires notification of changes made to the source code when distributing the software.\nMIT: Does not require specific notification of changes.\n\nCompatibility:\nApache 2.0: Known to be compatible with more licenses compared to MIT.\nMIT: Also quite compatible with various licenses, but Apache 2.0 License is often chosen in projects seeking greater interoperability with other licenses.\n\nAttribution:\nApache 2.0: Requires attribution and the inclusion of a copyright notice.\nMIT: Requires attribution to the original authorship but may have less strict requirements in terms of how that attribution is displayed.\n\nConsidering these currently analyzed metrics, let’s proceed with an example implementation using Pandera and Great Expectations.\n\nDataset\nFor the development of this example, we will use the dataset named ‘Tips.’ You can download the dataset from the followinge link.\nThe ‘tips’ dataset contains information about tips given in a restaurant, along with details about the total bill, the gender of the person who paid the bill, whether the customer is a smoker, the day of the week, and the meal’s time.\nColumn\nDescription\ntotal_bill\nThe total amount of the bill (including the tip).\ntip\nThe amount of tip given.\nsex\nThe gender of the bill payer (male or female).\nsmoker\nWhether the customer is a smoker or not.\nday\nThe day of the week when the meal was made.\ntime\nThe time of day (lunch or dinner).\nsize\nThe size of the group that shared the meal.\nBelow is a table with the first 5 rows of the dataset:\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n🟢 Pandera\nNext, we will provide an example of implementing Pandera using the dataset described earlier.\nInstall pandera\npip install pandas pandera \nImplementation Example\nImport pandas and pandera\nimport pandas as pd\nimport pandera as pa\nImport the dataframe file\npath = 'data/tips.csv'\ndata = pd.read_csv(path)\n\nprint(f\"Numero de columnas: {data.shape[1]}, Numero de filas: {data.shape[0]}\")\nprint(f\"Nombre de columnas: {list(data.columns)}\")\ndata.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   total_bill  244 non-null    float64\n 1   tip         244 non-null    float64\n 2   sex         244 non-null    object \n 3   smoker      244 non-null    object \n 4   day         244 non-null    object \n 5   time        244 non-null    object \n 6   size        244 non-null    int64  \ndtypes: float64(2), int64(1), object(4)\nmemory usage: 13.5+ KB\nNow, let’s create the schema object that contains all the validations we want to perform.\nYou can find additional validations that can be performed at the following link: <https://pandera.readthedocs.io/en/stable/dtype_validation.html>\nschema = pa.DataFrameSchema({\n  \"total_bill\": pa.Column(float, checks=pa.Check.le(50)),\n  \"tip\"       : pa.Column(float, checks=pa.Check.between(0,30)),\n  \"sex\"       : pa.Column(str, checks=[pa.Check.isin(['Female','Male'])]),\n  \"smoker\"    : pa.Column(str, checks=[pa.Check.isin(['No','Yes'])]),\n  \"day\"       : pa.Column(str, checks=[pa.Check.isin(['Sun','Sat'])]),\n  \"time\"      : pa.Column(str, checks=[pa.Check.isin(['Dinner','Lunch'])]),\n  \"size\"      : pa.Column(int, checks=[pa.Check.between(1,4)])\n})\nTo capture the error and subsequently analyze the output, it is necessary to catch it using an exception.\ntry:\n    schema(data).validate()\nexcept Exception as e:\n    print(e)\n    error = e\nSchema None: A total of 3 schema errors were found.\n\nError Counts\n------------\n- SchemaErrorReason.SCHEMA_COMPONENT_CHECK: 3\n\nSchema Error Summary\n--------------------\nschema_context column     check                     failure_cases  n_failure_cases\n                                                   \nColumn         day        isin(['Sun', 'Sat'])      [Thur, Fri]             2\n               size       in_range(1, 4)              [5, 6]                2\n               total_bill less_than_or_equal_to(50)   [50.81]               1\nBelow is a function that allows you to transform the output into a dictionary or a pandas dataframe\ndef get_errors(error, dtype_dict=True):\n    response = []\n\n \n    for item in range(len(error.schema_errors)):\n        error_item = error.schema_errors[item]\n        response.append(\n        {\n            'column'     :error_item.schema.name,\n            'check_error':error_item.schema.checks[0].error,\n            'num_cases'  :error_item.failure_cases.index.shape[0],\n            'check_rows' :error_item.failure_cases.to_dict()\n        })\n    \n    if dtype_dict:\n        return response\n    else:\n        return pd.DataFrame(response)\nget_errors(error,dtype_dict=True)\n[{'column': 'total_bill',\n  'check_error': 'less_than_or_equal_to(50)',\n  'num_cases': 1,\n  'check_rows': {'index': {0: 170}, 'failure_case': {0: 50.81}}},\n {'column': 'day',\n  'check_error': \"isin(['Sun', 'Sat'])\",\n  'num_cases': 81,\n  'check_rows': {'index': {0: 77,\n    1: 78,\n    2: 79,\n    3: 80,\n    4: 81,\n    5: 82,\n    6: 83,\n    7: 84,\n...\n    5: 156,\n    6: 185,\n    7: 187,\n    8: 216},\n   'failure_case': {0: 6, 1: 6, 2: 5, 3: 6, 4: 5, 5: 6, 6: 5, 7: 5, 8: 5}}}]\n🟠 Great Expectations\nGreat Expectations is an open-source Python-based library for validating, documenting, and profiling your data.\nIt helps maintain data quality and improve communication about data across teams.\n\nsource : <https://docs.greatexpectations.io/docs/>\nTherefore, we can describe Great Expectations as an open source tool designed to guarantee the quality and reliability of data in various sources, such as databases, tables, files and dataframes.\nIts operation is based on the creation of validation groups that specify the expectations or rules that the data must comply with.\nThe following are the steps that we must define when using this framework:\nDefinition of Expectations: Specify the expectations you have for the data.\nThese expectations can include simple constraints, such as value ranges, or more complex rules about data coherence and quality.\nConnecting to Data Sources: In this step, define the connections you need to make to various data sources, such as databases, tables, files, or dataframes.\nGeneration of Validation Suites: Based on the defined expectations, Great Expectations generates validation suites, which are organized sets of rules to be applied to the data.\nExecution of Validations: Validation suites are applied to the data to verify if they meet the defined expectations.\nThis can be done automatically in a scheduled workflow or interactively as needed.\nGeneration of Analysis and Reports: Great Expectations provides advanced analysis and reporting capabilities.\nThis includes detailed data quality profiles and reports summarizing the overall health of the data based on expectations.\nAlerts and Notifications: If the data does not meet the defined expectations, Great Expectations can generate alerts or notifications, allowing users to take immediate action to address data quality issues.\nTogether, Great Expectations offers a comprehensive solution to ensure data quality over time, facilitating early detection of problems and providing confidence in the integrity and usefulness of data used in analysis and decision-making\nInstall great expectation\n!pip install great_expectations==0.17.22 seaborn matplotlib numpy pandas\nImplementation example\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport re \n\nimport great_expectations as gx\nfrom ruamel.yaml import YAML\nfrom great_expectations.cli.datasource import sanitize_yaml_and_save_datasource\nfrom great_expectations.core.expectation_configuration import ExpectationConfiguration\n\nprint(f\"* great expectations version:{gx.__version__}\")\nprint(f\"* seaborn version:{sns.__version__}\")\nprint(f\"* numpy version:{np.__version__}\")\nprint(f\"* pandas:{pd.__version__}\")\n* great expectations version:0.17.22\n* seaborn version:0.13.0\n* numpy version:1.26.1\n* pandas:2.1.3\nImport dataset using great expectation\npath = 'data/tips.csv'\ndata_gx = gx.read_csv(path)\nList all available expectations by type\nlist_expectations = pd.DataFrame([item for item in dir(data_gx) if item.find('expect_')==0],columns=['expectation'])\nlist_expectations['expectation_type'] = np.select( [\n        list_expectations.expectation.str.find('_table_')>0, \n        list_expectations.expectation.str.find('_column_')>0,  \n        list_expectations.expectation.str.find('_multicolumn_')>0,\n    ],['table','column','multicolumn'],\n    default='other'\n)\n\nplt.figure(figsize=(20,6))\nsns.countplot(x=list_expectations.expectation_type)\nplt.show()\n\n\nIn the image, it can be observed that the available expectations are mainly applied to columns (for example: expect_column_max_to_be_between) and tables (for example: expect_table_columns_to_match_set), although an expectation based on the values of multiple columns can also be applied (for example: expect_multicolumn_values_to_be_unique).\n\nExpectations: Tables\n# The following list contains the columns that the dataframe must have:\ncolumns = ['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size']\ndata_gx.expect_table_columns_to_match_set(column_set = columns)\n{\n  \"success\": true,\n  \"result\": {\n    \"observed_value\": [\n      \"total_bill\",\n      \"tip\",\n      \"sex\",\n      \"smoker\",\n      \"day\",\n      \"time\",\n      \"size\"\n    ]\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\n# Now, we delete two columns, 'time' and 'size,' to validate the outcome\n\ncolumns = = ['total_bill', 'tip', 'sex', 'smoker', 'day']\ndata_gx.expect_table_columns_to_match_set(column_set = columns)\nIf we observe, the result is False, and in the details, they provide information about the columns that the dataframe has in addition to those expected.\n{\n  \"success\": false,\n  \"result\": {\n    \"observed_value\": [\n      \"day\",\n      \"sex\",\n      \"size\",\n      \"smoker\",\n      \"time\",\n      \"tip\",\n      \"total_bill\"\n    ],\n    \"details\": {\n      \"mismatched\": {\n        \"unexpected\": [\n          \"size\",\n          \"time\"\n        ]\n      }\n    }\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\nExpectations: Columns\nLet’s validate that there is a categorical value within a column\ndata_gx['total_bill_group'] = pd.cut(data_gx['total_bill'],\n                              bins=[0,10,20,30,40,50,float('inf')], \n                              labels=['0-10', '10-20', '20-30', '30-40', '40-50', '>50'],\n                              right=False, \n                              include_lowest=True)\n\n# Now, let's validate if 3 categories exist within the dataset\n\ndata_gx.expect_column_distinct_values_to_contain_set(column='total_bill_group',\n                                                      value_set=['0-10','10-20', '20-30'],\n                                                      result_format='BASIC') \n{\n  \"success\": true,\n  \"result\": {\n    \"observed_value\": [\n      \"0-10\",\n      \"10-20\",\n      \"20-30\",\n      \"30-40\",\n      \"40-50\",\n      \">50\"\n    ],\n    \"element_count\": 244,\n    \"missing_count\": null,\n    \"missing_percent\": null\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\nLet’s validate that the column does not have null values\ndata_gx.expect_column_values_to_not_be_null('sex')\n{\n  \"success\": true,\n  \"result\": {\n    \"element_count\": 244,\n    \"unexpected_count\": 0,\n    \"unexpected_percent\": 0.0,\n    \"unexpected_percent_total\": 0.0,\n    \"partial_unexpected_list\": []\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\nGreat Expectation Project\nNow, let’s generate a Great Expectations project to run a group of validations based on one or more datasets.\nInitialize the Great Expectations project:\n !yes Y | great_expectations init\n ___              _     ___                  _        _   _\n / __|_ _ ___ __ _| |_  | __|_ ___ __  ___ __| |_ __ _| |_(_)___ _ _  ___\n| (_ | '_/ -_) _` |  _| | _|\\ \\ / '_ \\/ -_) _|  _/ _` |  _| / _ \\ ' \\(_-<\n \\___|_| \\___\\__,_|\\__| |___/_\\_\\ .__/\\___\\__|\\__\\__,_|\\__|_\\___/_||_/__/\n                                |_|\n             ~ Always know what to expect from your data ~\n\nLet's create a new Data Context to hold your project configuration.\n\nGreat Expectations will create a new directory with the following structure:\n\n    great_expectations\n    |-- great_expectations.yml\n    |-- expectations\n    |-- checkpoints\n    |-- plugins\n    |-- .gitignore\n    |-- uncommitted\n        |-- config_variables.yml\n        |-- data_docs\n        |-- validations\n\nOK to proceed? [Y/n]: \n================================================================================\n\nCongratulations! You are now ready to customize your Great Expectations configuration.\n\nYou can customize your configuration in many ways. Here are some examples:\n\n  Use the CLI to:\n    - Run `great_expectations datasource new` to connect to your data.\n    - Run `great_expectations checkpoint new <checkpoint_name>` to bundle data with Expectation Suite(s) in a Checkpoint for later re-validation.\n    - Run `great_expectations suite --help` to create, edit, list, profile Expectation Suites.\n    - Run `great_expectations docs --help` to build and manage Data Docs sites.\n\n  Edit your configuration in great_expectations.yml to:\n    - Move Stores to the cloud\n    - Add Slack notifications, PagerDuty alerts, etc.\n    - Customize your Data Docs\n\nPlease see our documentation for more configuration options!\nCopy data into the ‘great_expectations’ folder generated from the project initialization\n!cp -r data gx\n# Let's print the contents of the folder\n\ndef print_directory_structure(directory_path, indent=0):\n    current_dir = os.path.basename(directory_path)\n    print(\"    |\" + \"    \" * indent + f\"-- {current_dir}\")\n    indent += 1\n    with os.scandir(directory_path) as entries:\n        for entry in entries:\n            if entry.is_dir():\n                print_directory_structure(entry.path, indent)\n            else:\n                print(\"    |\" + \"    \" * indent + f\"-- {entry.name}\")\n\n\nprint_directory_structure('gx')\n    |-- gx\n    |    -- great_expectations.yml\n    |    -- plugins\n    |        -- custom_data_docs\n    |            -- renderers\n    |            -- styles\n    |                -- data_docs_custom_styles.css\n    |            -- views\n    |    -- checkpoints\n    |    -- expectations\n    |        -- .ge_store_backend_id\n    |    -- profilers\n    |    -- .gitignore\n    |    -- data\n    |        -- tips.csv\n    |    -- uncommitted\n    |        -- data_docs\n    |        -- config_variables.yml\n    |        -- validations\n    |            -- .ge_store_backend_id\nHere are some clarifications about the files and folders generated in this directory:\nFiles/Folders\nDescription\n📄 great_expectations.yml\nThis file contains the main configuration of the project. Details such as storage locations and other configuration parameters are specified here\n📂 plugins\ncustom_data_docs:\n📄renderers: It contains custom renderers for data documents.\n📄 styles: It includes custom styles for data documents, such as CSS style sheets (data_docs_custom_styles.css).\n📄 views: It can contain custom views for data documents.\n\n📂 checkpoints\nThis folder could contain definitions of checkpoints, which are points in the data flow where specific validations can be performed.\n📂 expectations\nThis is where the expectations defined for the data are stored. This directory may contain various subfolders and files, depending on the project’s organization.\n📂 profilers\nIt can contain configurations for data profiles, which are detailed analyses of data statistics.\n📄 .gitignore\nIt is a Git configuration file that specifies files and folders to be ignored when performing tracking and commit operations. (commit)\n📂 data\nIt contains the data used in the project, in this case, the file tips.csv.\n📂 uncommitted\n📂data_docs: Folder where data documents are generated.\n📄config_variables.yml: Configuration file that can contain project-specific variables\n📂validations: It can contain results of validations performed on the data.\n\nConfiguration of datasource and data connectors:\nDataSource: It is the data source used (can be a file, API, database, etc.).\nData Connectors: These are the connectors that facilitate the connection to data sources and where access credentials, location, etc., should be defined.\n\ndatasource_name_file = 'tips.csv'\ndatasource_name = 'datasource_tips'\ndataconnector_name = 'connector_tips'\n# Let's create the configuration for the datasource\n\ncontext = gx.data_context.DataContext()\nmy_datasource_config = f\"\"\"\n    name: {datasource_name}\n    class_name: Datasource\n    execution_engine:\n      class_name: PandasExecutionEngine\n    data_connectors:\n      {dataconnector_name}:\n        class_name: InferredAssetFilesystemDataConnector\n        base_directory: data\n        default_regex:\n          group_names:\n            - data_asset_name\n          pattern: (.*)\n      default_runtime_data_connector_name:\n        class_name: RuntimeDataConnector\n        assets:\n          my_runtime_asset_name:\n            batch_identifiers:\n              - runtime_batch_identifier_name\n\"\"\"\n\nyaml = YAML()\ncontext.add_datasource(**yaml.load(my_datasource_config))\nsanitize_yaml_and_save_datasource(context, my_datasource_config, overwrite_existing=True)\nConfiguration of the expectations\n\nIn the following code snippet, the configuration of three expectations is presented.\nIn particular, the last one includes a parameter called ‘mostly’ with a value of 0.75.\nThis parameter indicates that the expectation can fail in up to 25% of cases, as by default, 100% compliance is expected unless specified otherwise.\nAdditionally, an error message can be specified in markdown format, as shown in the last expectation.\n\nexpectation_configuration_table =  ExpectationConfiguration(\n   expectation_type=\"expect_table_columns_to_match_set\",\n      kwargs= {\n        \"column_set\": ['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size']\n      },\n      meta= {}\n)\n\nexpectation_configuration_total_bill = ExpectationConfiguration(\n      expectation_type= \"expect_column_values_to_be_between\",\n      kwargs= {\n        \"column\": \"total_bill\",\n        \"min_value\": 0,\n        \"max_value\": 100\n      },\n      meta= {}\n)\n\n\nexpectation_configuration_size = ExpectationConfiguration(\n   expectation_type=\"expect_column_values_to_not_be_null\",\n   kwargs={\n      \"column\": \"size\",\n      \"mostly\": 0.75,\n   },\n   meta={\n      \"notes\": {\n         \"format\": \"markdown\",\n         \"content\": \"Expectation to validate column `size` does not have null values.\"\n      }\n   }\n)\nCreation of the expectation suite\nexpectation_suite_name = \"tips_expectation_suite\"\nexpectation_suite = context.create_expectation_suite(\n    expectation_suite_name=expectation_suite_name, \n    overwrite_existing=True\n)\n\n# Add expectations\nexpectation_suite.add_expectation(expectation_configuration=expectation_configuration_table)\nexpectation_suite.add_expectation(expectation_configuration=expectation_configuration_total_bill)\nexpectation_suite.add_expectation(expectation_configuration=expectation_configuration_size)\n\n# save expectation_suite\ncontext.save_expectation_suite(expectation_suite=expectation_suite, \n                               expectation_suite_name=expectation_suite_name)\ndata-quality/gx/expectations/tips_expectation_suite.json\n\nWithin the ‘expectations’ folder, a JSON file is created with all the expectations generated earlier.\n\nConfiguration of the checkpoints\ncheckpoint_name ='tips_checkpoint'\n\nconfig_checkpoint = f\"\"\"\n    name: {checkpoint_name}\n    config_version: 1\n    class_name: SimpleCheckpoint\n    expectation_suite_name: {expectation_suite_name}\n    validations:\n      - batch_request:\n          datasource_name: {datasource_name}\n          data_connector_name: {dataconnector_name}\n          data_asset_name: {datasource_name_file}\n          batch_spec_passthrough:\n            reader_method: read_csv\n            reader_options: \n              sep: \",\"\n          data_connector_query:\n            index: -1\n        expectation_suite_name: {expectation_suite_name}\n\"\"\"\n\n# Validate if the YAML structure is correct\ncontext.test_yaml_config(config_checkpoint)\n\n# Add the checkpoint to the generated context\ncontext.add_checkpoint(**yaml.load(config_checkpoint)) \nExecute the checkpoint to validate all the configured expectations on the dataset\nresponse = context.run_checkpoint(checkpoint_name=checkpoint_name)\nTo observe the result obtained from the validations, it can be converted to JSON\n response.to_json_dict()\n{'run_id': {'run_name': None, 'run_time': '2023-11-12T20:39:23.346946+01:00'},\n 'run_results': {'ValidationResultIdentifier::tips_expectation_suite/__none__/20231112T193923.346946Z/722b2e93e32fd7222c8ad9339f3e0e1d': {'validation_result': {'success': True,\n    'results': [{'success': True,\n      'expectation_config': {'expectation_type': 'expect_table_columns_to_match_set',\n       'kwargs': {'column_set': ['total_bill',\n         'tip',\n         'sex',\n         'smoker',\n         'day',\n         'time',\n         'size'],\n        'batch_id': '722b2e93e32fd7222c8ad9339f3e0e1d'},\n       'meta': {}},\n      'result': {'observed_value': ['total_bill',\n        'tip',\n        'sex',\n        'smoker',\n        'day',\n        'time',\n        'size']},\n      'meta': {},\n      'exception_info': {'raised_exception': False,\n       'exception_traceback': None,\n       'exception_message': None}},\n     {'success': True,\n...\n  'notify_on': None,\n  'default_validation_id': None,\n  'site_names': None,\n  'profilers': []},\n 'success': True}\nNow, let’s obtain the results\n context.open_data_docs()\n\nBy executing this code chunk, an HTML file with the results of the validations will open at gx/uncommitted/data_docs/local_site/validations/tips_expectation_suite/__none__/20231112T192529.002401Z/722b2e93e32fd7222c8ad9339f3e0e1d.html\n\n\n\n\nIf you want to learn…\nPandera Documentación Oficial\nPandera: Statistical Data Validation of Pandas Dataframes - Researchgate\nGreat Expectation Documentación Oficial\nData Quality Fundamentals Book O’relly\nGreat Expectation Yoututbe Channel\nImage preview reference: Image by jcomp on Freepik\n\n\n\n",
    "preview": "posts_en/2023-11-17-data-quality/preview.jpg",
    "last_modified": "2023-12-24T18:39:18+01:00",
    "input_file": {}
  }
]
